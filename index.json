[{"categories":["Moment"],"content":"My first blog!","date":"2021-12-12","objectID":"/first-post/","series":["Casual"],"tags":["Blog"],"title":"First Post","uri":"/first-post/"},{"categories":["Moment"],"content":"It’s 2021 now, and I just begin to write my first blog. Well, it’s better late than never. ","date":"2021-12-12","objectID":"/first-post/:0:0","series":["Casual"],"tags":["Blog"],"title":"First Post","uri":"/first-post/#"},{"categories":null,"content":"PceWlkr   Github: https://github.com/Peacewalker365   GPA(2019-now): \u003e3.9 ","date":"2021-12-12","objectID":"/about/:1:0","series":null,"tags":null,"title":"About ME","uri":"/about/#pcewlkr"},{"categories":null,"content":"Profile A quick learner and problem-solving-oriented senior student seeking opportunities to software developing, NLP, or image processing \u0026 recognition with AI. ","date":"2021-12-12","objectID":"/about/:1:1","series":null,"tags":null,"title":"About ME","uri":"/about/#profile"},{"categories":null,"content":"Projects InCollege CLI: A CLI based student platform developed with Python3 to communicate, learn skills, and post and find internships. Console2048: A console version 2048 game developed with C#. ATM: An event-driven GUI based software realized the basic functions of an ATM using Java. Data Structure \u0026 Algo Wheels: A collection of data structures and algorithms built from scratch using C/C++. Amazon Review Analysis Perform N-grams NLP using Python, Spark, and self-built Hadoop Cluster on Amazon official data set. Personal Blog Deploy personal tech blog using Hugo and github pages. CUDA Project Develop and optimize algorithms for Parallel Radix Partition and Spatial Distance Histogram Computation problems. ","date":"2021-12-12","objectID":"/about/:1:2","series":null,"tags":null,"title":"About ME","uri":"/about/#projects"},{"categories":null,"content":"Skills Programming Language: C/C++, Java, Python, Swift\u0026SwiftUI, CUDA, SQL Framework: Spring5, SpringBoot CS Basics: Design Patterns, Data Structure, Analysis of Algorithms, Operating Systems Dev Skills: Git, UML, Agile, bash/zsh, Jira, Hadoop Cluster ","date":"2021-12-12","objectID":"/about/:1:3","series":null,"tags":null,"title":"About ME","uri":"/about/#skills"},{"categories":null,"content":"Advice Read book for learning in depth Learn and talk to successful people (in terms of indie dev, not the money) Your friends may not know your field, though they may support you mentally or the otherwise Don’t be satisfied with only talking about your ideas with your friends Books can also be seen as a communication to successes Getting Real The smarter, faster, easier way to build a successful web application Becoming big is not the only way to success Small, elegant, and targeted application may be better You can develop something for yourself first What’s your problem? If you have a problem, it’s likely many else are bothered in the same way And this is your market It’s a problem when it’s a problem You launch a app without the ability to bill your customers? Make Opinionated Software Bullshit: software should always be as flexible as possible The best software has a vision If they don’t like your vision, then there are plenty of others out there Don’t try to chasing people you’ll never make happy Don’t be a yes-man You should only consider features if they’re willing to stand on the porch for three days waiting to be let in You can listen but don’t act all the time The initial response is “not now” If a request for a feature keeps coming back, that’s when we should take a deeper look Less Software Each time you increase the amount of code, your software grows exponentially more complicated Bugs always emerge from the feature you don’t use Ride the Blog Wave Start off by creating a blog that not only touts your product but offers helpful advices, tips, tricks, etc. It will attract readers with a week thanks to the helpful, informative, and interesting bits and anecdotes we post on a daily basis. ","date":"2021-12-12","objectID":"/advice-on-being-solo-dev/:0:1","series":null,"tags":["Advice","Note"],"title":"Advice for Solo Dev","uri":"/advice-on-being-solo-dev/#advice"},{"categories":null,"content":"Marketing Lessons: Grateful Dead Share journey, blog, videos, which will get you a fan base Just be yourself, which is more attractive for most people online The marketplace is incredibly forgiving of mistakes Owns up to a mistake immediately Explains how or why it happened and how you are fixing it Don’t forget to say thanks Stop hiding your personality behind the scene Don’t create a new account for your new product every time People can’t see the reason and motivations for it Give the best deals to existing customers and tell your fans first Show the people who invest their time and money in your company that you care Don’t try to use the best deals for your new target customers only The way to reach your marketplace is to create tons of remarkable, free content, like blogs, videos, white papers, and e-books. Cultivate more and you’ll receive more But remember that great product comes first ","date":"2021-12-12","objectID":"/advice-on-being-solo-dev/:0:2","series":null,"tags":["Advice","Note"],"title":"Advice for Solo Dev","uri":"/advice-on-being-solo-dev/#marketing-lessons-grateful-dead"},{"categories":null,"content":"The Goal by Eliyahu M. Goldratt Problems Hard to concentrate because your head is full of things Can’t prioritize tasks due to too many things to do Feeling restless and uneasy because you are always chased by something Unstable mind and uncontrollable motivation Solutions What is the fundamental skill for continuously improving your productivity by yourself DO LESS Less code Less marketing Less user support So you get more room to the further When you are productive you are accomplishing something in terms of your goal The easier way is to solve your own problem as a motivation of your development Whatever the bottlenecks produce in an hour is the equivalent of what the plant produces in an hour. So… an hour lost at a bottleneck is an hour lost for the entire system Step1. Identify the system’s bottlenecks Step2. decide how to exploit the bottlenecks Step3. Subordinate everything else to the above decision Step4. Elevate the system’s bottlenecks Step5. If in a previous step, a bottleneck has been broken go back to step 1 Anything can be a bottleneck Software design Physical \u0026 mental health Relationships with your family, friends, colleagues, customers, etc. Home \u0026 work environment Equipment etc. Have a daily habit to checking or reflecting yourself After reading a book. Put it into practice immediately! Don’t do it in your way but do it exactly as what the book says as possible as you can from youtube channel: devaslife ","date":"2021-12-12","objectID":"/advice-on-being-solo-dev/:0:3","series":null,"tags":["Advice","Note"],"title":"Advice for Solo Dev","uri":"/advice-on-being-solo-dev/#the-goal-by-eliyahu-m-goldratt"},{"categories":null,"content":"The Goal by Eliyahu M. Goldratt Problems Hard to concentrate because your head is full of things Can’t prioritize tasks due to too many things to do Feeling restless and uneasy because you are always chased by something Unstable mind and uncontrollable motivation Solutions What is the fundamental skill for continuously improving your productivity by yourself DO LESS Less code Less marketing Less user support So you get more room to the further When you are productive you are accomplishing something in terms of your goal The easier way is to solve your own problem as a motivation of your development Whatever the bottlenecks produce in an hour is the equivalent of what the plant produces in an hour. So… an hour lost at a bottleneck is an hour lost for the entire system Step1. Identify the system’s bottlenecks Step2. decide how to exploit the bottlenecks Step3. Subordinate everything else to the above decision Step4. Elevate the system’s bottlenecks Step5. If in a previous step, a bottleneck has been broken go back to step 1 Anything can be a bottleneck Software design Physical \u0026 mental health Relationships with your family, friends, colleagues, customers, etc. Home \u0026 work environment Equipment etc. Have a daily habit to checking or reflecting yourself After reading a book. Put it into practice immediately! Don’t do it in your way but do it exactly as what the book says as possible as you can from youtube channel: devaslife ","date":"2021-12-12","objectID":"/advice-on-being-solo-dev/:0:3","series":null,"tags":["Advice","Note"],"title":"Advice for Solo Dev","uri":"/advice-on-being-solo-dev/#problems"},{"categories":null,"content":"The Goal by Eliyahu M. Goldratt Problems Hard to concentrate because your head is full of things Can’t prioritize tasks due to too many things to do Feeling restless and uneasy because you are always chased by something Unstable mind and uncontrollable motivation Solutions What is the fundamental skill for continuously improving your productivity by yourself DO LESS Less code Less marketing Less user support So you get more room to the further When you are productive you are accomplishing something in terms of your goal The easier way is to solve your own problem as a motivation of your development Whatever the bottlenecks produce in an hour is the equivalent of what the plant produces in an hour. So… an hour lost at a bottleneck is an hour lost for the entire system Step1. Identify the system’s bottlenecks Step2. decide how to exploit the bottlenecks Step3. Subordinate everything else to the above decision Step4. Elevate the system’s bottlenecks Step5. If in a previous step, a bottleneck has been broken go back to step 1 Anything can be a bottleneck Software design Physical \u0026 mental health Relationships with your family, friends, colleagues, customers, etc. Home \u0026 work environment Equipment etc. Have a daily habit to checking or reflecting yourself After reading a book. Put it into practice immediately! Don’t do it in your way but do it exactly as what the book says as possible as you can from youtube channel: devaslife ","date":"2021-12-12","objectID":"/advice-on-being-solo-dev/:0:3","series":null,"tags":["Advice","Note"],"title":"Advice for Solo Dev","uri":"/advice-on-being-solo-dev/#solutions"},{"categories":null,"content":"Software Architectural Patterns ","date":"2021-12-12","objectID":"/software-architectural-patterns/:0:0","series":null,"tags":["Software Architectural Patterns","Note"],"title":"Arch Patterns Note","uri":"/software-architectural-patterns/#software-architectural-patterns"},{"categories":null,"content":"Layered pattern ","date":"2021-12-12","objectID":"/software-architectural-patterns/:1:0","series":null,"tags":["Software Architectural Patterns","Note"],"title":"Arch Patterns Note","uri":"/software-architectural-patterns/#layered-pattern"},{"categories":null,"content":"Client-server pattern ","date":"2021-12-12","objectID":"/software-architectural-patterns/:2:0","series":null,"tags":["Software Architectural Patterns","Note"],"title":"Arch Patterns Note","uri":"/software-architectural-patterns/#client-server-pattern"},{"categories":null,"content":"Master-slave pattern ","date":"2021-12-12","objectID":"/software-architectural-patterns/:3:0","series":null,"tags":["Software Architectural Patterns","Note"],"title":"Arch Patterns Note","uri":"/software-architectural-patterns/#master-slave-pattern"},{"categories":null,"content":"Pipe-filter pattern ","date":"2021-12-12","objectID":"/software-architectural-patterns/:4:0","series":null,"tags":["Software Architectural Patterns","Note"],"title":"Arch Patterns Note","uri":"/software-architectural-patterns/#pipe-filter-pattern"},{"categories":null,"content":"Broker pattern ","date":"2021-12-12","objectID":"/software-architectural-patterns/:5:0","series":null,"tags":["Software Architectural Patterns","Note"],"title":"Arch Patterns Note","uri":"/software-architectural-patterns/#broker-pattern"},{"categories":null,"content":"Peer-to-peer pattern ","date":"2021-12-12","objectID":"/software-architectural-patterns/:6:0","series":null,"tags":["Software Architectural Patterns","Note"],"title":"Arch Patterns Note","uri":"/software-architectural-patterns/#peer-to-peer-pattern"},{"categories":null,"content":"Event-bus pattern ","date":"2021-12-12","objectID":"/software-architectural-patterns/:7:0","series":null,"tags":["Software Architectural Patterns","Note"],"title":"Arch Patterns Note","uri":"/software-architectural-patterns/#event-bus-pattern"},{"categories":null,"content":"Model-view-controller pattern ","date":"2021-12-12","objectID":"/software-architectural-patterns/:8:0","series":null,"tags":["Software Architectural Patterns","Note"],"title":"Arch Patterns Note","uri":"/software-architectural-patterns/#model-view-controller-pattern"},{"categories":null,"content":"Blackboard pattern ","date":"2021-12-12","objectID":"/software-architectural-patterns/:9:0","series":null,"tags":["Software Architectural Patterns","Note"],"title":"Arch Patterns Note","uri":"/software-architectural-patterns/#blackboard-pattern"},{"categories":null,"content":"Interpreter pattern ","date":"2021-12-12","objectID":"/software-architectural-patterns/:10:0","series":null,"tags":["Software Architectural Patterns","Note"],"title":"Arch Patterns Note","uri":"/software-architectural-patterns/#interpreter-pattern"},{"categories":null,"content":"CI Tools Bamboo run multiple builds in parallel for faster compilation built in functionality to connect with repos and has built tasks for Ant, Maven, etc. Good for deployment on diff envs. Builtbot open source written in py support distribution Apache Gump Great for java Travis CI a hosted, continuous integration service used to built and test software projs hosted at GitHub. for team of all sizes support 20 diff langs Jenkins open source written in java most popular Jenkins Easy installation Easy config Plug-ins Extensible (extend it not create a new version) Distributed Jenkins Pipline Dev–\u003ecode commit–\u003eBuild–\u003eTest–\u003eRelease–\u003eDeploy/Deliver–\u003eProduction Jenkins arch ","date":"2021-12-12","objectID":"/ci-tools/:0:0","series":null,"tags":["CI","Note"],"title":"CI Tools","uri":"/ci-tools/#"},{"categories":null,"content":"Design Patterns For OOL ","date":"2021-12-12","objectID":"/design-patterns-for-ool/:0:0","series":null,"tags":["Design Patterns","Note"],"title":"Design Patterns For OOL","uri":"/design-patterns-for-ool/#design-patterns-for-ool"},{"categories":null,"content":"Terms Association Aggregation implies a relationship where the child can exist independently of the parent. Example: Class (parent) and Student (child). Delete the Class and the Students still exist. Composition implies a relationship where the child cannot exist independent of the parent. Example: House (parent) and Room (child). Rooms don’t exist separate to a House. ","date":"2021-12-12","objectID":"/design-patterns-for-ool/:0:1","series":null,"tags":["Design Patterns","Note"],"title":"Design Patterns For OOL","uri":"/design-patterns-for-ool/#terms"},{"categories":null,"content":"SOLID Design Principles S: Single-responsibility principle O: Open-closed principle L: Liskov’s substitution principle I: Interface segregation principle D: Dependency inversion principle There are mainly 3 catagories for design patterns. Creational: Diff ways to create objs Structural: Relationships btw objs Behavioural: Interactions and comm. btw objs There are more undocumented design patterns existed. We can use design patterns to comm. with dev at a more abstract lvl. And make reusable, extensible, maintainable software. ","date":"2021-12-12","objectID":"/design-patterns-for-ool/:1:0","series":null,"tags":["Design Patterns","Note"],"title":"Design Patterns For OOL","uri":"/design-patterns-for-ool/#solid-design-principles"},{"categories":null,"content":"Strategy Pattern The strategy pattern defines a family of algorithms and encapsulate each one, and makes them interchangeable.Strategy let the algorithms very independently found the clients who use it. (If you want to change the algorithems, you don’t need to change the client.) https://www.youtube.com/watch?v=OMPfEXIlTVE\u0026t=1782s Null Object Pattern The key idea of strategy pattern: Compostion \u0026 Injection Inheritance is useful for sepecialization not for sharing codes! Name the concepts using spread sheet. Class data ??? ???? House data nothing? RandomHouse data shuffle EchoHouse data ?? duplicate nothing? is not a nothing, it is a algorithm. ??? is the order! Order is the role of this, and nothing? is the default order. So, we can do this: class House attr_reader :data def initialize(orderer: DefaultOrder.new) @data = orderer.order(DATA) end #... end class DefaultOrder def order(data) data end end class RandomOrder def order(data) data.shuffle end end Similarly, we can get this: Class data order format House data default default RandomHouse data shuffle default EchoHouse data default duplicate We can make the design neat and elegant by analysis. Just creating subclasses for every object having some unique behavior is not the solution. Now we realize the same goal with less code and zero duplication. We isolate the differences and name the concepts and define that role and APIs, then inject into the object. Make good use of the “Active Nothing” (do not search for the null everytime, let the object stand in for that since “Nothing is always something\"). Composition + Denpendency Injection For example, there are abstract class Dog with several methods. Then there are 2 subclasses sharing some methods that the super class does not have. Here we should use interface instead of inheritance. And then implement the interface differently and apply accordingly to each subclass of dog. When you use inheritance, though the subclass don’t have that much methods, the instance of the subclass can be large since all hidden methods from the super class are also there. Pro: Prevents the conditional statements. (switch, if, else…) The algorithms are loosely coupled with the context entity. They can be changed/replaced without changing the context entity. Very easy extendable. Con: Clients must know existence of different strategies and a client must understand how the Strategies differ It increases the number of objects in the application. ","date":"2021-12-12","objectID":"/design-patterns-for-ool/:2:0","series":null,"tags":["Design Patterns","Note"],"title":"Design Patterns For OOL","uri":"/design-patterns-for-ool/#strategy-pattern"},{"categories":null,"content":"Observer Pattern Push vs. Poll Think about that we now have a wether station and a client like a phone. Now we want to know if there are any updated news. One way to do this is through polling. The phone will simply query the data periodically. It consumes a lot of resources to keep asking for the changes. (There might be millions of clients need the data) The other way is to let the phone subscribe the weather station which will notify the phone whenever the data get updated. And this solvement leans to the concept of pushing. The clients are observers, and the weather station is the observerable. The weather station has the following methods: add(…) remove(…) notify() The client has the following methods: update() We should notice that in this case, the client may “has-a” weather station. And we know that usually this relationship only appear on the higher abstraction level rather than the implementation level. The reason is that we have to pass in the weather station object into the constructor of the client so that when the client call the update() method where no arguments are passed in, it know how to access the data from the weather station. In the notify, we iterate through the list of subscribers and call the update() method of it. There are a lot of vairiants of ob pattern. You can do it in push\u0026pull, poll\u0026push, and etc. Please google it if you forget. For the push\u0026poll, we need to pass in the type of the observable into the constructor of the observer so that the implemented instances can access the data of the corresponding observables without creating abstractions for every type of the observables. Check Christopher Okhravi’s youtube vids if you forget everything. Pro: Con: You might end up with complicated nested event-driven structure, a event send signal to a event that send a sigal back to the first event, and there could be more complex structure in the middle. ","date":"2021-12-12","objectID":"/design-patterns-for-ool/:3:0","series":null,"tags":["Design Patterns","Note"],"title":"Design Patterns For OOL","uri":"/design-patterns-for-ool/#observer-pattern"},{"categories":null,"content":"Decorator Pattern You have an object, we send a message like speak() to the object and expect a string returned. If we want to change the object’s behavior without change the code of the object itself, we can use a decorator to wrap it up, which is like an outer object. Now we send the message to the decorator and it sends the mesage to the object. Then the object send the string to the decorator, the decorator processes the result and send it to us. The decorator has a component and is a component. The decorators is the same type of its inside while the decorators are also the wrappers of the inside. We say the decorator is a its component because we want to treat it as it was the same thing. We say the decorator has a its component because you want to send the message downward and get them back. Each level of the decorators attach additional responsibilities to an object dynamically. Decorator pattern provides flexibility for the subclasses to extend functionalities. At runtime, you cannot create a object combine subclass A and B but you can use decorator X and Y to combine them. We have a abstract class Beverage, and we also have bunch of subclasses such as Decaf, Mocha, Steamed, Milk.., We cannot create a subclass for each of the combination since those would be too many and each of them just do some tiny work. Most importantly, they are unmanageable. Attempt: All these options can be treated as bools. And we can include those bools throught setters in the superclass. Probs: But this is extremely unexpressive, and when we have such conditionals we will put ourselves in a place that needs more conditionals! (conditional breed) And if we want to expand the options, we have to modify the superclass so that many other subclass will also get this functions inherited they’ll never use, which disobeys the SOLID principles. Return a boolean usually invoke a conditional statement else where in your code. Attempt: Now introducing =\u003e Decoraaaaators Abstract class Beverage has getDress() cost() class Decaf class Espresso Addon Decorator is a Beverage and has a Beverage. And it has getDress() And it has bunch of implementations like CaramelDeco, SoyDeco No matter how many decorators you want, just add it as an additional outer wrapper. Notice here the Addon Decorator has the getDress() function so that it acts like the Beverage, and the fact that it is the outer decorator make it has a Beverage inside. Please note that in this case the decorator pattern may not be the best implementation but this case illustrates the concepts very well. The value passing downward and upward is through recursion. For example, the cost() here. The decorator calls the cost() recursively so that when it reaches the base case, which is when it reaches the cost() of Decaf or Espresso, the value get returned from inside to outside. There might be multiple decorators but each of them just think it is wrapping up only one thing. abstract class Beverage{ public abstract int cost(); } abstract class AddonDecorator: Beverage{ public abstract int cost(); } class Espresso: Beverage{ public abstract int cost(){ return 1; } } class CaramelDeco: AddonDecorator{ Beverage beverage; int price = 2; public CaramelDeco(Beverage b) { this.beverage = b; } public int cost() { return this.beverage.cost() + this.price; } } Frankly, iteration pattern would be more properate to calculate the total cost, since the cost() in each decorator behaves the same. A better example would be buffterAlignmentDeco –\u003e …non-deco or deco… –\u003e bufferInputDeco –\u003e inputString This is better since: They are working on the same type: String Each decorator behaves differently ","date":"2021-12-12","objectID":"/design-patterns-for-ool/:4:0","series":null,"tags":["Design Patterns","Note"],"title":"Design Patterns For OOL","uri":"/design-patterns-for-ool/#decorator-pattern"},{"categories":null,"content":"Factory Pattern ","date":"2021-12-12","objectID":"/design-patterns-for-ool/:5:0","series":null,"tags":["Design Patterns","Note"],"title":"Design Patterns For OOL","uri":"/design-patterns-for-ool/#factory-pattern"},{"categories":["note"],"content":"Why Docker? Your proj can only run on your machine but not others. One or more file missing. Software version mismatch. Diff config setting. Docker can pack everything in an isolated env and make it run anywhere. And diff docker containers can run on same machine. HYPERVISORS VitualBox VMware Hyper-v (win only) Probs With Virtual Machines Each VM needs a full-blown OS Slow to start Resource intensive (it takes a slice of the hardware) Containers Allow running multiple apps in isolation Lightweight Use OS of the host (all containers share the same host OS) Start quickly Need less hardware resource (share the host resources so that we don’t need to give cores to it specifically) ","date":"2021-12-12","objectID":"/docker/:0:1","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#why-docker"},{"categories":["note"],"content":"Why Docker? Your proj can only run on your machine but not others. One or more file missing. Software version mismatch. Diff config setting. Docker can pack everything in an isolated env and make it run anywhere. And diff docker containers can run on same machine. HYPERVISORS VitualBox VMware Hyper-v (win only) Probs With Virtual Machines Each VM needs a full-blown OS Slow to start Resource intensive (it takes a slice of the hardware) Containers Allow running multiple apps in isolation Lightweight Use OS of the host (all containers share the same host OS) Start quickly Need less hardware resource (share the host resources so that we don’t need to give cores to it specifically) ","date":"2021-12-12","objectID":"/docker/:0:1","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#hypervisors"},{"categories":["note"],"content":"Why Docker? Your proj can only run on your machine but not others. One or more file missing. Software version mismatch. Diff config setting. Docker can pack everything in an isolated env and make it run anywhere. And diff docker containers can run on same machine. HYPERVISORS VitualBox VMware Hyper-v (win only) Probs With Virtual Machines Each VM needs a full-blown OS Slow to start Resource intensive (it takes a slice of the hardware) Containers Allow running multiple apps in isolation Lightweight Use OS of the host (all containers share the same host OS) Start quickly Need less hardware resource (share the host resources so that we don’t need to give cores to it specifically) ","date":"2021-12-12","objectID":"/docker/:0:1","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#probs-with-virtual-machines"},{"categories":["note"],"content":"Why Docker? Your proj can only run on your machine but not others. One or more file missing. Software version mismatch. Diff config setting. Docker can pack everything in an isolated env and make it run anywhere. And diff docker containers can run on same machine. HYPERVISORS VitualBox VMware Hyper-v (win only) Probs With Virtual Machines Each VM needs a full-blown OS Slow to start Resource intensive (it takes a slice of the hardware) Containers Allow running multiple apps in isolation Lightweight Use OS of the host (all containers share the same host OS) Start quickly Need less hardware resource (share the host resources so that we don’t need to give cores to it specifically) ","date":"2021-12-12","objectID":"/docker/:0:1","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#containers"},{"categories":["note"],"content":"Arch of Docker ","date":"2021-12-12","objectID":"/docker/:0:2","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#arch-of-docker"},{"categories":["note"],"content":"Installing # docs.docker.com/get-docker/ # open dockerHub docker version ","date":"2021-12-12","objectID":"/docker/:0:3","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#installing"},{"categories":["note"],"content":"Dev Workflow The images get loaded in a container. docker run appName to run it in a container. ","date":"2021-12-12","objectID":"/docker/:0:4","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#dev-workflow"},{"categories":["note"],"content":"Docker in Action mkdir hello-docker cd hello-docker code . # open in vsCode, you can use others like vim add a new file -\u003e app.js add one line -\u003e console.log(\"Hello Docker!\"); You need Node installed. node app.js then it should print Hello Docker! in terminal Instructions start with an others install Node Copy app files Run node app.js BUT NOW we can write this instructions in the dockerfile! add another file named Dockerfile with no extensions. install the docker extension for vsCode. FROMnode:alpineCOPY . /app # we gonna copy all the files in the current dir to the /app of that imageCMD node /app/app.jsor useWORKDIR/appCMD node app.js# go to dockerHub to find these image like linux or node etc.# after the : is the tag of the image, for example alpine is a specific ver of linux docker build -t hello-docker . # -t for tag that to identify the image # then the app name # then the dir to find it notice that the image is not in the app dir. docker image ls ![Docker Image(latest) with Node, Linux alpine, and the app files](/img/Docker_Image(latest)_with_Node,_Linux alpine,_and_the_app_files.png) docker run hello-docker run it using image name You can publish it on dockerhub to run it anywhere. you can also go to play with docker to have a try. docker run PceWlkr/hello-docker ","date":"2021-12-12","objectID":"/docker/:0:5","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#docker-in-action"},{"categories":["note"],"content":"Docker in Action mkdir hello-docker cd hello-docker code . # open in vsCode, you can use others like vim add a new file - app.js add one line - console.log(\"Hello Docker!\"); You need Node installed. node app.js then it should print Hello Docker! in terminal Instructions start with an others install Node Copy app files Run node app.js BUT NOW we can write this instructions in the dockerfile! add another file named Dockerfile with no extensions. install the docker extension for vsCode. FROMnode:alpineCOPY . /app # we gonna copy all the files in the current dir to the /app of that imageCMD node /app/app.jsor useWORKDIR/appCMD node app.js# go to dockerHub to find these image like linux or node etc.# after the : is the tag of the image, for example alpine is a specific ver of linux docker build -t hello-docker . # -t for tag that to identify the image # then the app name # then the dir to find it notice that the image is not in the app dir. docker image ls ![Docker Image(latest) with Node, Linux alpine, and the app files](/img/Docker_Image(latest)_with_Node,_Linux alpine,_and_the_app_files.png) docker run hello-docker run it using image name You can publish it on dockerhub to run it anywhere. you can also go to play with docker to have a try. docker run PceWlkr/hello-docker ","date":"2021-12-12","objectID":"/docker/:0:5","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#instructions"},{"categories":["note"],"content":"Docker in Action mkdir hello-docker cd hello-docker code . # open in vsCode, you can use others like vim add a new file - app.js add one line - console.log(\"Hello Docker!\"); You need Node installed. node app.js then it should print Hello Docker! in terminal Instructions start with an others install Node Copy app files Run node app.js BUT NOW we can write this instructions in the dockerfile! add another file named Dockerfile with no extensions. install the docker extension for vsCode. FROMnode:alpineCOPY . /app # we gonna copy all the files in the current dir to the /app of that imageCMD node /app/app.jsor useWORKDIR/appCMD node app.js# go to dockerHub to find these image like linux or node etc.# after the : is the tag of the image, for example alpine is a specific ver of linux docker build -t hello-docker . # -t for tag that to identify the image # then the app name # then the dir to find it notice that the image is not in the app dir. docker image ls ![Docker Image(latest) with Node, Linux alpine, and the app files](/img/Docker_Image(latest)_with_Node,_Linux alpine,_and_the_app_files.png) docker run hello-docker run it using image name You can publish it on dockerhub to run it anywhere. you can also go to play with docker to have a try. docker run PceWlkr/hello-docker ","date":"2021-12-12","objectID":"/docker/:0:5","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#but-now-we-can-write-this-instructions-in-the-dockerfile"},{"categories":["note"],"content":"Linux Distros Ubuntu Debian Alpine Fedora CentOS Ubuntu go to dockerhub, search for Ubuntu. we can use docker pull ubuntu but here I will use a shortcut we can use docker run ubuntu, if docker cannot find it locally then it will download it and run it in a container. However, since we didn’t interact with the container, it stopped. if we docker ps we cannot see it if we docker ps -a we can see it’s stopped We need to use docker run -it ubuntu to start it in interactive mode And then we entered its shell. root@2f856e34654e5:/# root indicates I’m having the root user mode having the highest access permission After the @ is the name of the container. The / means the dir we are at. And there is a # meaning we have the highest privilege since we are in root mode. whoami echo $0 # /bin/bash # we can see the location of shell program history !2 # will run the 2nd command in history Package Manager apt list - list oackages based on names search - search in package descriptions show - show package details install reinstall remove autoremove - remove automatically all unused packages update - update list of available packages upgrade - upgrade the sys by installing/upgrading packages full-upgrade - upgrade the sys by removing/installing/upgrading edit-sources - edit the source information file satisfy - satisfy dependency strings apt-get - look it up yourself LMAO apt install nano will give a error msg since you may not have nano in your package list apt update to update the package database apt install nano ","date":"2021-12-12","objectID":"/docker/:0:6","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#linux-distros"},{"categories":["note"],"content":"Linux Distros Ubuntu Debian Alpine Fedora CentOS Ubuntu go to dockerhub, search for Ubuntu. we can use docker pull ubuntu but here I will use a shortcut we can use docker run ubuntu, if docker cannot find it locally then it will download it and run it in a container. However, since we didn’t interact with the container, it stopped. if we docker ps we cannot see it if we docker ps -a we can see it’s stopped We need to use docker run -it ubuntu to start it in interactive mode And then we entered its shell. root@2f856e34654e5:/# root indicates I’m having the root user mode having the highest access permission After the @ is the name of the container. The / means the dir we are at. And there is a # meaning we have the highest privilege since we are in root mode. whoami echo $0 # /bin/bash # we can see the location of shell program history !2 # will run the 2nd command in history Package Manager apt list - list oackages based on names search - search in package descriptions show - show package details install reinstall remove autoremove - remove automatically all unused packages update - update list of available packages upgrade - upgrade the sys by installing/upgrading packages full-upgrade - upgrade the sys by removing/installing/upgrading edit-sources - edit the source information file satisfy - satisfy dependency strings apt-get - look it up yourself LMAO apt install nano will give a error msg since you may not have nano in your package list apt update to update the package database apt install nano ","date":"2021-12-12","objectID":"/docker/:0:6","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#ubuntu"},{"categories":["note"],"content":"Linux Distros Ubuntu Debian Alpine Fedora CentOS Ubuntu go to dockerhub, search for Ubuntu. we can use docker pull ubuntu but here I will use a shortcut we can use docker run ubuntu, if docker cannot find it locally then it will download it and run it in a container. However, since we didn’t interact with the container, it stopped. if we docker ps we cannot see it if we docker ps -a we can see it’s stopped We need to use docker run -it ubuntu to start it in interactive mode And then we entered its shell. root@2f856e34654e5:/# root indicates I’m having the root user mode having the highest access permission After the @ is the name of the container. The / means the dir we are at. And there is a # meaning we have the highest privilege since we are in root mode. whoami echo $0 # /bin/bash # we can see the location of shell program history !2 # will run the 2nd command in history Package Manager apt list - list oackages based on names search - search in package descriptions show - show package details install reinstall remove autoremove - remove automatically all unused packages update - update list of available packages upgrade - upgrade the sys by installing/upgrading packages full-upgrade - upgrade the sys by removing/installing/upgrading edit-sources - edit the source information file satisfy - satisfy dependency strings apt-get - look it up yourself LMAO apt install nano will give a error msg since you may not have nano in your package list apt update to update the package database apt install nano ","date":"2021-12-12","objectID":"/docker/:0:6","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#package-manager"},{"categories":["note"],"content":"File System bin: binaries boot: booting related dev: devices, in linux, everything is a file etc: editable text configuration home: the home dir of users root: the home dir of root user lib: libs, e.g. software dependencies var: varibles, updated frequently, like logs or app data proc: running processes (as files) pwd print current dir ls ls -1 one item per line ls -l full info list cd ../.. 2 level up cd ~ mv hello.txt /etc hello123.txt notice that \u003cctrl + w\u003e remove one word in shell rm file1.txt file2.txt rm file* rm -r fileDir/ touch file1.txt cat file1.txt more fileLong.txt use more if the file is long. we can use space or enter apt install less less file1.txt we can you up and down and space and enter head -n 5 file1.txt tail -n 2 file2.txt Redirectory cat file1.txt \u003e file2.txt cat file1.txt file2.txt \u003e combined.txt echo hello \u003e hello.txt ls -l /etc \u003e files.txt \u003c Redirectory for input ","date":"2021-12-12","objectID":"/docker/:0:7","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#file-system"},{"categories":["note"],"content":"File System bin: binaries boot: booting related dev: devices, in linux, everything is a file etc: editable text configuration home: the home dir of users root: the home dir of root user lib: libs, e.g. software dependencies var: varibles, updated frequently, like logs or app data proc: running processes (as files) pwd print current dir ls ls -1 one item per line ls -l full info list cd ../.. 2 level up cd ~ mv hello.txt /etc hello123.txt notice that remove one word in shell rm file1.txt file2.txt rm file* rm -r fileDir/ touch file1.txt cat file1.txt more fileLong.txt use more if the file is long. we can use space or enter apt install less less file1.txt we can you up and down and space and enter head -n 5 file1.txt tail -n 2 file2.txt Redirectory cat file1.txt file2.txt cat file1.txt file2.txt combined.txt echo hello hello.txt ls -l /etc files.txt ","date":"2021-12-12","objectID":"/docker/:0:7","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#redirectory"},{"categories":["note"],"content":"Demo Project ","date":"2021-12-12","objectID":"/docker/:1:0","series":null,"tags":["Docker","Note"],"title":"Docker_note","uri":"/docker/#demo-project"},{"categories":null,"content":"Git ","date":"2021-12-12","objectID":"/gitreview/:0:0","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#git"},{"categories":null,"content":"First Run #need sudo /etc/gitconfig git config --system #work on all git repos of the current user, ~/.gitconfig 或 ~/.config/git/config git config --global #this is by default. If you are at a git repo, this r/w .git/config for this repo git config --local #每一个级别会覆盖上一级别的配置 #check all the config and their paths git config --list --show-origin 首先设置用户名和邮件地址，每一次commit都会写入信息，且无法更改。 git config --global user.name \"John Doe\" git config --global user.email johndoe@example.com 设置编辑器 $ git config --global core.editor emacs # or $ git config --global core.editor \"'C:/Program Files/Notepad++/notepad++.exe' -multiInst -notabbar -nosession -noPlugin\" 检查配置 git config --list 输入 git config \u003ckey\u003e： 来检查 Git 的某一项配置 $ git config user.name John Doe git \u003cverb\u003e --help man git-\u003cverb\u003e ","date":"2021-12-12","objectID":"/gitreview/:1:0","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#first-run"},{"categories":null,"content":"Init ","date":"2021-12-12","objectID":"/gitreview/:2:0","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#init"},{"categories":null,"content":"For an Existing Dir for Linux: $ cd /home/user/my_project for macOS: $ cd /Users/user/my_project for Windows: $ cd C:/Users/user/my_project and type: $ git init 如果在一个已存在文件的文件夹（而非空文件夹）中进行版本控制，你应该开始追踪这些文件并进行初始提交。 可以通过 git add 命令来指定所需的文件来进行追踪，然后执行 git commit ： $ git add *.c $ git add LICENSE $ git commit -m 'initial project version' ","date":"2021-12-12","objectID":"/gitreview/:2:1","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#for-an-existing-dir"},{"categories":null,"content":"Clone 克隆仓库的命令是 git clone \u003curl\u003e 。 比如，要克隆 Git 的链接库 libgit2，可以用下面的命令： $ git clone https://github.com/libgit2/libgit2 如果你想在克隆远程仓库的时候，自定义本地仓库的名字，你可以通过额外的参数指定新的目录名： $ git clone https://github.com/libgit2/libgit2 mylibgit 这会执行与上一条命令相同的操作，但目标目录名变为了 mylibgit。 Git 支持多种数据传输协议。 上面的例子使用的是 https:// 协议，不过你也可以使用 git:// 协议或者使用 SSH 传输协议，比如 user@server:path/to/repo.git 。 在服务器上搭建 Git 将会介绍所有这些协议在服务器端如何配置使用，以及各种方式之间的利弊。 ","date":"2021-12-12","objectID":"/gitreview/:2:2","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#clone"},{"categories":null,"content":"Recording Changes to the Repository ","date":"2021-12-12","objectID":"/gitreview/:3:0","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#recording-changes-to-the-repository"},{"categories":null,"content":"检查当前文件状态 可以用 git status 命令查看哪些文件处于什么状态。 如果在克隆仓库后立即使用此命令，会看到类似这样的输出： $ git status On branch master Your branch is up-to-date with 'origin/master'. nothing to commit, working directory clean 现在，让我们在项目下创建一个新的 README 文件。 如果之前并不存在这个文件，使用 git status 命令，你将看到一个新的未跟踪文件： $ echo 'My Project' \u003e README $ git status On branch master Your branch is up-to-date with 'origin/master'. Untracked files: (use \"git add \u003cfile\u003e...\" to include in what will be committed) README nothing added to commit but untracked files present (use \"git add\" to track) 使用命令 git add 开始跟踪一个文件。 所以，要跟踪 README 文件，运行： $ git add README ","date":"2021-12-12","objectID":"/gitreview/:3:1","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#检查当前文件状态"},{"categories":null,"content":"暂存已修改的文件 git add CONTRIBUTING.md ","date":"2021-12-12","objectID":"/gitreview/:3:2","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#暂存已修改的文件"},{"categories":null,"content":"状态简览 git status 命令的输出十分详细，但其用语有些繁琐。 Git 有一个选项可以帮你缩短状态命令的输出，这样可以以简洁的方式查看更改。 如果你使用 git status -s 命令或 git status --short 命令，你将得到一种格式更为紧凑的输出。 $ git status -s M README MM Rakefile A lib/git.rb M lib/simplegit.rb ?? LICENSE.txt ","date":"2021-12-12","objectID":"/gitreview/:3:3","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#状态简览"},{"categories":null,"content":"忽略文件 一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。 通常都是些自动生成的文件，比如日志文件，或者编译过程中创建的临时文件等。 在这种情况下，我们可以创建一个名为 .gitignore 的文件，列出要忽略的文件的模式。 来看一个实际的 .gitignore 例子： $ cat .gitignore *.[oa] *~ 第一行告诉 Git 忽略所有以 .o 或 .a 结尾的文件。一般这类对象文件和存档文件都是编译过程中出现的。 第二行告诉 Git 忽略所有名字以波浪符（~）结尾的文件，许多文本编辑软件（比如 Emacs）都用这样的文件名保存副本。 此外，你可能还需要忽略 log，tmp 或者 pid 目录，以及自动生成的文档等等。 要养成一开始就为你的新仓库设置好 .gitignore 文件的习惯，以免将来误提交这类无用的文件。 文件 .gitignore 的格式规范如下： 所有空行或者以 # 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配，它会递归地应用在整个工作区中。 匹配模式可以以（/）开头防止递归。 匹配模式可以以（/）结尾指定目录。 要忽略指定模式以外的文件或目录，可以在模式前加上叹号（!）取反。 所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。 星号（*）匹配零个或多个任意字符；[abc] 匹配任何一个列在方括号中的字符 （这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c）； 问号（?）只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符， 表示所有在这两个字符范围内的都可以匹配（比如 [0-9] 表示匹配所有 0 到 9 的数字）。 使用两个星号（**）表示匹配任意中间目录，比如 a/**/z 可以匹配 a/z 、 a/b/z 或 a/b/c/z 等。 我们再看一个 .gitignore 文件的例子： # 忽略所有的 .a 文件 *.a # 但跟踪所有的 lib.a，即便你在前面忽略了 .a 文件 !lib.a # 只忽略当前目录下的 TODO 文件，而不忽略 subdir/TODO /TODO # 忽略任何目录下名为 build 的文件夹 build/ # 忽略 doc/notes.txt，但不忽略 doc/server/arch.txt doc/*.txt # 忽略 doc/ 目录及其所有子目录下的 .pdf 文件 doc/**/*.pdf ","date":"2021-12-12","objectID":"/gitreview/:3:4","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#忽略文件"},{"categories":null,"content":"查看已暂存和未暂存的修改 如果 git status 命令的输出对于你来说过于简略，而你想知道具体修改了什么地方，可以用 git diff 命令。 要查看尚未暂存的文件更新了哪些部分，不加参数直接输入 git diff 此命令比较的是工作目录中当前文件和暂存区域快照之间的差异。 也就是修改之后还没有暂存起来的变化内容。 若要查看已暂存的将要添加到下次提交里的内容，可以用 git diff --staged 命令。 这条命令将比对已暂存文件与最后一次提交的文件差异： 请注意，git diff 本身只显示尚未暂存的改动，而不是自上次提交以来所做的所有改动。 所以有时候你一下子暂存了所有更新过的文件，运行 git diff 后却什么也没有，就是这个原因。 ","date":"2021-12-12","objectID":"/gitreview/:3:5","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#查看已暂存和未暂存的修改"},{"categories":null,"content":"提交更新 git commit 另外，你也可以在 commit 命令后添加 -m 选项，将提交信息与命令放在同一行，如下所示： $ git commit -m \"Story 182: Fix benchmarks for speed\" [master 463dc4f] Story 182: Fix benchmarks for speed 2 files changed, 2 insertions(+) create mode 100644 README 好，现在你已经创建了第一个提交！ 可以看到，提交后它会告诉你，当前是在哪个分支（master）提交的，本次提交的完整 SHA-1 校验和是什么（463dc4f），以及在本次提交中，有多少文件修订过，多少行添加和删改过。 请记住，提交时记录的是放在暂存区域的快照。 任何还未暂存文件的仍然保持已修改状态，可以在下次提交时纳入版本管理。 每一次运行提交操作，都是对你项目作一次快照，以后可以回到这个状态，或者进行比较。 ","date":"2021-12-12","objectID":"/gitreview/:3:6","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#提交更新"},{"categories":null,"content":"跳过使用暂存区域 尽管使用暂存区域的方式可以精心准备要提交的细节，但有时候这么做略显繁琐。 Git 提供了一个跳过使用暂存区域的方式， 只要在提交的时候，给 git commit 加上 -a 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤 ","date":"2021-12-12","objectID":"/gitreview/:3:7","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#跳过使用暂存区域"},{"categories":null,"content":"移除文件 要从 Git 中移除某个文件，就必须要从已跟踪文件清单中移除（确切地说，是从暂存区域移除），然后提交。 可以用 git rm 命令完成此项工作，并连带从工作目录中删除指定的文件，这样以后就不会出现在未跟踪文件清单中了。 如果要删除之前修改过或已经放到暂存区的文件，则必须使用强制删除选项 -f（译注：即 force 的首字母）。 这是一种安全特性，用于防止误删尚未添加到快照的数据，这样的数据不能被 Git 恢复。 另外一种情况是，我们想把文件从 Git 仓库中删除（亦即从暂存区域移除），但仍然希望保留在当前工作目录中。 换句话说，你想让文件保留在磁盘，但是并不想让 Git 继续跟踪。 当你忘记添加 .gitignore 文件，不小心把一个很大的日志文件或一堆 .a 这样的编译生成文件添加到暂存区时，这一做法尤其有用。 为达到这一目的，使用 --cached 选项： $ git rm --cached README git rm 命令后面可以列出文件或者目录的名字，也可以使用 glob 模式。比如： $ git rm log/\\*.log 注意到星号 * 之前的反斜杠 \\， 因为 Git 有它自己的文件模式扩展匹配方式，所以我们不用 shell 来帮忙展开。 此命令删除 log/ 目录下扩展名为 .log 的所有文件。 类似的比如： $ git rm \\*~ 该命令会删除所有名字以 ~ 结尾的文件。 ","date":"2021-12-12","objectID":"/gitreview/:3:8","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#移除文件"},{"categories":null,"content":"移动文件 不像其它的 VCS 系统，Git 并不显式跟踪文件移动操作。 如果在 Git 中重命名了某个文件，仓库中存储的元数据并不会体现出这是一次改名操作。 不过 Git 非常聪明，它会推断出究竟发生了什么，至于具体是如何做到的，我们稍后再谈。 既然如此，当你看到 Git 的 mv 命令时一定会困惑不已。 要在 Git 中对文件改名，可以这么做： $ git mv file_from file_to 其实，运行 git mv 就相当于运行了下面三条命令： $ mv README.md README $ git rm README.md $ git add README 如此分开操作，Git 也会意识到这是一次重命名，所以不管何种方式结果都一样。 两者唯一的区别在于，git mv 是一条命令而非三条命令，直接使用 git mv 方便得多。 不过在使用其他工具重命名文件时，记得在提交前 git rm 删除旧文件名，再 git add 添加新文件名。 ","date":"2021-12-12","objectID":"/gitreview/:3:9","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#移动文件"},{"categories":null,"content":"查看提交历史 ","date":"2021-12-12","objectID":"/gitreview/:0:0","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#查看提交历史"},{"categories":null,"content":"查看提交历史 在提交了若干更新，又或者克隆了某个项目之后，你也许想回顾下提交历史。 完成这个任务最简单而又有效的工具是 git log 命令。 git log 有许多选项可以帮助你搜寻你所要找的提交， 下面我们会介绍几个最常用的选项。 其中一个比较有用的选项是 -p 或 --patch ，它会显示每次提交所引入的差异（按 补丁 的格式输出）。 你也可以限制显示的日志条目数量，例如使用 -2 选项来只显示最近的两次提交 该选项除了显示基本信息之外，还附带了每次提交的变化。 当进行代码审查，或者快速浏览某个搭档的提交所带来的变化的时候，这个参数就非常有用了。 你也可以为 git log 附带一系列的总结性选项。 比如你想看到每次提交的简略统计信息，可以使用 --stat 选项： --stat 选项在每次提交的下面列出所有被修改过的文件、有多少文件被修改了以及被修改过的文件的哪些行被移除或是添加了。 在每次提交的最后还有一个总结。 另一个非常有用的选项是 --pretty。 这个选项可以使用不同于默认格式的方式展示提交历史。 这个选项有一些内建的子选项供你使用。 比如 oneline 会将每个提交放在一行显示，在浏览大量的提交时非常有用。 另外还有 short，full 和 fuller 选项，它们展示信息的格式基本一致，但是详尽程度不一 $ git log --pretty=oneline ca82a6dff817ec66f44342007202690a93763949 changed the version number 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7 removed unnecessary test a11bef06a3f659402fe7563abf99ad00de2209e6 first commit 最有意思的是 format ，可以定制记录的显示格式。 这样的输出对后期提取分析格外有用——因为你知道输出的格式不会随着 Git 的更新而发生改变： $ git log --pretty=format:\"%h - %an, %ar : %s\" ca82a6d - Scott Chacon, 6 years ago : changed the version number 085bb3b - Scott Chacon, 6 years ago : removed unnecessary test a11bef0 - Scott Chacon, 6 years ago : first commit git log --pretty=format 常用的选项 列出了 format 接受的常用格式占位符的写法及其代表的意义。 选项 说明 %H 提交的完整哈希值 %h 提交的简写哈希值 %T 树的完整哈希值 %t 树的简写哈希值 %P 父提交的完整哈希值 %p 父提交的简写哈希值 %an 作者名字 %ae 作者的电子邮件地址 %ad 作者修订日期（可以用 –date=选项 来定制格式） %ar 作者修订日期，按多久以前的方式显示 %cn 提交者的名字 %ce 提交者的电子邮件地址 %cd 提交日期 %cr 提交日期（距今多长时间） %s 提交说明 你一定奇怪 作者 和 提交者 之间究竟有何差别， 其实作者指的是实际作出修改的人，提交者指的是最后将此工作成果提交到仓库的人。 所以，当你为某个项目发布补丁，然后某个核心成员将你的补丁并入项目时，你就是作者，而那个核心成员就是提交者。 我们会在 分布式 Git 再详细介绍两者之间的细微差别。 当 oneline 或 format 与另一个 log 选项 --graph 结合使用时尤其有用。 这个选项添加了一些 ASCII 字符串来形象地展示你的分支、合并历史 选项 说明 -p 按补丁格式显示每个提交引入的差异。 --stat 显示每次提交的文件修改统计信息。 --shortstat 只显示 –stat 中最后的行数修改添加移除统计。 --name-only 仅在提交信息后显示已修改的文件清单。 --name-status 显示新增、修改、删除的文件清单。 --abbrev-commit 仅显示 SHA-1 校验和所有 40 个字符中的前几个字符。 --relative-date 使用较短的相对时间而不是完整格式显示日期（比如“2 weeks ago”）。 --graph 在日志旁以 ASCII 图形显示分支与合并历史。 --pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline、short、full、fuller 和 format（用来定义自己的格式）。 --oneline --pretty=oneline --abbrev-commit 合用的简写。 ","date":"2021-12-12","objectID":"/gitreview/:1:0","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#查看提交历史-1"},{"categories":null,"content":"限制输出长度 除了定制输出格式的选项之外，git log 还有许多非常实用的限制输出长度的选项，也就是只输出一部分的提交。 之前你已经看到过 -2 选项了，它只会显示最近的两条提交， 实际上，你可以使用类似 -\u003cn\u003e 的选项，其中的 n 可以是任何整数，表示仅显示最近的 n 条提交。 不过实践中这个选项不是很常用，因为 Git 默认会将所有的输出传送到分页程序中，所以你一次只会看到一页的内容。 但是，类似 --since 和 --until 这种按照时间作限制的选项很有用。 例如，下面的命令会列出最近两周的所有提交： $ git log --since=2.weeks 该命令可用的格式十分丰富——可以是类似 \"2008-01-15\" 的具体的某一天，也可以是类似 \"2 years 1 day 3 minutes ago\" 的相对日期。 还可以过滤出匹配指定条件的提交。 用 --author 选项显示指定作者的提交，用 --grep 选项搜索提交说明中的关键字。 另一个非常有用的过滤器是 -S（俗称“pickaxe”选项，取“用鹤嘴锄在土里捡石头”之意）， 它接受一个字符串参数，并且只会显示那些添加或删除了该字符串的提交。 假设你想找出添加或删除了对某一个特定函数的引用的提交，可以调用： $ git log -S function_name 最后一个很实用的 git log 选项是路径（path）， 如果只关心某些文件或者目录的历史提交，可以在 git log 选项的最后指定它们的路径。 因为是放在最后位置上的选项，所以用两个短划线（–）隔开之前的选项和后面限定的路径名。 在 限制 git log 输出的选项 中列出了常用的选项 选项 说明 -\u003cn\u003e 仅显示最近的 n 条提交。 --since, --after 仅显示指定时间之后的提交。 --until, --before 仅显示指定时间之前的提交。 --author 仅显示作者匹配指定字符串的提交。 --committer 仅显示提交者匹配指定字符串的提交。 --grep 仅显示提交说明中包含指定字符串的提交。 -S 仅显示添加或删除内容匹配指定字符串的提交。 来看一个实际的例子，如果要在 Git 源码库中查看 Junio Hamano 在 2008 年 10 月其间， 除了合并提交之外的哪一个提交修改了测试文件，可以使用下面的命令： $ git log --pretty=\"%h - %s\" --author='Junio C Hamano' --since=\"2008-10-01\" \\ --before=\"2008-11-01\" --no-merges -- t/ 5610e3b - Fix testcase failure when extended attributes are in use acd3b9e - Enhance hold_lock_file_for_{update,append}() API f563754 - demonstrate breakage of detached checkout with symbolic link HEAD d1a43f2 - reset --hard/read-tree --reset -u: remove unmerged new paths 51a94af - Fix \"checkout --track -b newbranch\" on detached HEAD b0ad11e - pull: allow \"git pull origin $something:$current_branch\" into an unborn branch 在近 40000 条提交中，上面的输出仅列出了符合条件的 6 条记录。 git log --all --decorate --oneline --graph ","date":"2021-12-12","objectID":"/gitreview/:1:1","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#限制输出长度"},{"categories":null,"content":"撤消操作 ","date":"2021-12-12","objectID":"/gitreview/:0:0","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#撤消操作"},{"categories":null,"content":"远程仓库的使用 ","date":"2021-12-12","objectID":"/gitreview/:0:0","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#远程仓库的使用"},{"categories":null,"content":"打标签 ","date":"2021-12-12","objectID":"/gitreview/:0:0","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#打标签"},{"categories":null,"content":"Git 别名 ","date":"2021-12-12","objectID":"/gitreview/:0:0","series":null,"tags":["Git","Note"],"title":"Git Review","uri":"/gitreview/#git-别名"},{"categories":null,"content":"Golang why golang c/c++ fast but have to carry historical burdens, slow compile java kinda same python, easy to use but slow. None of these are developed toward to the best performance in terms of concurrency Strong and staticaaly typed excellent comm. Key features simplicity fast compile times garbage collected compile to standaline binaries(all dependencies are in there) built-in concurrency package main // have to specify the package name import ( \"fmt\" ) func main() { fmt.Println(\"Hello, playground\") } ","date":"2021-12-12","objectID":"/golang/:0:0","series":null,"tags":["Golang","Note"],"title":"Golang Note","uri":"/golang/#golang"},{"categories":null,"content":"Set up the env if you don’t install it in the default route, you may want to set the GOROOT and GOPATH in the bash file. GOROOT is where you golang are installed. GOPATH is where your files and binaries are located. But you can set up 2 GOPATH, the first one is going to be used for go get to hold all files or 3rd party go libs, while both are searched for source code. So it helps us to set up the workspace. In a workspace, we have src for sourec code, we have bin for binaries, and pkg for intermediate binaries liked some 3rd party libs you want to integrated into your project. ","date":"2021-12-12","objectID":"/golang/:1:0","series":null,"tags":["Golang","Note"],"title":"Golang Note","uri":"/golang/#set-up-the-env"},{"categories":null,"content":"Complile go run src/github.com/PceWlkr/fisrtapp/Main.go go build github.com/PceWlkr/firstapp // use the package addr. it will look into it if there is a main.go ./firstapp go install github.com/PceWlkr/firstapp ./bin/firstapp Notice there might be some changes since the build and install are not working for me with the package path. ","date":"2021-12-12","objectID":"/golang/:2:0","series":null,"tags":["Golang","Note"],"title":"Golang Note","uri":"/golang/#complile"},{"categories":null,"content":"Complile go run src/github.com/PceWlkr/fisrtapp/Main.go go build github.com/PceWlkr/firstapp // use the package addr. it will look into it if there is a main.go ./firstapp go install github.com/PceWlkr/firstapp ./bin/firstapp Notice there might be some changes since the build and install are not working for me with the package path. ","date":"2021-12-12","objectID":"/golang/:2:0","series":null,"tags":["Golang","Note"],"title":"Golang Note","uri":"/golang/#notice-there-might-be-some-changes-since-the-build-and-install-are-not-working-for-me-with-the-package-path"},{"categories":null,"content":"Variables package main import( \"fmt\" ) var i int 42 // you have to use this syntax if you declare it on package level var I int 54 // upper case var declared at package level is exposed to the ouside of the package while lower case vars declared at package level stays in the package scope var ( str string = \"my name\" num int = 3 ) func main(){ var o int // sometime you don't want a init, and this is in the block scope o = 42 // or var j float32 = 27. // or k := 99 i := 1 // error: you cannot delcare the same var in the same scope for twice i = 1 // this is OK var i int = 22 // This is also OK, but the package level i is hidden now, we can only the local i here var p int = 0; // this will throw a error, you cannot just declare it without using in your code fmt.Println(i) fmt.Printf(\"%v, %T, %T\", j, j, k) // will print 27, float32, float64 } var i int = 42 var j float32 j = float32(i) var i int = 42 var j string j = string(i) // the j here will be * // since it will look up the unicode for 42 and it's * j = strconv.Itoa(i) // we need to use this to convert int to string you want ","date":"2021-12-12","objectID":"/golang/:3:0","series":null,"tags":["Golang","Note"],"title":"Golang Note","uri":"/golang/#variables"},{"categories":null,"content":"https://www.hacksplaining.com/lessons ","date":"2021-12-12","objectID":"/hackspaining/:0:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#"},{"categories":null,"content":"SQL Injection ","date":"2021-12-12","objectID":"/hackspaining/:1:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#sql-injection"},{"categories":null,"content":"Harm Extract sensitive information, like Social Security numbers, or credit card details. Enumerate the authentication details of users registered on a website, so these logins can be used in attacks on other sites. Delete data or drop tables, corrupting the database, and making the website unusable. Inject further malicious code to be executed when users visit the site. Parameterized Statements Make sure that the parameters passed into SQL statements are treated in a safe manner. Object Relational Mapping ORM makes the translation of SQL result sets into code objects more seamless. Using an ORM does not automatically make you immune to SQL injection, however. As a general rule of thumb: if you find yourself writing SQL statements by concatenating strings, think very carefully about what you are doing. Escaping Inputs Programming languages have standard ways to describe strings containing quotes within them – SQL is no different in this respect. Typically, doubling up the quote character – replacing ' with '' – means “treat this quote as part of the string, not the end of the string”. Sanitizing Inputs Client-side validation (i.e. in JavaScript) is useful for giving the user immediate feedback when filling out a form, but is no defense against a serious hacker. Most hack attempts are performed using scripts, rather than the browser itself. ","date":"2021-12-12","objectID":"/hackspaining/:1:1","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#harm"},{"categories":null,"content":"Harm Extract sensitive information, like Social Security numbers, or credit card details. Enumerate the authentication details of users registered on a website, so these logins can be used in attacks on other sites. Delete data or drop tables, corrupting the database, and making the website unusable. Inject further malicious code to be executed when users visit the site. Parameterized Statements Make sure that the parameters passed into SQL statements are treated in a safe manner. Object Relational Mapping ORM makes the translation of SQL result sets into code objects more seamless. Using an ORM does not automatically make you immune to SQL injection, however. As a general rule of thumb: if you find yourself writing SQL statements by concatenating strings, think very carefully about what you are doing. Escaping Inputs Programming languages have standard ways to describe strings containing quotes within them – SQL is no different in this respect. Typically, doubling up the quote character – replacing ' with '' – means “treat this quote as part of the string, not the end of the string”. Sanitizing Inputs Client-side validation (i.e. in JavaScript) is useful for giving the user immediate feedback when filling out a form, but is no defense against a serious hacker. Most hack attempts are performed using scripts, rather than the browser itself. ","date":"2021-12-12","objectID":"/hackspaining/:1:1","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#parameterized-statements"},{"categories":null,"content":"Harm Extract sensitive information, like Social Security numbers, or credit card details. Enumerate the authentication details of users registered on a website, so these logins can be used in attacks on other sites. Delete data or drop tables, corrupting the database, and making the website unusable. Inject further malicious code to be executed when users visit the site. Parameterized Statements Make sure that the parameters passed into SQL statements are treated in a safe manner. Object Relational Mapping ORM makes the translation of SQL result sets into code objects more seamless. Using an ORM does not automatically make you immune to SQL injection, however. As a general rule of thumb: if you find yourself writing SQL statements by concatenating strings, think very carefully about what you are doing. Escaping Inputs Programming languages have standard ways to describe strings containing quotes within them – SQL is no different in this respect. Typically, doubling up the quote character – replacing ' with '' – means “treat this quote as part of the string, not the end of the string”. Sanitizing Inputs Client-side validation (i.e. in JavaScript) is useful for giving the user immediate feedback when filling out a form, but is no defense against a serious hacker. Most hack attempts are performed using scripts, rather than the browser itself. ","date":"2021-12-12","objectID":"/hackspaining/:1:1","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#object-relational-mapping"},{"categories":null,"content":"Harm Extract sensitive information, like Social Security numbers, or credit card details. Enumerate the authentication details of users registered on a website, so these logins can be used in attacks on other sites. Delete data or drop tables, corrupting the database, and making the website unusable. Inject further malicious code to be executed when users visit the site. Parameterized Statements Make sure that the parameters passed into SQL statements are treated in a safe manner. Object Relational Mapping ORM makes the translation of SQL result sets into code objects more seamless. Using an ORM does not automatically make you immune to SQL injection, however. As a general rule of thumb: if you find yourself writing SQL statements by concatenating strings, think very carefully about what you are doing. Escaping Inputs Programming languages have standard ways to describe strings containing quotes within them – SQL is no different in this respect. Typically, doubling up the quote character – replacing ' with '' – means “treat this quote as part of the string, not the end of the string”. Sanitizing Inputs Client-side validation (i.e. in JavaScript) is useful for giving the user immediate feedback when filling out a form, but is no defense against a serious hacker. Most hack attempts are performed using scripts, rather than the browser itself. ","date":"2021-12-12","objectID":"/hackspaining/:1:1","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#escaping-inputs"},{"categories":null,"content":"Harm Extract sensitive information, like Social Security numbers, or credit card details. Enumerate the authentication details of users registered on a website, so these logins can be used in attacks on other sites. Delete data or drop tables, corrupting the database, and making the website unusable. Inject further malicious code to be executed when users visit the site. Parameterized Statements Make sure that the parameters passed into SQL statements are treated in a safe manner. Object Relational Mapping ORM makes the translation of SQL result sets into code objects more seamless. Using an ORM does not automatically make you immune to SQL injection, however. As a general rule of thumb: if you find yourself writing SQL statements by concatenating strings, think very carefully about what you are doing. Escaping Inputs Programming languages have standard ways to describe strings containing quotes within them – SQL is no different in this respect. Typically, doubling up the quote character – replacing ' with '' – means “treat this quote as part of the string, not the end of the string”. Sanitizing Inputs Client-side validation (i.e. in JavaScript) is useful for giving the user immediate feedback when filling out a form, but is no defense against a serious hacker. Most hack attempts are performed using scripts, rather than the browser itself. ","date":"2021-12-12","objectID":"/hackspaining/:1:1","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#sanitizing-inputs"},{"categories":null,"content":"Cross-Site Scripting (XSS) Spreading worms on social media sites. Facebook, Twitter and YouTube have all been successfully attacked in this way. Session hijacking. Malicious JavaScript may be able to send the session ID to a remote site under the hacker’s control, allowing the hacker to impersonate that user by hijacking a session in progress. Identity theft. If the user enters confidential information such as credit card numbers into a compromised website, these details can be stolen using malicious JavaScript. Denial of service attacks and website vandalism. Theft of sensitive data, like passwords. Financial fraud on banking sites. Escape Dynamic Content you should escape all dynamic content coming from a data store, so the browser knows it is to be treated as the contents of HTML tags, as opposed to raw HTML. Escaping dynamic content generally consists of replacing significant characters with the HTML entity encoding: \" \u0026#34 # \u0026#35 \u0026 \u0026#38 ' \u0026#39 ( \u0026#40 ) \u0026#41 / \u0026#47 ; \u0026#59 \u003c \u0026#60 \u003e \u0026#62 Whitelist Values instead of asking a user to type in their country of residence, have them select from a drop-down list. Implement a Content-Security Policy allow the author of a web-page to control where JavaScript (and other resources) can be loaded and executed from. The content security policy can also be set in a tag in the element of the page: \u003cmeta http-equiv=\"Content-Security-Policy\" content=\"script-src 'self' https://apis.google.com\"\u003e This approach will protect your users very effectively! Sanitize HTML Some sites have a legitimate need to store and render raw HTML, especially now that contentEditable has become part of the HTML5 standard. If your site stores and renders rich content, you need to use a HTML sanitization library to ensure malicious users cannot inject scripts in their HTML submissions. ","date":"2021-12-12","objectID":"/hackspaining/:2:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#cross-site-scripting-xss"},{"categories":null,"content":"Cross-Site Scripting (XSS) Spreading worms on social media sites. Facebook, Twitter and YouTube have all been successfully attacked in this way. Session hijacking. Malicious JavaScript may be able to send the session ID to a remote site under the hacker’s control, allowing the hacker to impersonate that user by hijacking a session in progress. Identity theft. If the user enters confidential information such as credit card numbers into a compromised website, these details can be stolen using malicious JavaScript. Denial of service attacks and website vandalism. Theft of sensitive data, like passwords. Financial fraud on banking sites. Escape Dynamic Content you should escape all dynamic content coming from a data store, so the browser knows it is to be treated as the contents of HTML tags, as opposed to raw HTML. Escaping dynamic content generally consists of replacing significant characters with the HTML entity encoding: \" \" # # \u0026 \u0026 ' ' ( ( ) ) / / ; ; \u003e Whitelist Values instead of asking a user to type in their country of residence, have them select from a drop-down list. Implement a Content-Security Policy allow the author of a web-page to control where JavaScript (and other resources) can be loaded and executed from. The content security policy can also be set in a tag in the element of the page: This approach will protect your users very effectively! Sanitize HTML Some sites have a legitimate need to store and render raw HTML, especially now that contentEditable has become part of the HTML5 standard. If your site stores and renders rich content, you need to use a HTML sanitization library to ensure malicious users cannot inject scripts in their HTML submissions. ","date":"2021-12-12","objectID":"/hackspaining/:2:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#escape-dynamic-content"},{"categories":null,"content":"Cross-Site Scripting (XSS) Spreading worms on social media sites. Facebook, Twitter and YouTube have all been successfully attacked in this way. Session hijacking. Malicious JavaScript may be able to send the session ID to a remote site under the hacker’s control, allowing the hacker to impersonate that user by hijacking a session in progress. Identity theft. If the user enters confidential information such as credit card numbers into a compromised website, these details can be stolen using malicious JavaScript. Denial of service attacks and website vandalism. Theft of sensitive data, like passwords. Financial fraud on banking sites. Escape Dynamic Content you should escape all dynamic content coming from a data store, so the browser knows it is to be treated as the contents of HTML tags, as opposed to raw HTML. Escaping dynamic content generally consists of replacing significant characters with the HTML entity encoding: \" \" # # \u0026 \u0026 ' ' ( ( ) ) / / ; ; Whitelist Values instead of asking a user to type in their country of residence, have them select from a drop-down list. Implement a Content-Security Policy allow the author of a web-page to control where JavaScript (and other resources) can be loaded and executed from. The content security policy can also be set in a tag in the element of the page: This approach will protect your users very effectively! Sanitize HTML Some sites have a legitimate need to store and render raw HTML, especially now that contentEditable has become part of the HTML5 standard. If your site stores and renders rich content, you need to use a HTML sanitization library to ensure malicious users cannot inject scripts in their HTML submissions. ","date":"2021-12-12","objectID":"/hackspaining/:2:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#whitelist-values"},{"categories":null,"content":"Cross-Site Scripting (XSS) Spreading worms on social media sites. Facebook, Twitter and YouTube have all been successfully attacked in this way. Session hijacking. Malicious JavaScript may be able to send the session ID to a remote site under the hacker’s control, allowing the hacker to impersonate that user by hijacking a session in progress. Identity theft. If the user enters confidential information such as credit card numbers into a compromised website, these details can be stolen using malicious JavaScript. Denial of service attacks and website vandalism. Theft of sensitive data, like passwords. Financial fraud on banking sites. Escape Dynamic Content you should escape all dynamic content coming from a data store, so the browser knows it is to be treated as the contents of HTML tags, as opposed to raw HTML. Escaping dynamic content generally consists of replacing significant characters with the HTML entity encoding: \" \" # # \u0026 \u0026 ' ' ( ( ) ) / / ; ; Whitelist Values instead of asking a user to type in their country of residence, have them select from a drop-down list. Implement a Content-Security Policy allow the author of a web-page to control where JavaScript (and other resources) can be loaded and executed from. The content security policy can also be set in a tag in the element of the page: This approach will protect your users very effectively! Sanitize HTML Some sites have a legitimate need to store and render raw HTML, especially now that contentEditable has become part of the HTML5 standard. If your site stores and renders rich content, you need to use a HTML sanitization library to ensure malicious users cannot inject scripts in their HTML submissions. ","date":"2021-12-12","objectID":"/hackspaining/:2:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#implement-a-content-security-policy"},{"categories":null,"content":"Cross-Site Scripting (XSS) Spreading worms on social media sites. Facebook, Twitter and YouTube have all been successfully attacked in this way. Session hijacking. Malicious JavaScript may be able to send the session ID to a remote site under the hacker’s control, allowing the hacker to impersonate that user by hijacking a session in progress. Identity theft. If the user enters confidential information such as credit card numbers into a compromised website, these details can be stolen using malicious JavaScript. Denial of service attacks and website vandalism. Theft of sensitive data, like passwords. Financial fraud on banking sites. Escape Dynamic Content you should escape all dynamic content coming from a data store, so the browser knows it is to be treated as the contents of HTML tags, as opposed to raw HTML. Escaping dynamic content generally consists of replacing significant characters with the HTML entity encoding: \" \" # # \u0026 \u0026 ' ' ( ( ) ) / / ; ; Whitelist Values instead of asking a user to type in their country of residence, have them select from a drop-down list. Implement a Content-Security Policy allow the author of a web-page to control where JavaScript (and other resources) can be loaded and executed from. The content security policy can also be set in a tag in the element of the page: This approach will protect your users very effectively! Sanitize HTML Some sites have a legitimate need to store and render raw HTML, especially now that contentEditable has become part of the HTML5 standard. If your site stores and renders rich content, you need to use a HTML sanitization library to ensure malicious users cannot inject scripts in their HTML submissions. ","date":"2021-12-12","objectID":"/hackspaining/:2:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#sanitize-html"},{"categories":null,"content":"Command Execution Remote code execution is a major security lapse, and the last step along the road to complete system takeover. After gaining access, an attacker will attempt to escalate their privileges on the server, install malicious scripts, or make your server part of a botnet to be used at a later date. Command injection vulnerabilities often occur in older, legacy code, such as CGI scripts. Try to Avoid Command Line Calls Altogether Use APIs wherever possible – only use shell commands where absolutely necessary. Escape Inputs Correctly Injection vulnerabilities occur when untrusted input is not sanitized correctly. If you use shell commands, be sure to scrub input values for potentially malicious characters: ; \u0026 | ` Even better, restrict input by testing it against a regular expression of known safe characters. (For example, alphanumeric characters.) Restrict the Permitted Commands Perform Thorough Code Reviews Run with Restricted Permissions ","date":"2021-12-12","objectID":"/hackspaining/:3:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#command-execution"},{"categories":null,"content":"Command Execution Remote code execution is a major security lapse, and the last step along the road to complete system takeover. After gaining access, an attacker will attempt to escalate their privileges on the server, install malicious scripts, or make your server part of a botnet to be used at a later date. Command injection vulnerabilities often occur in older, legacy code, such as CGI scripts. Try to Avoid Command Line Calls Altogether Use APIs wherever possible – only use shell commands where absolutely necessary. Escape Inputs Correctly Injection vulnerabilities occur when untrusted input is not sanitized correctly. If you use shell commands, be sure to scrub input values for potentially malicious characters: ; \u0026 | ` Even better, restrict input by testing it against a regular expression of known safe characters. (For example, alphanumeric characters.) Restrict the Permitted Commands Perform Thorough Code Reviews Run with Restricted Permissions ","date":"2021-12-12","objectID":"/hackspaining/:3:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#try-to-avoid-command-line-calls-altogether"},{"categories":null,"content":"Command Execution Remote code execution is a major security lapse, and the last step along the road to complete system takeover. After gaining access, an attacker will attempt to escalate their privileges on the server, install malicious scripts, or make your server part of a botnet to be used at a later date. Command injection vulnerabilities often occur in older, legacy code, such as CGI scripts. Try to Avoid Command Line Calls Altogether Use APIs wherever possible – only use shell commands where absolutely necessary. Escape Inputs Correctly Injection vulnerabilities occur when untrusted input is not sanitized correctly. If you use shell commands, be sure to scrub input values for potentially malicious characters: ; \u0026 | ` Even better, restrict input by testing it against a regular expression of known safe characters. (For example, alphanumeric characters.) Restrict the Permitted Commands Perform Thorough Code Reviews Run with Restricted Permissions ","date":"2021-12-12","objectID":"/hackspaining/:3:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#escape-inputs-correctly"},{"categories":null,"content":"Command Execution Remote code execution is a major security lapse, and the last step along the road to complete system takeover. After gaining access, an attacker will attempt to escalate their privileges on the server, install malicious scripts, or make your server part of a botnet to be used at a later date. Command injection vulnerabilities often occur in older, legacy code, such as CGI scripts. Try to Avoid Command Line Calls Altogether Use APIs wherever possible – only use shell commands where absolutely necessary. Escape Inputs Correctly Injection vulnerabilities occur when untrusted input is not sanitized correctly. If you use shell commands, be sure to scrub input values for potentially malicious characters: ; \u0026 | ` Even better, restrict input by testing it against a regular expression of known safe characters. (For example, alphanumeric characters.) Restrict the Permitted Commands Perform Thorough Code Reviews Run with Restricted Permissions ","date":"2021-12-12","objectID":"/hackspaining/:3:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#restrict-the-permitted-commands"},{"categories":null,"content":"Command Execution Remote code execution is a major security lapse, and the last step along the road to complete system takeover. After gaining access, an attacker will attempt to escalate their privileges on the server, install malicious scripts, or make your server part of a botnet to be used at a later date. Command injection vulnerabilities often occur in older, legacy code, such as CGI scripts. Try to Avoid Command Line Calls Altogether Use APIs wherever possible – only use shell commands where absolutely necessary. Escape Inputs Correctly Injection vulnerabilities occur when untrusted input is not sanitized correctly. If you use shell commands, be sure to scrub input values for potentially malicious characters: ; \u0026 | ` Even better, restrict input by testing it against a regular expression of known safe characters. (For example, alphanumeric characters.) Restrict the Permitted Commands Perform Thorough Code Reviews Run with Restricted Permissions ","date":"2021-12-12","objectID":"/hackspaining/:3:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#perform-thorough-code-reviews"},{"categories":null,"content":"Command Execution Remote code execution is a major security lapse, and the last step along the road to complete system takeover. After gaining access, an attacker will attempt to escalate their privileges on the server, install malicious scripts, or make your server part of a botnet to be used at a later date. Command injection vulnerabilities often occur in older, legacy code, such as CGI scripts. Try to Avoid Command Line Calls Altogether Use APIs wherever possible – only use shell commands where absolutely necessary. Escape Inputs Correctly Injection vulnerabilities occur when untrusted input is not sanitized correctly. If you use shell commands, be sure to scrub input values for potentially malicious characters: ; \u0026 | ` Even better, restrict input by testing it against a regular expression of known safe characters. (For example, alphanumeric characters.) Restrict the Permitted Commands Perform Thorough Code Reviews Run with Restricted Permissions ","date":"2021-12-12","objectID":"/hackspaining/:3:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#run-with-restricted-permissions"},{"categories":null,"content":"Clickjacking Harvest login credentials, by rendering a fake login box on top of the real one. Trick users into turning on their web-cam or microphone, by rendering invisible elements over the Adobe Flash settings page. Spread worms on social media sites like Twitter and MySpace. Promote online scams by tricking people into clicking on things they otherwise would not. Spread malware by diverting users to malicious download links. X-Frame-Options The X-Frame-Options HTTP header can be used to indicate whether or not a browser should be allowed to render a page in a \u003cframe\u003e, \u003ciframe\u003e or \u003cobject\u003e tag. It was designed specifically to help protect against clickjacking. There are three permitted values for the header: DENY The page cannot be displayed in a frame, regardless of the site attempting to do so. SAMEORIGIN The page can only be displayed in a frame on the same origin as the page itself. ALLOW-FROM uri The page can only be displayed in a frame on the specified origins. Content Security Policy Frame-Killing In older browsers, the most common way to protect users against clickjacking was to include a frame-killing JavaScript snippet in pages to prevent them being included in foreign iframes. Frame-killing offers a large degree of protection against clickjacking, but it can be error-prone. Be sure to set appropriate HTTP headers as the first recourse in protecting your site. ","date":"2021-12-12","objectID":"/hackspaining/:4:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#clickjacking"},{"categories":null,"content":"Clickjacking Harvest login credentials, by rendering a fake login box on top of the real one. Trick users into turning on their web-cam or microphone, by rendering invisible elements over the Adobe Flash settings page. Spread worms on social media sites like Twitter and MySpace. Promote online scams by tricking people into clicking on things they otherwise would not. Spread malware by diverting users to malicious download links. X-Frame-Options The X-Frame-Options HTTP header can be used to indicate whether or not a browser should be allowed to render a page in a , or tag. It was designed specifically to help protect against clickjacking. There are three permitted values for the header: DENY The page cannot be displayed in a frame, regardless of the site attempting to do so. SAMEORIGIN The page can only be displayed in a frame on the same origin as the page itself. ALLOW-FROM uri The page can only be displayed in a frame on the specified origins. Content Security Policy Frame-Killing In older browsers, the most common way to protect users against clickjacking was to include a frame-killing JavaScript snippet in pages to prevent them being included in foreign iframes. Frame-killing offers a large degree of protection against clickjacking, but it can be error-prone. Be sure to set appropriate HTTP headers as the first recourse in protecting your site. ","date":"2021-12-12","objectID":"/hackspaining/:4:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#x-frame-options"},{"categories":null,"content":"Clickjacking Harvest login credentials, by rendering a fake login box on top of the real one. Trick users into turning on their web-cam or microphone, by rendering invisible elements over the Adobe Flash settings page. Spread worms on social media sites like Twitter and MySpace. Promote online scams by tricking people into clicking on things they otherwise would not. Spread malware by diverting users to malicious download links. X-Frame-Options The X-Frame-Options HTTP header can be used to indicate whether or not a browser should be allowed to render a page in a , or tag. It was designed specifically to help protect against clickjacking. There are three permitted values for the header: DENY The page cannot be displayed in a frame, regardless of the site attempting to do so. SAMEORIGIN The page can only be displayed in a frame on the same origin as the page itself. ALLOW-FROM uri The page can only be displayed in a frame on the specified origins. Content Security Policy Frame-Killing In older browsers, the most common way to protect users against clickjacking was to include a frame-killing JavaScript snippet in pages to prevent them being included in foreign iframes. Frame-killing offers a large degree of protection against clickjacking, but it can be error-prone. Be sure to set appropriate HTTP headers as the first recourse in protecting your site. ","date":"2021-12-12","objectID":"/hackspaining/:4:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#content-security-policy"},{"categories":null,"content":"Clickjacking Harvest login credentials, by rendering a fake login box on top of the real one. Trick users into turning on their web-cam or microphone, by rendering invisible elements over the Adobe Flash settings page. Spread worms on social media sites like Twitter and MySpace. Promote online scams by tricking people into clicking on things they otherwise would not. Spread malware by diverting users to malicious download links. X-Frame-Options The X-Frame-Options HTTP header can be used to indicate whether or not a browser should be allowed to render a page in a , or tag. It was designed specifically to help protect against clickjacking. There are three permitted values for the header: DENY The page cannot be displayed in a frame, regardless of the site attempting to do so. SAMEORIGIN The page can only be displayed in a frame on the same origin as the page itself. ALLOW-FROM uri The page can only be displayed in a frame on the specified origins. Content Security Policy Frame-Killing In older browsers, the most common way to protect users against clickjacking was to include a frame-killing JavaScript snippet in pages to prevent them being included in foreign iframes. Frame-killing offers a large degree of protection against clickjacking, but it can be error-prone. Be sure to set appropriate HTTP headers as the first recourse in protecting your site. ","date":"2021-12-12","objectID":"/hackspaining/:4:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#frame-killing"},{"categories":null,"content":"Cross-Site Request Forgery (CSRF) Steal confidential data. Spread worms on social media. Install malware on mobile phones. REST Representation State Transfer (REST) is a series of design principles that assign certain types of action (view, create, delete, update) to different HTTP verbs. Following REST-ful designs will keep your code clean and help your site scale. Moreover, REST insists that GET requests are used only to view resources. Keeping your GET requests side-effect free will limit the harm that can be done by maliciously crafted URLs–an attacker will have to work much harder to generate harmful POST requests. Anti-Forgery Tokens This is called an anti-forgery token. Each time your server renders a page that performs sensitive actions, it should write out an anti-forgery token in a hidden HTML form field. This token must be included with form submissions, or AJAX calls. The server should validate the token when it is returned in subsequent requests, and reject any calls with missing or invalid tokens. Ensure Cookies are sent with the SameSite Cookie Attribute A value of Strict will mean than any request initiated by a third-party domain to your domain will have any cookies stripped by the browser. This is the most secure setting, since it prevents malicious sites attempting to perform harmful actions under a user’s session. A value of Lax permits GET request from a third-party domain to your domain to have cookies attached - but only GET requests. With this setting a user will not have to sign in again to your site if the follow a link from another site (say, Google search results). This makes for a friendlier user-experience - but make sure your GET requests are side-effect free! Include Addition Authentication for Sensitive Actions ","date":"2021-12-12","objectID":"/hackspaining/:5:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#cross-site-request-forgery-csrf"},{"categories":null,"content":"Cross-Site Request Forgery (CSRF) Steal confidential data. Spread worms on social media. Install malware on mobile phones. REST Representation State Transfer (REST) is a series of design principles that assign certain types of action (view, create, delete, update) to different HTTP verbs. Following REST-ful designs will keep your code clean and help your site scale. Moreover, REST insists that GET requests are used only to view resources. Keeping your GET requests side-effect free will limit the harm that can be done by maliciously crafted URLs–an attacker will have to work much harder to generate harmful POST requests. Anti-Forgery Tokens This is called an anti-forgery token. Each time your server renders a page that performs sensitive actions, it should write out an anti-forgery token in a hidden HTML form field. This token must be included with form submissions, or AJAX calls. The server should validate the token when it is returned in subsequent requests, and reject any calls with missing or invalid tokens. Ensure Cookies are sent with the SameSite Cookie Attribute A value of Strict will mean than any request initiated by a third-party domain to your domain will have any cookies stripped by the browser. This is the most secure setting, since it prevents malicious sites attempting to perform harmful actions under a user’s session. A value of Lax permits GET request from a third-party domain to your domain to have cookies attached - but only GET requests. With this setting a user will not have to sign in again to your site if the follow a link from another site (say, Google search results). This makes for a friendlier user-experience - but make sure your GET requests are side-effect free! Include Addition Authentication for Sensitive Actions ","date":"2021-12-12","objectID":"/hackspaining/:5:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#rest"},{"categories":null,"content":"Cross-Site Request Forgery (CSRF) Steal confidential data. Spread worms on social media. Install malware on mobile phones. REST Representation State Transfer (REST) is a series of design principles that assign certain types of action (view, create, delete, update) to different HTTP verbs. Following REST-ful designs will keep your code clean and help your site scale. Moreover, REST insists that GET requests are used only to view resources. Keeping your GET requests side-effect free will limit the harm that can be done by maliciously crafted URLs–an attacker will have to work much harder to generate harmful POST requests. Anti-Forgery Tokens This is called an anti-forgery token. Each time your server renders a page that performs sensitive actions, it should write out an anti-forgery token in a hidden HTML form field. This token must be included with form submissions, or AJAX calls. The server should validate the token when it is returned in subsequent requests, and reject any calls with missing or invalid tokens. Ensure Cookies are sent with the SameSite Cookie Attribute A value of Strict will mean than any request initiated by a third-party domain to your domain will have any cookies stripped by the browser. This is the most secure setting, since it prevents malicious sites attempting to perform harmful actions under a user’s session. A value of Lax permits GET request from a third-party domain to your domain to have cookies attached - but only GET requests. With this setting a user will not have to sign in again to your site if the follow a link from another site (say, Google search results). This makes for a friendlier user-experience - but make sure your GET requests are side-effect free! Include Addition Authentication for Sensitive Actions ","date":"2021-12-12","objectID":"/hackspaining/:5:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#anti-forgery-tokens"},{"categories":null,"content":"Cross-Site Request Forgery (CSRF) Steal confidential data. Spread worms on social media. Install malware on mobile phones. REST Representation State Transfer (REST) is a series of design principles that assign certain types of action (view, create, delete, update) to different HTTP verbs. Following REST-ful designs will keep your code clean and help your site scale. Moreover, REST insists that GET requests are used only to view resources. Keeping your GET requests side-effect free will limit the harm that can be done by maliciously crafted URLs–an attacker will have to work much harder to generate harmful POST requests. Anti-Forgery Tokens This is called an anti-forgery token. Each time your server renders a page that performs sensitive actions, it should write out an anti-forgery token in a hidden HTML form field. This token must be included with form submissions, or AJAX calls. The server should validate the token when it is returned in subsequent requests, and reject any calls with missing or invalid tokens. Ensure Cookies are sent with the SameSite Cookie Attribute A value of Strict will mean than any request initiated by a third-party domain to your domain will have any cookies stripped by the browser. This is the most secure setting, since it prevents malicious sites attempting to perform harmful actions under a user’s session. A value of Lax permits GET request from a third-party domain to your domain to have cookies attached - but only GET requests. With this setting a user will not have to sign in again to your site if the follow a link from another site (say, Google search results). This makes for a friendlier user-experience - but make sure your GET requests are side-effect free! Include Addition Authentication for Sensitive Actions ","date":"2021-12-12","objectID":"/hackspaining/:5:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#ensure-cookies-are-sent-with-the-samesite-cookie-attribute"},{"categories":null,"content":"Cross-Site Request Forgery (CSRF) Steal confidential data. Spread worms on social media. Install malware on mobile phones. REST Representation State Transfer (REST) is a series of design principles that assign certain types of action (view, create, delete, update) to different HTTP verbs. Following REST-ful designs will keep your code clean and help your site scale. Moreover, REST insists that GET requests are used only to view resources. Keeping your GET requests side-effect free will limit the harm that can be done by maliciously crafted URLs–an attacker will have to work much harder to generate harmful POST requests. Anti-Forgery Tokens This is called an anti-forgery token. Each time your server renders a page that performs sensitive actions, it should write out an anti-forgery token in a hidden HTML form field. This token must be included with form submissions, or AJAX calls. The server should validate the token when it is returned in subsequent requests, and reject any calls with missing or invalid tokens. Ensure Cookies are sent with the SameSite Cookie Attribute A value of Strict will mean than any request initiated by a third-party domain to your domain will have any cookies stripped by the browser. This is the most secure setting, since it prevents malicious sites attempting to perform harmful actions under a user’s session. A value of Lax permits GET request from a third-party domain to your domain to have cookies attached - but only GET requests. With this setting a user will not have to sign in again to your site if the follow a link from another site (say, Google search results). This makes for a friendlier user-experience - but make sure your GET requests are side-effect free! Include Addition Authentication for Sensitive Actions ","date":"2021-12-12","objectID":"/hackspaining/:5:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#include-addition-authentication-for-sensitive-actions"},{"categories":null,"content":"Reflected XSS Search results - does the search criteria get displayed back to the user? Is it written out in the page title? Are you sure it is being escaped properly? Error pages - if you have error messages that complain about invalid inputs, does the input get escaped properly when it is displayed back to the user? Does your 404 page mention the path being searched for? Form submissions - if a page POSTs data, does any part of the data being submitted by the form get displayed back to the user? What if the form submission is rejected – does the error page allow injection of malicious code? Does an erroneously submitted form get pre-populated with the values previously submitted? Escape Dynamic Content Whitelist Values Implement a Content-Security Policy ","date":"2021-12-12","objectID":"/hackspaining/:6:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#reflected-xss"},{"categories":null,"content":"Reflected XSS Search results - does the search criteria get displayed back to the user? Is it written out in the page title? Are you sure it is being escaped properly? Error pages - if you have error messages that complain about invalid inputs, does the input get escaped properly when it is displayed back to the user? Does your 404 page mention the path being searched for? Form submissions - if a page POSTs data, does any part of the data being submitted by the form get displayed back to the user? What if the form submission is rejected – does the error page allow injection of malicious code? Does an erroneously submitted form get pre-populated with the values previously submitted? Escape Dynamic Content Whitelist Values Implement a Content-Security Policy ","date":"2021-12-12","objectID":"/hackspaining/:6:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#escape-dynamic-content-1"},{"categories":null,"content":"Reflected XSS Search results - does the search criteria get displayed back to the user? Is it written out in the page title? Are you sure it is being escaped properly? Error pages - if you have error messages that complain about invalid inputs, does the input get escaped properly when it is displayed back to the user? Does your 404 page mention the path being searched for? Form submissions - if a page POSTs data, does any part of the data being submitted by the form get displayed back to the user? What if the form submission is rejected – does the error page allow injection of malicious code? Does an erroneously submitted form get pre-populated with the values previously submitted? Escape Dynamic Content Whitelist Values Implement a Content-Security Policy ","date":"2021-12-12","objectID":"/hackspaining/:6:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#whitelist-values-1"},{"categories":null,"content":"Reflected XSS Search results - does the search criteria get displayed back to the user? Is it written out in the page title? Are you sure it is being escaped properly? Error pages - if you have error messages that complain about invalid inputs, does the input get escaped properly when it is displayed back to the user? Does your 404 page mention the path being searched for? Form submissions - if a page POSTs data, does any part of the data being submitted by the form get displayed back to the user? What if the form submission is rejected – does the error page allow injection of malicious code? Does an erroneously submitted form get pre-populated with the values previously submitted? Escape Dynamic Content Whitelist Values Implement a Content-Security Policy ","date":"2021-12-12","objectID":"/hackspaining/:6:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#implement-a-content-security-policy-1"},{"categories":null,"content":"File Upload Vulnerabilities Sophisticated hackers typically exploit a combination of vulnerabilities when attacking your site – uploading malicious code to a server is step one in the hacker playbook. The next step is finding a way to execute the malicious code. Even big companies fall foul to this vulnerability, particularly if they are running complex, legacy code bases. Segregate Your Uploads File uploads are generally intended to be inert. Unless you are building a very particular type of website, you are typically expecting images, videos, or document files, rather than executable code. If this is the case, making sure uploaded files are kept separate from application code is a key security consideration. Consider using cloud-based storage or a content management system to store uploaded files. Alternatively, if you are sure of your ability to scale your backend, you could write uploaded files to your database. Both of these approaches prevent accidental execution of an executable file. Ensure Upload Files Cannot Be Executed Rename Files on Upload Validate File Formats and Extensions Validate the Content-Type Header Use a Virus Scanner ","date":"2021-12-12","objectID":"/hackspaining/:7:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#file-upload-vulnerabilities"},{"categories":null,"content":"File Upload Vulnerabilities Sophisticated hackers typically exploit a combination of vulnerabilities when attacking your site – uploading malicious code to a server is step one in the hacker playbook. The next step is finding a way to execute the malicious code. Even big companies fall foul to this vulnerability, particularly if they are running complex, legacy code bases. Segregate Your Uploads File uploads are generally intended to be inert. Unless you are building a very particular type of website, you are typically expecting images, videos, or document files, rather than executable code. If this is the case, making sure uploaded files are kept separate from application code is a key security consideration. Consider using cloud-based storage or a content management system to store uploaded files. Alternatively, if you are sure of your ability to scale your backend, you could write uploaded files to your database. Both of these approaches prevent accidental execution of an executable file. Ensure Upload Files Cannot Be Executed Rename Files on Upload Validate File Formats and Extensions Validate the Content-Type Header Use a Virus Scanner ","date":"2021-12-12","objectID":"/hackspaining/:7:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#segregate-your-uploads"},{"categories":null,"content":"File Upload Vulnerabilities Sophisticated hackers typically exploit a combination of vulnerabilities when attacking your site – uploading malicious code to a server is step one in the hacker playbook. The next step is finding a way to execute the malicious code. Even big companies fall foul to this vulnerability, particularly if they are running complex, legacy code bases. Segregate Your Uploads File uploads are generally intended to be inert. Unless you are building a very particular type of website, you are typically expecting images, videos, or document files, rather than executable code. If this is the case, making sure uploaded files are kept separate from application code is a key security consideration. Consider using cloud-based storage or a content management system to store uploaded files. Alternatively, if you are sure of your ability to scale your backend, you could write uploaded files to your database. Both of these approaches prevent accidental execution of an executable file. Ensure Upload Files Cannot Be Executed Rename Files on Upload Validate File Formats and Extensions Validate the Content-Type Header Use a Virus Scanner ","date":"2021-12-12","objectID":"/hackspaining/:7:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#ensure-upload-files-cannot-be-executed"},{"categories":null,"content":"File Upload Vulnerabilities Sophisticated hackers typically exploit a combination of vulnerabilities when attacking your site – uploading malicious code to a server is step one in the hacker playbook. The next step is finding a way to execute the malicious code. Even big companies fall foul to this vulnerability, particularly if they are running complex, legacy code bases. Segregate Your Uploads File uploads are generally intended to be inert. Unless you are building a very particular type of website, you are typically expecting images, videos, or document files, rather than executable code. If this is the case, making sure uploaded files are kept separate from application code is a key security consideration. Consider using cloud-based storage or a content management system to store uploaded files. Alternatively, if you are sure of your ability to scale your backend, you could write uploaded files to your database. Both of these approaches prevent accidental execution of an executable file. Ensure Upload Files Cannot Be Executed Rename Files on Upload Validate File Formats and Extensions Validate the Content-Type Header Use a Virus Scanner ","date":"2021-12-12","objectID":"/hackspaining/:7:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#rename-files-on-upload"},{"categories":null,"content":"File Upload Vulnerabilities Sophisticated hackers typically exploit a combination of vulnerabilities when attacking your site – uploading malicious code to a server is step one in the hacker playbook. The next step is finding a way to execute the malicious code. Even big companies fall foul to this vulnerability, particularly if they are running complex, legacy code bases. Segregate Your Uploads File uploads are generally intended to be inert. Unless you are building a very particular type of website, you are typically expecting images, videos, or document files, rather than executable code. If this is the case, making sure uploaded files are kept separate from application code is a key security consideration. Consider using cloud-based storage or a content management system to store uploaded files. Alternatively, if you are sure of your ability to scale your backend, you could write uploaded files to your database. Both of these approaches prevent accidental execution of an executable file. Ensure Upload Files Cannot Be Executed Rename Files on Upload Validate File Formats and Extensions Validate the Content-Type Header Use a Virus Scanner ","date":"2021-12-12","objectID":"/hackspaining/:7:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#validate-file-formats-and-extensions"},{"categories":null,"content":"File Upload Vulnerabilities Sophisticated hackers typically exploit a combination of vulnerabilities when attacking your site – uploading malicious code to a server is step one in the hacker playbook. The next step is finding a way to execute the malicious code. Even big companies fall foul to this vulnerability, particularly if they are running complex, legacy code bases. Segregate Your Uploads File uploads are generally intended to be inert. Unless you are building a very particular type of website, you are typically expecting images, videos, or document files, rather than executable code. If this is the case, making sure uploaded files are kept separate from application code is a key security consideration. Consider using cloud-based storage or a content management system to store uploaded files. Alternatively, if you are sure of your ability to scale your backend, you could write uploaded files to your database. Both of these approaches prevent accidental execution of an executable file. Ensure Upload Files Cannot Be Executed Rename Files on Upload Validate File Formats and Extensions Validate the Content-Type Header Use a Virus Scanner ","date":"2021-12-12","objectID":"/hackspaining/:7:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#validate-the-content-type-header"},{"categories":null,"content":"File Upload Vulnerabilities Sophisticated hackers typically exploit a combination of vulnerabilities when attacking your site – uploading malicious code to a server is step one in the hacker playbook. The next step is finding a way to execute the malicious code. Even big companies fall foul to this vulnerability, particularly if they are running complex, legacy code bases. Segregate Your Uploads File uploads are generally intended to be inert. Unless you are building a very particular type of website, you are typically expecting images, videos, or document files, rather than executable code. If this is the case, making sure uploaded files are kept separate from application code is a key security consideration. Consider using cloud-based storage or a content management system to store uploaded files. Alternatively, if you are sure of your ability to scale your backend, you could write uploaded files to your database. Both of these approaches prevent accidental execution of an executable file. Ensure Upload Files Cannot Be Executed Rename Files on Upload Validate File Formats and Extensions Validate the Content-Type Header Use a Virus Scanner ","date":"2021-12-12","objectID":"/hackspaining/:7:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#use-a-virus-scanner"},{"categories":null,"content":"Open Redirects Redirects are a useful function to have when building a website. If a user attempts to access a resource before they are logged in, it is conventional to redirect them to the login page, put the original URL in a query parameter, and after they have logged in, automatically redirect them to their original destination. This type of functionality shows you are putting thought into the user experience, and is to be encouraged. However, you need to be sure anywhere you do redirects, they are done safely – otherwise you are putting your users in harm’s way by enabling phishing attacks. Disallow Offsite Redirects Make sure all redirect URLs are relative paths – i.e. they start with a single / character. (Note that URLs starting with // will be interpreted by the browser as a protocol agnostic, absolute URL – so they should be rejected too.) If you do need to perform external redirects, consider whitelisting the individual sites that you permit redirects to. Check the Referrer When Doing Redirects Redirects to URLs passed in query parameters should only be triggered by pages on your site. As a second layer of defense, check that the Referer in the HTTP request matches your domain whenever you perform a redirect. ","date":"2021-12-12","objectID":"/hackspaining/:8:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#open-redirects"},{"categories":null,"content":"Open Redirects Redirects are a useful function to have when building a website. If a user attempts to access a resource before they are logged in, it is conventional to redirect them to the login page, put the original URL in a query parameter, and after they have logged in, automatically redirect them to their original destination. This type of functionality shows you are putting thought into the user experience, and is to be encouraged. However, you need to be sure anywhere you do redirects, they are done safely – otherwise you are putting your users in harm’s way by enabling phishing attacks. Disallow Offsite Redirects Make sure all redirect URLs are relative paths – i.e. they start with a single / character. (Note that URLs starting with // will be interpreted by the browser as a protocol agnostic, absolute URL – so they should be rejected too.) If you do need to perform external redirects, consider whitelisting the individual sites that you permit redirects to. Check the Referrer When Doing Redirects Redirects to URLs passed in query parameters should only be triggered by pages on your site. As a second layer of defense, check that the Referer in the HTTP request matches your domain whenever you perform a redirect. ","date":"2021-12-12","objectID":"/hackspaining/:8:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#disallow-offsite-redirects"},{"categories":null,"content":"Open Redirects Redirects are a useful function to have when building a website. If a user attempts to access a resource before they are logged in, it is conventional to redirect them to the login page, put the original URL in a query parameter, and after they have logged in, automatically redirect them to their original destination. This type of functionality shows you are putting thought into the user experience, and is to be encouraged. However, you need to be sure anywhere you do redirects, they are done safely – otherwise you are putting your users in harm’s way by enabling phishing attacks. Disallow Offsite Redirects Make sure all redirect URLs are relative paths – i.e. they start with a single / character. (Note that URLs starting with // will be interpreted by the browser as a protocol agnostic, absolute URL – so they should be rejected too.) If you do need to perform external redirects, consider whitelisting the individual sites that you permit redirects to. Check the Referrer When Doing Redirects Redirects to URLs passed in query parameters should only be triggered by pages on your site. As a second layer of defense, check that the Referer in the HTTP request matches your domain whenever you perform a redirect. ","date":"2021-12-12","objectID":"/hackspaining/:8:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#check-the-referrer-when-doing-redirects"},{"categories":null,"content":"Unencrypted Communication Insecure Wi-Fi hotspots, as illustrated in our exercise, are just one way enterprising hackers have found to take advantage of unencrypted communication. They may also try to sniff traffic within your network, and if they get access, inspect traffic going through compromised edge devices. Buy a certificate, install it, and configure your web server to use it. It’s really as simple as that. Web servers are typically able to serve the same content over HTTP (on port 80) and HTTPS (on port 443). Any non-trivial website should use HTTPS. Facebook and Twitter use HTTPS by default, and this a good example to follow. But make sure you know how to force your web server to elevate to a secure connection, and do so whenever a user is authenticating or establishing a session. A common way of enforcing this is to make sure that cookies are set to secure – that way, sessions can only be established over HTTPS. ","date":"2021-12-12","objectID":"/hackspaining/:9:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#unencrypted-communication"},{"categories":null,"content":"Unencrypted Communication Insecure Wi-Fi hotspots, as illustrated in our exercise, are just one way enterprising hackers have found to take advantage of unencrypted communication. They may also try to sniff traffic within your network, and if they get access, inspect traffic going through compromised edge devices. Buy a certificate, install it, and configure your web server to use it. It’s really as simple as that. Web servers are typically able to serve the same content over HTTP (on port 80) and HTTPS (on port 443). Any non-trivial website should use HTTPS. Facebook and Twitter use HTTPS by default, and this a good example to follow. But make sure you know how to force your web server to elevate to a secure connection, and do so whenever a user is authenticating or establishing a session. A common way of enforcing this is to make sure that cookies are set to secure – that way, sessions can only be established over HTTPS. ","date":"2021-12-12","objectID":"/hackspaining/:9:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#buy-a-certificate-install-it-and-configure-your-web-server-to-use-it"},{"categories":null,"content":"User Enumeration Login Make sure to return a generic “No such username or password” message when a login failure occurs. Make sure the HTTP response, and the time taken to respond are no different when a username does not exist, and an incorrect password is entered. Password Reset Make sure your “forgotten password” page does not reveal usernames. If your password reset process involves sending an email, have the user enter their email address. Then send an email with a password reset link if the account exists. Registration Avoid having your site tell people that a supplied username is already taken. If your usernames are email addresses, send a password reset email if a user tries to sign-up with an existing address. If usernames are not email addresses, protect your sign-up page with a CAPTCHA. Profile Pages If your users have profile pages, make sure they are only visible to other users who are already logged in. If you hide a profile page, ensure a hidden profile is indistinguishable from a non-existent profile. ","date":"2021-12-12","objectID":"/hackspaining/:10:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#user-enumeration"},{"categories":null,"content":"User Enumeration Login Make sure to return a generic “No such username or password” message when a login failure occurs. Make sure the HTTP response, and the time taken to respond are no different when a username does not exist, and an incorrect password is entered. Password Reset Make sure your “forgotten password” page does not reveal usernames. If your password reset process involves sending an email, have the user enter their email address. Then send an email with a password reset link if the account exists. Registration Avoid having your site tell people that a supplied username is already taken. If your usernames are email addresses, send a password reset email if a user tries to sign-up with an existing address. If usernames are not email addresses, protect your sign-up page with a CAPTCHA. Profile Pages If your users have profile pages, make sure they are only visible to other users who are already logged in. If you hide a profile page, ensure a hidden profile is indistinguishable from a non-existent profile. ","date":"2021-12-12","objectID":"/hackspaining/:10:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#login"},{"categories":null,"content":"User Enumeration Login Make sure to return a generic “No such username or password” message when a login failure occurs. Make sure the HTTP response, and the time taken to respond are no different when a username does not exist, and an incorrect password is entered. Password Reset Make sure your “forgotten password” page does not reveal usernames. If your password reset process involves sending an email, have the user enter their email address. Then send an email with a password reset link if the account exists. Registration Avoid having your site tell people that a supplied username is already taken. If your usernames are email addresses, send a password reset email if a user tries to sign-up with an existing address. If usernames are not email addresses, protect your sign-up page with a CAPTCHA. Profile Pages If your users have profile pages, make sure they are only visible to other users who are already logged in. If you hide a profile page, ensure a hidden profile is indistinguishable from a non-existent profile. ","date":"2021-12-12","objectID":"/hackspaining/:10:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#password-reset"},{"categories":null,"content":"User Enumeration Login Make sure to return a generic “No such username or password” message when a login failure occurs. Make sure the HTTP response, and the time taken to respond are no different when a username does not exist, and an incorrect password is entered. Password Reset Make sure your “forgotten password” page does not reveal usernames. If your password reset process involves sending an email, have the user enter their email address. Then send an email with a password reset link if the account exists. Registration Avoid having your site tell people that a supplied username is already taken. If your usernames are email addresses, send a password reset email if a user tries to sign-up with an existing address. If usernames are not email addresses, protect your sign-up page with a CAPTCHA. Profile Pages If your users have profile pages, make sure they are only visible to other users who are already logged in. If you hide a profile page, ensure a hidden profile is indistinguishable from a non-existent profile. ","date":"2021-12-12","objectID":"/hackspaining/:10:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#registration"},{"categories":null,"content":"User Enumeration Login Make sure to return a generic “No such username or password” message when a login failure occurs. Make sure the HTTP response, and the time taken to respond are no different when a username does not exist, and an incorrect password is entered. Password Reset Make sure your “forgotten password” page does not reveal usernames. If your password reset process involves sending an email, have the user enter their email address. Then send an email with a password reset link if the account exists. Registration Avoid having your site tell people that a supplied username is already taken. If your usernames are email addresses, send a password reset email if a user tries to sign-up with an existing address. If usernames are not email addresses, protect your sign-up page with a CAPTCHA. Profile Pages If your users have profile pages, make sure they are only visible to other users who are already logged in. If you hide a profile page, ensure a hidden profile is indistinguishable from a non-existent profile. ","date":"2021-12-12","objectID":"/hackspaining/:10:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#profile-pages"},{"categories":null,"content":"Password Mismanagement Use Third-Party Authentication if Possible Ensure Password Complexity Allow Password Resets via Email Confirm Old Password On Reset Prevent Brute-Forcing Store Passwords With A Strong Hash, Salted Passwords should always be stored as salted hashes. Timeout Sessions After Inactivity, and Provide a Logout Function Use HTTPS for Secure Communication ","date":"2021-12-12","objectID":"/hackspaining/:11:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#password-mismanagement"},{"categories":null,"content":"Password Mismanagement Use Third-Party Authentication if Possible Ensure Password Complexity Allow Password Resets via Email Confirm Old Password On Reset Prevent Brute-Forcing Store Passwords With A Strong Hash, Salted Passwords should always be stored as salted hashes. Timeout Sessions After Inactivity, and Provide a Logout Function Use HTTPS for Secure Communication ","date":"2021-12-12","objectID":"/hackspaining/:11:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#use-third-party-authentication-if-possible"},{"categories":null,"content":"Password Mismanagement Use Third-Party Authentication if Possible Ensure Password Complexity Allow Password Resets via Email Confirm Old Password On Reset Prevent Brute-Forcing Store Passwords With A Strong Hash, Salted Passwords should always be stored as salted hashes. Timeout Sessions After Inactivity, and Provide a Logout Function Use HTTPS for Secure Communication ","date":"2021-12-12","objectID":"/hackspaining/:11:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#ensure-password-complexity"},{"categories":null,"content":"Password Mismanagement Use Third-Party Authentication if Possible Ensure Password Complexity Allow Password Resets via Email Confirm Old Password On Reset Prevent Brute-Forcing Store Passwords With A Strong Hash, Salted Passwords should always be stored as salted hashes. Timeout Sessions After Inactivity, and Provide a Logout Function Use HTTPS for Secure Communication ","date":"2021-12-12","objectID":"/hackspaining/:11:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#allow-password-resets-via-email"},{"categories":null,"content":"Password Mismanagement Use Third-Party Authentication if Possible Ensure Password Complexity Allow Password Resets via Email Confirm Old Password On Reset Prevent Brute-Forcing Store Passwords With A Strong Hash, Salted Passwords should always be stored as salted hashes. Timeout Sessions After Inactivity, and Provide a Logout Function Use HTTPS for Secure Communication ","date":"2021-12-12","objectID":"/hackspaining/:11:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#confirm-old-password-on-reset"},{"categories":null,"content":"Password Mismanagement Use Third-Party Authentication if Possible Ensure Password Complexity Allow Password Resets via Email Confirm Old Password On Reset Prevent Brute-Forcing Store Passwords With A Strong Hash, Salted Passwords should always be stored as salted hashes. Timeout Sessions After Inactivity, and Provide a Logout Function Use HTTPS for Secure Communication ","date":"2021-12-12","objectID":"/hackspaining/:11:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#prevent-brute-forcing"},{"categories":null,"content":"Password Mismanagement Use Third-Party Authentication if Possible Ensure Password Complexity Allow Password Resets via Email Confirm Old Password On Reset Prevent Brute-Forcing Store Passwords With A Strong Hash, Salted Passwords should always be stored as salted hashes. Timeout Sessions After Inactivity, and Provide a Logout Function Use HTTPS for Secure Communication ","date":"2021-12-12","objectID":"/hackspaining/:11:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#store-passwords-with-a-strong-hash-salted"},{"categories":null,"content":"Password Mismanagement Use Third-Party Authentication if Possible Ensure Password Complexity Allow Password Resets via Email Confirm Old Password On Reset Prevent Brute-Forcing Store Passwords With A Strong Hash, Salted Passwords should always be stored as salted hashes. Timeout Sessions After Inactivity, and Provide a Logout Function Use HTTPS for Secure Communication ","date":"2021-12-12","objectID":"/hackspaining/:11:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#timeout-sessions-after-inactivity-and-provide-a-logout-function"},{"categories":null,"content":"Password Mismanagement Use Third-Party Authentication if Possible Ensure Password Complexity Allow Password Resets via Email Confirm Old Password On Reset Prevent Brute-Forcing Store Passwords With A Strong Hash, Salted Passwords should always be stored as salted hashes. Timeout Sessions After Inactivity, and Provide a Logout Function Use HTTPS for Secure Communication ","date":"2021-12-12","objectID":"/hackspaining/:11:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#use-https-for-secure-communication"},{"categories":null,"content":"Email Spoofing Steal their credentials by sending “phishing” messages. Trick them into falling for online scams by abusing the trust they have in your site. Spread malware by sharing malicious attachments. SPF Implement the Sender Policy Framework (SPF): publish a DNS record to explicitly state which servers are allowed to send email from your domain. Implement Domain Key Identified Mail (DKIM): use a digital signature to prove that outgoing email was legitimately sent from your domain, and that it wasn’t modified in transit. DMARC There is also an emerging umbrella standard called DMARC (“Domain-based Message Authentication, Reporting \u0026 Conformance”) that you should be aware of. Read more about DMARC here. ","date":"2021-12-12","objectID":"/hackspaining/:12:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#email-spoofing"},{"categories":null,"content":"Email Spoofing Steal their credentials by sending “phishing” messages. Trick them into falling for online scams by abusing the trust they have in your site. Spread malware by sharing malicious attachments. SPF Implement the Sender Policy Framework (SPF): publish a DNS record to explicitly state which servers are allowed to send email from your domain. Implement Domain Key Identified Mail (DKIM): use a digital signature to prove that outgoing email was legitimately sent from your domain, and that it wasn’t modified in transit. DMARC There is also an emerging umbrella standard called DMARC (“Domain-based Message Authentication, Reporting \u0026 Conformance”) that you should be aware of. Read more about DMARC here. ","date":"2021-12-12","objectID":"/hackspaining/:12:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#spf"},{"categories":null,"content":"Email Spoofing Steal their credentials by sending “phishing” messages. Trick them into falling for online scams by abusing the trust they have in your site. Spread malware by sharing malicious attachments. SPF Implement the Sender Policy Framework (SPF): publish a DNS record to explicitly state which servers are allowed to send email from your domain. Implement Domain Key Identified Mail (DKIM): use a digital signature to prove that outgoing email was legitimately sent from your domain, and that it wasn’t modified in transit. DMARC There is also an emerging umbrella standard called DMARC (“Domain-based Message Authentication, Reporting \u0026 Conformance”) that you should be aware of. Read more about DMARC here. ","date":"2021-12-12","objectID":"/hackspaining/:12:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#dmarc"},{"categories":null,"content":"configuration mplementing SPF and DKIM requires publishing new DNS records and making configuration changes to your technology stack - consult the documentation for your email sending service or software for details. Here are the relevant documentation links for some of the more common methods of sending email. Transactional Email Services Email Marketing Services Mail Transfer Agents ","date":"2021-12-12","objectID":"/hackspaining/:12:1","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#configuration"},{"categories":null,"content":"configuration mplementing SPF and DKIM requires publishing new DNS records and making configuration changes to your technology stack - consult the documentation for your email sending service or software for details. Here are the relevant documentation links for some of the more common methods of sending email. Transactional Email Services Email Marketing Services Mail Transfer Agents ","date":"2021-12-12","objectID":"/hackspaining/:12:1","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#transactional-email-services"},{"categories":null,"content":"configuration mplementing SPF and DKIM requires publishing new DNS records and making configuration changes to your technology stack - consult the documentation for your email sending service or software for details. Here are the relevant documentation links for some of the more common methods of sending email. Transactional Email Services Email Marketing Services Mail Transfer Agents ","date":"2021-12-12","objectID":"/hackspaining/:12:1","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#email-marketing-services"},{"categories":null,"content":"configuration mplementing SPF and DKIM requires publishing new DNS records and making configuration changes to your technology stack - consult the documentation for your email sending service or software for details. Here are the relevant documentation links for some of the more common methods of sending email. Transactional Email Services Email Marketing Services Mail Transfer Agents ","date":"2021-12-12","objectID":"/hackspaining/:12:1","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#mail-transfer-agents"},{"categories":null,"content":"Malvertising Malicious downloads, including ransomware. “Drive-by” downloads don’t even require the user to click on an advert - simply viewing the page may be enough to deliver the payload. Malware is usually delivered through vulnerable versions of Flash or Adobe Acrobat. Redirects to phishing sites that attempt to steal a user’s credentials. Scareware - adverts designed to trick a user into downloading unnecessary and potentially dangerous software, such as fake antivirus protection. Browser lockers - malware that locks up the browser, often posing as a security alert. ","date":"2021-12-12","objectID":"/hackspaining/:13:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#malvertising"},{"categories":null,"content":"Protection Working with reputable ad networks. Choose networks that are certified by e.g. Google. If you are evaluating a new ad network, see if they have any existing big-name clients. Avoid advertising networks that use deceptive practices pop-ups and pop-under windows. Performing due diligence on agencies and advertisers. Restrict your advertising to relevant market segments, and if your ad networks permits it, consider individually whitelisting advertisers. Implementing a content security policy. Implementing a Content-Security Policy will help control what domains can host content used in your web-pages. Unfortunately, many advertising toolkits (e.g. Google Adsense) cannot be restricted in this fashion - so you may have to create a “soft” whitelist using the Content-Security-Policy-Report-Only header, and monitor unexpected domains. Using client-side error reporting tools. Tools for recording errors in the browser - like Sentry, TrackJS, Rollbar and Airbrake - will help you detect unexpected and anomalous behavior that could indicate a malvertising infection. Logging out-going URLs. Capturing click-strings for adverts will help with forensic analysis in the case of a malvertising outbreak. ","date":"2021-12-12","objectID":"/hackspaining/:13:1","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#protection"},{"categories":null,"content":"Logging And Monitoring https://www.hacksplaining.com/prevention/logging-and-monitoring ","date":"2021-12-12","objectID":"/hackspaining/:14:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#logging-and-monitoring"},{"categories":null,"content":"Buffer Overflows Insecure Function Secure Alternative gets() fgets() strcpy() strncpy() strcat() strncat() sprintf() snprintf() It’s pretty rare for web-developers to write low-level code in languages like C or C++, so the biggest risk of buffer overflows for must of us in the applications we use. Web Servers Most websites are deployed using a web server to serve static content. (This is distinct from the application server that executes dynamic content.) The three most common web-servers are: Apache HTTP Server Microsoft Internet Information Services (IIS) Nginx Each of these has been found to be vulnerable to buffer overflows at different times. Web-server vendors are very quick to patch vulnerabilities, so the key to keeping yourself secure is deploying security patches as soon as they become available. Operating Systems and Language Runtimes Buffer overflow attacks have been launched against websites by taking advantage of vulnerabilities in operating systems and language runtimes. The Heartbleed attack took advantage of a serious vulnerability in the OpenSSL cryptographic software library that Linux-based web-servers use to encrypt SSL/TLS traffic. Similarly, security researchers have discovered vulnerabilities in various functions in the PHP runtime which allow attackers to launch buffer overflow attacks remotely by crafting malicious input. Remediation To avoid being exposed to buffer overflow vulnerabilities in the applications you use, you need to keep them up-to-date with the latest security patches. These are the key things to need to do: Automate your build and deployment process. You need to know which versions of each application your are running on each server. This means writing deployment scripts for web-servers and language runtimes, and retaining copies of deployment logs. Keep on top of security bulletins. Make sure your team is on the lookout for security announcements for the applications you use. Sign up for mailing lists, join forums, and follow software vendors on social media. Deploy security patches as soon as they become available! Hackers will find ways to take advantage of security vulnerabilities as soon as they are made public, so make sure you are not amongst the target audience. ","date":"2021-12-12","objectID":"/hackspaining/:15:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#buffer-overflows"},{"categories":null,"content":"Buffer Overflows Insecure Function Secure Alternative gets() fgets() strcpy() strncpy() strcat() strncat() sprintf() snprintf() It’s pretty rare for web-developers to write low-level code in languages like C or C++, so the biggest risk of buffer overflows for must of us in the applications we use. Web Servers Most websites are deployed using a web server to serve static content. (This is distinct from the application server that executes dynamic content.) The three most common web-servers are: Apache HTTP Server Microsoft Internet Information Services (IIS) Nginx Each of these has been found to be vulnerable to buffer overflows at different times. Web-server vendors are very quick to patch vulnerabilities, so the key to keeping yourself secure is deploying security patches as soon as they become available. Operating Systems and Language Runtimes Buffer overflow attacks have been launched against websites by taking advantage of vulnerabilities in operating systems and language runtimes. The Heartbleed attack took advantage of a serious vulnerability in the OpenSSL cryptographic software library that Linux-based web-servers use to encrypt SSL/TLS traffic. Similarly, security researchers have discovered vulnerabilities in various functions in the PHP runtime which allow attackers to launch buffer overflow attacks remotely by crafting malicious input. Remediation To avoid being exposed to buffer overflow vulnerabilities in the applications you use, you need to keep them up-to-date with the latest security patches. These are the key things to need to do: Automate your build and deployment process. You need to know which versions of each application your are running on each server. This means writing deployment scripts for web-servers and language runtimes, and retaining copies of deployment logs. Keep on top of security bulletins. Make sure your team is on the lookout for security announcements for the applications you use. Sign up for mailing lists, join forums, and follow software vendors on social media. Deploy security patches as soon as they become available! Hackers will find ways to take advantage of security vulnerabilities as soon as they are made public, so make sure you are not amongst the target audience. ","date":"2021-12-12","objectID":"/hackspaining/:15:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#web-servers"},{"categories":null,"content":"Buffer Overflows Insecure Function Secure Alternative gets() fgets() strcpy() strncpy() strcat() strncat() sprintf() snprintf() It’s pretty rare for web-developers to write low-level code in languages like C or C++, so the biggest risk of buffer overflows for must of us in the applications we use. Web Servers Most websites are deployed using a web server to serve static content. (This is distinct from the application server that executes dynamic content.) The three most common web-servers are: Apache HTTP Server Microsoft Internet Information Services (IIS) Nginx Each of these has been found to be vulnerable to buffer overflows at different times. Web-server vendors are very quick to patch vulnerabilities, so the key to keeping yourself secure is deploying security patches as soon as they become available. Operating Systems and Language Runtimes Buffer overflow attacks have been launched against websites by taking advantage of vulnerabilities in operating systems and language runtimes. The Heartbleed attack took advantage of a serious vulnerability in the OpenSSL cryptographic software library that Linux-based web-servers use to encrypt SSL/TLS traffic. Similarly, security researchers have discovered vulnerabilities in various functions in the PHP runtime which allow attackers to launch buffer overflow attacks remotely by crafting malicious input. Remediation To avoid being exposed to buffer overflow vulnerabilities in the applications you use, you need to keep them up-to-date with the latest security patches. These are the key things to need to do: Automate your build and deployment process. You need to know which versions of each application your are running on each server. This means writing deployment scripts for web-servers and language runtimes, and retaining copies of deployment logs. Keep on top of security bulletins. Make sure your team is on the lookout for security announcements for the applications you use. Sign up for mailing lists, join forums, and follow software vendors on social media. Deploy security patches as soon as they become available! Hackers will find ways to take advantage of security vulnerabilities as soon as they are made public, so make sure you are not amongst the target audience. ","date":"2021-12-12","objectID":"/hackspaining/:15:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#operating-systems-and-language-runtimes"},{"categories":null,"content":"Buffer Overflows Insecure Function Secure Alternative gets() fgets() strcpy() strncpy() strcat() strncat() sprintf() snprintf() It’s pretty rare for web-developers to write low-level code in languages like C or C++, so the biggest risk of buffer overflows for must of us in the applications we use. Web Servers Most websites are deployed using a web server to serve static content. (This is distinct from the application server that executes dynamic content.) The three most common web-servers are: Apache HTTP Server Microsoft Internet Information Services (IIS) Nginx Each of these has been found to be vulnerable to buffer overflows at different times. Web-server vendors are very quick to patch vulnerabilities, so the key to keeping yourself secure is deploying security patches as soon as they become available. Operating Systems and Language Runtimes Buffer overflow attacks have been launched against websites by taking advantage of vulnerabilities in operating systems and language runtimes. The Heartbleed attack took advantage of a serious vulnerability in the OpenSSL cryptographic software library that Linux-based web-servers use to encrypt SSL/TLS traffic. Similarly, security researchers have discovered vulnerabilities in various functions in the PHP runtime which allow attackers to launch buffer overflow attacks remotely by crafting malicious input. Remediation To avoid being exposed to buffer overflow vulnerabilities in the applications you use, you need to keep them up-to-date with the latest security patches. These are the key things to need to do: Automate your build and deployment process. You need to know which versions of each application your are running on each server. This means writing deployment scripts for web-servers and language runtimes, and retaining copies of deployment logs. Keep on top of security bulletins. Make sure your team is on the lookout for security announcements for the applications you use. Sign up for mailing lists, join forums, and follow software vendors on social media. Deploy security patches as soon as they become available! Hackers will find ways to take advantage of security vulnerabilities as soon as they are made public, so make sure you are not amongst the target audience. ","date":"2021-12-12","objectID":"/hackspaining/:15:0","series":null,"tags":["Security","Note"],"title":"Hacking Defense Note","uri":"/hackspaining/#remediation"},{"categories":null,"content":"Report: Setup Hadoop Cluster on MS Azure https://klasserom.azurewebsites.net/Lessons/Binder/2410 Notice: In this report, the ssh .pem is named SSH_keypair.pem, and the user name in Linux by default is set to PceWlkr. ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:0:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#report-setup-hadoop-cluster-on-ms-azure"},{"categories":null,"content":"set up virtual machine ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:1:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#set-up-virtual-machine"},{"categories":null,"content":"Basics use the B1s as the bare bone std The first vm set in the group will need to generate a new key pair of SSH public key, and give it a name. Others can just use the SSH we had already. Set the public inbound ports. (we set this up for testing purpose only, so the fact that all IP addrs will be allowed to access my vms doesn’t bother.) ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:1:1","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#basics"},{"categories":null,"content":"Disks go with standard SSD encryption type: default ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:1:2","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#disks"},{"categories":null,"content":"Networking virtual network: default subnet: default public IP: use the namenode ip NIC network security group: Basic Public inbound ports: allow selected ports inbound ports ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:1:3","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#networking"},{"categories":null,"content":"Management auto-shutdown: do this since we may forgot shut then down and lose money for that ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:1:4","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#management"},{"categories":null,"content":"Advanced ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:1:5","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#advanced"},{"categories":null,"content":"Tags ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:1:6","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#tags"},{"categories":null,"content":"Review and create review the price with extra cautions ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:1:7","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#review-and-create"},{"categories":null,"content":"Additional info it takes some time to create/ Download the private SSH key pair and store it carefully Better to have WinSCP available SFTP host name is the public ip port num is what we set before input the .pem file, which is the OpenSSH private key WinSCP will convert it to .ppk call this node namenode when connected, you can see the /home/PceWlkr/ Putty use putty to login the server sudo apt-get update //update the linux sys and everything installed ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:1:8","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#additional-info"},{"categories":null,"content":"Additional info it takes some time to create/ Download the private SSH key pair and store it carefully Better to have WinSCP available SFTP host name is the public ip port num is what we set before input the .pem file, which is the OpenSSH private key WinSCP will convert it to .ppk call this node namenode when connected, you can see the /home/PceWlkr/ Putty use putty to login the server sudo apt-get update //update the linux sys and everything installed ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:1:8","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#better-to-have-winscp-available"},{"categories":null,"content":"Additional info it takes some time to create/ Download the private SSH key pair and store it carefully Better to have WinSCP available SFTP host name is the public ip port num is what we set before input the .pem file, which is the OpenSSH private key WinSCP will convert it to .ppk call this node namenode when connected, you can see the /home/PceWlkr/ Putty use putty to login the server sudo apt-get update //update the linux sys and everything installed ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:1:8","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#putty"},{"categories":null,"content":"Add VMI to your Virtual Network Now we should have a public IP addr Everytime you restart the VM, it will give you a new public IP. Remember to update the IP you used for WinSCP. We are going to create more VMs as the DataNodes. Resource group: VirtualMachines name: DataNode001 image: Ubuntu Server 20.04 LTS Gen 1 Size: Std B1s Username: PceWlkr SSH: SSH_keypair inbound ports: SSH(22) OS disk type: std subnet: make it on the same network as last time security group: Basic (will adjust later) enable the auto-shutdown like before Go to Virtual Machines on the dash board, check if everything is there. Go to the namenode and copy the IP addr and then go the WinSCP to change the host. Clone it to a new site. Edit the newsite to register the datanode001 on WinSCP like before. Go the terminal to check if things are there. ![VM Network](img/VM Network.png) ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:2:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#add-vmi-to-your-virtual-network"},{"categories":null,"content":"Add a DNS to your VMI Go to the namenode and click the DNS name. Enter a DNS name label which must be unique that nobody else has reserved. Now we can use the DNS in the WinSCP instead of IP. ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:3:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#add-a-dns-to-your-vmi"},{"categories":null,"content":"Network Security Ports Go to the VM and then go to the Networking setting. Add inbound port rule. Service shoule be changed to SSH. Priority: 100 Name: SSH_22 Then add another inbound port for the remote desktop. Service:RDP Priority: 110 (lower then SSH) Name: RDP_3389 Also set up a PPK SSH for the namenode. Go to Advanced and Authentication, and go for the .pub in the private key file. Convert it and then save and login. ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:4:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#network-security-ports"},{"categories":null,"content":"Update/Upgrade Packages ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:5:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#updateupgrade-packages"},{"categories":null,"content":"Disable Unattended Upgrades sudo apt -y remove unattended-upgrades or if you do not want to remove Unattended-Upgrades, use sudo dpkg-reconfigure unattended-upgrades to turn on or off the features. ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:5:1","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#disable-unattended-upgrades"},{"categories":null,"content":"Update and Upgrade Packages sudo apt-get update -y \u0026\u0026 sudo apt-get upgrade -y ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:5:2","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#update-and-upgrade-packages"},{"categories":null,"content":"Hadoop Installation and Configuration You will learn the following: Administrate users in Ubuntu Manage environment variables in Ubuntu Use Secure Shell (SSH) to communicate with Virtual Machines Hadoop Prerequistes Java Developers Kit (JDK) pdsh rsync Install and Configure Hadoop Administrate Hadoop configuration files Test HDFS commands Test MapReduce using sample applications ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:6:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#hadoop-installation-and-configuration"},{"categories":null,"content":"Cluster Environment Setup We have done this: sudo apt -y remove unattended-upgrades \u0026\u0026 sudo apt-get update -y \u0026\u0026 sudo apt-get upgrade -y Secure Admin Account In this case, my admin account’s name is PceWlkr. sudo passwd PceWlkr Common Environment Variables for Hadoop Clusters In general, these include: Java - JAVA_HOME Hadoop - HADOOP_HOME Hive - HIVE_HOME Pig - PIG_HOME The etc/profile.d dir holding shell script will be run at start-up and install fuatures for all users. So we’ll create a file bigdata.sh for later installations. sudo touch /etc/profile.d/bigdata.sh export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=\"/.ssh/SSH_keypair.pem\" export SSHConfigFile=\"/.ssh/config\" sudo rm $BigDataSH echo -e '#!/bin/bash \\n' | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"# Environment Variables for Big Data tools\\n\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export BigDataSH=${BigDataSH}\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export IdentityFile=~${IdentityFile}\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export SSHConfigFile=~${SSHConfigFile}\\n\" | sudo tee --append $BigDataSH \u003e /dev/null sudo chmod 644 $BigDataSH cat $BigDataSH The result should be: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=~/.ssh/SSH_keypair.pem export SSHConfigFile=~/.ssh/config sudo reboot After rebooting, confirm the vars exist. echo $BigDataSH $IdentityFile $SSHConfigFile The result should be: /etc/profile.d/bigdata.sh /home/PceWlkr/.ssh/SSH_keypair.pem /home/PceWlkr/.ssh/config Map Virtual Machine Cluster Environment Variables Collect the Public DNS and “Internal” IP addresses from your Virtual Machine Instances Copy and paste the following commands into a text editor: export NameNodeDNS=\"Namenode\" export DataNode001DNS=\"Datanode001\" export DataNode002DNS=\"Datanode002\" export DataNode003DNS=\"Datanode003\" export DataNode004DNS=\"Datanode004\" export NameNodeIP=\"10.0.0.4\" export DataNode001IP=\"10.0.0.5\" export DataNode002IP=\"10.0.0.6\" export DataNode003IP=\"10.0.0.7\" Notice: you can use hostname and hostname -I to find the hostname and private IP Copy all of the commands above from the text editor Paste the commands into the SSH session at the command line for your Virtual Machine Instance When you pasted the commands, they were executed automatically. Try to use one of the variables we set: echo $DataNode001DNS The result should look something like this: Datanode001 Execute the following to add the schema to the bigdata.sh file: echo -e \"# Cluster Variables START\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export NameNodeDNS=\\\"${NameNodeDNS}\\\"\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export DataNode001DNS=\\\"${DataNode001DNS}\\\"\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export DataNode002DNS=\\\"${DataNode002DNS}\\\"\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export DataNode003DNS=\\\"${DataNode003DNS}\\\"\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export DataNode004DNS=\\\"${DataNode004DNS}\\\"\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export NameNodeIP=\\\"${NameNodeIP}\\\"\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export DataNode001IP=\\\"${DataNode001IP}\\\"\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export DataNode002IP=\\\"${DataNode002IP}\\\"\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export DataNode003IP=\\\"${DataNode003IP}\\\"\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export DataNode004IP=\\\"${DataNode004IP}\\\"\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"# Cluster Variables END\" | sudo tee --append $BigDataSH \u003e /dev/null Then Display the contents of the $BigDataSh file: cat $BigDataSH result should be like: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=~/.ssh/SSH_keypair.pem export SSHConfigFile=~/.ssh/config # Cluster Variable","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:6:1","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#cluster-environment-setup"},{"categories":null,"content":"Cluster Environment Setup We have done this: sudo apt -y remove unattended-upgrades \u0026\u0026 sudo apt-get update -y \u0026\u0026 sudo apt-get upgrade -y Secure Admin Account In this case, my admin account’s name is PceWlkr. sudo passwd PceWlkr Common Environment Variables for Hadoop Clusters In general, these include: Java - JAVA_HOME Hadoop - HADOOP_HOME Hive - HIVE_HOME Pig - PIG_HOME The etc/profile.d dir holding shell script will be run at start-up and install fuatures for all users. So we’ll create a file bigdata.sh for later installations. sudo touch /etc/profile.d/bigdata.sh export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=\"/.ssh/SSH_keypair.pem\" export SSHConfigFile=\"/.ssh/config\" sudo rm $BigDataSH echo -e '#!/bin/bash \\n' | sudo tee --append $BigDataSH /dev/null echo -e \"# Environment Variables for Big Data tools\\n\" | sudo tee --append $BigDataSH /dev/null echo -e \"export BigDataSH=${BigDataSH}\" | sudo tee --append $BigDataSH /dev/null echo -e \"export IdentityFile=~${IdentityFile}\" | sudo tee --append $BigDataSH /dev/null echo -e \"export SSHConfigFile=~${SSHConfigFile}\\n\" | sudo tee --append $BigDataSH /dev/null sudo chmod 644 $BigDataSH cat $BigDataSH The result should be: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=~/.ssh/SSH_keypair.pem export SSHConfigFile=~/.ssh/config sudo reboot After rebooting, confirm the vars exist. echo $BigDataSH $IdentityFile $SSHConfigFile The result should be: /etc/profile.d/bigdata.sh /home/PceWlkr/.ssh/SSH_keypair.pem /home/PceWlkr/.ssh/config Map Virtual Machine Cluster Environment Variables Collect the Public DNS and “Internal” IP addresses from your Virtual Machine Instances Copy and paste the following commands into a text editor: export NameNodeDNS=\"Namenode\" export DataNode001DNS=\"Datanode001\" export DataNode002DNS=\"Datanode002\" export DataNode003DNS=\"Datanode003\" export DataNode004DNS=\"Datanode004\" export NameNodeIP=\"10.0.0.4\" export DataNode001IP=\"10.0.0.5\" export DataNode002IP=\"10.0.0.6\" export DataNode003IP=\"10.0.0.7\" Notice: you can use hostname and hostname -I to find the hostname and private IP Copy all of the commands above from the text editor Paste the commands into the SSH session at the command line for your Virtual Machine Instance When you pasted the commands, they were executed automatically. Try to use one of the variables we set: echo $DataNode001DNS The result should look something like this: Datanode001 Execute the following to add the schema to the bigdata.sh file: echo -e \"# Cluster Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export NameNodeDNS=\\\"${NameNodeDNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode001DNS=\\\"${DataNode001DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode002DNS=\\\"${DataNode002DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode003DNS=\\\"${DataNode003DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode004DNS=\\\"${DataNode004DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export NameNodeIP=\\\"${NameNodeIP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode001IP=\\\"${DataNode001IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode002IP=\\\"${DataNode002IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode003IP=\\\"${DataNode003IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode004IP=\\\"${DataNode004IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"# Cluster Variables END\" | sudo tee --append $BigDataSH /dev/null Then Display the contents of the $BigDataSh file: cat $BigDataSH result should be like: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=~/.ssh/SSH_keypair.pem export SSHConfigFile=~/.ssh/config # Cluster Variable","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:6:1","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#secure-admin-account"},{"categories":null,"content":"Cluster Environment Setup We have done this: sudo apt -y remove unattended-upgrades \u0026\u0026 sudo apt-get update -y \u0026\u0026 sudo apt-get upgrade -y Secure Admin Account In this case, my admin account’s name is PceWlkr. sudo passwd PceWlkr Common Environment Variables for Hadoop Clusters In general, these include: Java - JAVA_HOME Hadoop - HADOOP_HOME Hive - HIVE_HOME Pig - PIG_HOME The etc/profile.d dir holding shell script will be run at start-up and install fuatures for all users. So we’ll create a file bigdata.sh for later installations. sudo touch /etc/profile.d/bigdata.sh export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=\"/.ssh/SSH_keypair.pem\" export SSHConfigFile=\"/.ssh/config\" sudo rm $BigDataSH echo -e '#!/bin/bash \\n' | sudo tee --append $BigDataSH /dev/null echo -e \"# Environment Variables for Big Data tools\\n\" | sudo tee --append $BigDataSH /dev/null echo -e \"export BigDataSH=${BigDataSH}\" | sudo tee --append $BigDataSH /dev/null echo -e \"export IdentityFile=~${IdentityFile}\" | sudo tee --append $BigDataSH /dev/null echo -e \"export SSHConfigFile=~${SSHConfigFile}\\n\" | sudo tee --append $BigDataSH /dev/null sudo chmod 644 $BigDataSH cat $BigDataSH The result should be: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=~/.ssh/SSH_keypair.pem export SSHConfigFile=~/.ssh/config sudo reboot After rebooting, confirm the vars exist. echo $BigDataSH $IdentityFile $SSHConfigFile The result should be: /etc/profile.d/bigdata.sh /home/PceWlkr/.ssh/SSH_keypair.pem /home/PceWlkr/.ssh/config Map Virtual Machine Cluster Environment Variables Collect the Public DNS and “Internal” IP addresses from your Virtual Machine Instances Copy and paste the following commands into a text editor: export NameNodeDNS=\"Namenode\" export DataNode001DNS=\"Datanode001\" export DataNode002DNS=\"Datanode002\" export DataNode003DNS=\"Datanode003\" export DataNode004DNS=\"Datanode004\" export NameNodeIP=\"10.0.0.4\" export DataNode001IP=\"10.0.0.5\" export DataNode002IP=\"10.0.0.6\" export DataNode003IP=\"10.0.0.7\" Notice: you can use hostname and hostname -I to find the hostname and private IP Copy all of the commands above from the text editor Paste the commands into the SSH session at the command line for your Virtual Machine Instance When you pasted the commands, they were executed automatically. Try to use one of the variables we set: echo $DataNode001DNS The result should look something like this: Datanode001 Execute the following to add the schema to the bigdata.sh file: echo -e \"# Cluster Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export NameNodeDNS=\\\"${NameNodeDNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode001DNS=\\\"${DataNode001DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode002DNS=\\\"${DataNode002DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode003DNS=\\\"${DataNode003DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode004DNS=\\\"${DataNode004DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export NameNodeIP=\\\"${NameNodeIP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode001IP=\\\"${DataNode001IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode002IP=\\\"${DataNode002IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode003IP=\\\"${DataNode003IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode004IP=\\\"${DataNode004IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"# Cluster Variables END\" | sudo tee --append $BigDataSH /dev/null Then Display the contents of the $BigDataSh file: cat $BigDataSH result should be like: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=~/.ssh/SSH_keypair.pem export SSHConfigFile=~/.ssh/config # Cluster Variable","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:6:1","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#common-environment-variables-for-hadoop-clusters"},{"categories":null,"content":"Cluster Environment Setup We have done this: sudo apt -y remove unattended-upgrades \u0026\u0026 sudo apt-get update -y \u0026\u0026 sudo apt-get upgrade -y Secure Admin Account In this case, my admin account’s name is PceWlkr. sudo passwd PceWlkr Common Environment Variables for Hadoop Clusters In general, these include: Java - JAVA_HOME Hadoop - HADOOP_HOME Hive - HIVE_HOME Pig - PIG_HOME The etc/profile.d dir holding shell script will be run at start-up and install fuatures for all users. So we’ll create a file bigdata.sh for later installations. sudo touch /etc/profile.d/bigdata.sh export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=\"/.ssh/SSH_keypair.pem\" export SSHConfigFile=\"/.ssh/config\" sudo rm $BigDataSH echo -e '#!/bin/bash \\n' | sudo tee --append $BigDataSH /dev/null echo -e \"# Environment Variables for Big Data tools\\n\" | sudo tee --append $BigDataSH /dev/null echo -e \"export BigDataSH=${BigDataSH}\" | sudo tee --append $BigDataSH /dev/null echo -e \"export IdentityFile=~${IdentityFile}\" | sudo tee --append $BigDataSH /dev/null echo -e \"export SSHConfigFile=~${SSHConfigFile}\\n\" | sudo tee --append $BigDataSH /dev/null sudo chmod 644 $BigDataSH cat $BigDataSH The result should be: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=~/.ssh/SSH_keypair.pem export SSHConfigFile=~/.ssh/config sudo reboot After rebooting, confirm the vars exist. echo $BigDataSH $IdentityFile $SSHConfigFile The result should be: /etc/profile.d/bigdata.sh /home/PceWlkr/.ssh/SSH_keypair.pem /home/PceWlkr/.ssh/config Map Virtual Machine Cluster Environment Variables Collect the Public DNS and “Internal” IP addresses from your Virtual Machine Instances Copy and paste the following commands into a text editor: export NameNodeDNS=\"Namenode\" export DataNode001DNS=\"Datanode001\" export DataNode002DNS=\"Datanode002\" export DataNode003DNS=\"Datanode003\" export DataNode004DNS=\"Datanode004\" export NameNodeIP=\"10.0.0.4\" export DataNode001IP=\"10.0.0.5\" export DataNode002IP=\"10.0.0.6\" export DataNode003IP=\"10.0.0.7\" Notice: you can use hostname and hostname -I to find the hostname and private IP Copy all of the commands above from the text editor Paste the commands into the SSH session at the command line for your Virtual Machine Instance When you pasted the commands, they were executed automatically. Try to use one of the variables we set: echo $DataNode001DNS The result should look something like this: Datanode001 Execute the following to add the schema to the bigdata.sh file: echo -e \"# Cluster Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export NameNodeDNS=\\\"${NameNodeDNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode001DNS=\\\"${DataNode001DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode002DNS=\\\"${DataNode002DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode003DNS=\\\"${DataNode003DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode004DNS=\\\"${DataNode004DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export NameNodeIP=\\\"${NameNodeIP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode001IP=\\\"${DataNode001IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode002IP=\\\"${DataNode002IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode003IP=\\\"${DataNode003IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode004IP=\\\"${DataNode004IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"# Cluster Variables END\" | sudo tee --append $BigDataSH /dev/null Then Display the contents of the $BigDataSh file: cat $BigDataSH result should be like: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=~/.ssh/SSH_keypair.pem export SSHConfigFile=~/.ssh/config # Cluster Variable","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:6:1","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#map-virtual-machine-cluster-environment-variables"},{"categories":null,"content":"Cluster Environment Setup We have done this: sudo apt -y remove unattended-upgrades \u0026\u0026 sudo apt-get update -y \u0026\u0026 sudo apt-get upgrade -y Secure Admin Account In this case, my admin account’s name is PceWlkr. sudo passwd PceWlkr Common Environment Variables for Hadoop Clusters In general, these include: Java - JAVA_HOME Hadoop - HADOOP_HOME Hive - HIVE_HOME Pig - PIG_HOME The etc/profile.d dir holding shell script will be run at start-up and install fuatures for all users. So we’ll create a file bigdata.sh for later installations. sudo touch /etc/profile.d/bigdata.sh export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=\"/.ssh/SSH_keypair.pem\" export SSHConfigFile=\"/.ssh/config\" sudo rm $BigDataSH echo -e '#!/bin/bash \\n' | sudo tee --append $BigDataSH /dev/null echo -e \"# Environment Variables for Big Data tools\\n\" | sudo tee --append $BigDataSH /dev/null echo -e \"export BigDataSH=${BigDataSH}\" | sudo tee --append $BigDataSH /dev/null echo -e \"export IdentityFile=~${IdentityFile}\" | sudo tee --append $BigDataSH /dev/null echo -e \"export SSHConfigFile=~${SSHConfigFile}\\n\" | sudo tee --append $BigDataSH /dev/null sudo chmod 644 $BigDataSH cat $BigDataSH The result should be: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=~/.ssh/SSH_keypair.pem export SSHConfigFile=~/.ssh/config sudo reboot After rebooting, confirm the vars exist. echo $BigDataSH $IdentityFile $SSHConfigFile The result should be: /etc/profile.d/bigdata.sh /home/PceWlkr/.ssh/SSH_keypair.pem /home/PceWlkr/.ssh/config Map Virtual Machine Cluster Environment Variables Collect the Public DNS and “Internal” IP addresses from your Virtual Machine Instances Copy and paste the following commands into a text editor: export NameNodeDNS=\"Namenode\" export DataNode001DNS=\"Datanode001\" export DataNode002DNS=\"Datanode002\" export DataNode003DNS=\"Datanode003\" export DataNode004DNS=\"Datanode004\" export NameNodeIP=\"10.0.0.4\" export DataNode001IP=\"10.0.0.5\" export DataNode002IP=\"10.0.0.6\" export DataNode003IP=\"10.0.0.7\" Notice: you can use hostname and hostname -I to find the hostname and private IP Copy all of the commands above from the text editor Paste the commands into the SSH session at the command line for your Virtual Machine Instance When you pasted the commands, they were executed automatically. Try to use one of the variables we set: echo $DataNode001DNS The result should look something like this: Datanode001 Execute the following to add the schema to the bigdata.sh file: echo -e \"# Cluster Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export NameNodeDNS=\\\"${NameNodeDNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode001DNS=\\\"${DataNode001DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode002DNS=\\\"${DataNode002DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode003DNS=\\\"${DataNode003DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode004DNS=\\\"${DataNode004DNS}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export NameNodeIP=\\\"${NameNodeIP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode001IP=\\\"${DataNode001IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode002IP=\\\"${DataNode002IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode003IP=\\\"${DataNode003IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export DataNode004IP=\\\"${DataNode004IP}\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"# Cluster Variables END\" | sudo tee --append $BigDataSH /dev/null Then Display the contents of the $BigDataSh file: cat $BigDataSH result should be like: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=~/.ssh/SSH_keypair.pem export SSHConfigFile=~/.ssh/config # Cluster Variable","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:6:1","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#passwordless-ssh-for-cluster-using-config-and-pem-files"},{"categories":null,"content":"Java Developers Kit (JDK) sudo apt-get -y update sudo apt-get -y install default-jdk confirm you have it: cd /usr/lib/jvm/ \u0026\u0026 ls result should be like: default-java java-1.11.0-openjdk-amd64 java-11-openjdk-amd64 openjdk-11 Add Environment Variables to /etc/profile.d/bigdata.sh Open the file /etc/profile.d/bigdata.sh sudo nano $BigDataSH Add these lines to the end of the bigdata.sh file: export JAVA_HOME=/usr/lib/jvm/default-java PATH=$PATH:$JAVA_HOME/bin Or, copy the following code and paste it at the command line. This will echo the entries into the bigdata.sh file echo \"# JAVA Variables START\" | sudo tee --append $BigDataSH \u003e /dev/null echo \"export JAVA_HOME=/usr/lib/jvm/default-java\" | sudo tee --append $BigDataSH \u003e /dev/null echo \"PATH=\\$PATH:\\$JAVA_HOME/bin\" | sudo tee --append $BigDataSH \u003e /dev/null echo \"# JAVA Variables END\" | sudo tee --append $BigDataSH \u003e /dev/null Confirm that your Java variables were added, open the /etc/profile.d/bigdata.sh file: cat $BigDataSH Your bigdata.sh file output should look like this now: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=/etc/ssh/ssh_config.d/SSH_keypair.pem export SSHConfigFile=/etc/ssh/ssh_config.d/config.conf # JAVA Variables START export JAVA_HOME=/usr/lib/jvm/default-java PATH=$PATH:$JAVA_HOME/bin # JAVA Variables END sudo reboot Confirm the Java Version and Environment Variables java -version result should be like: openjdk version \"11.0.11\" 2021-04-20 OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04) OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing) confirm you have it: echo $JAVA_HOME result should be like: /usr/lib/jvm/default-java Java Developers Kit (JDK) for DataNodes in a Cluster To send a command to a DateNode using SSH, we wrap the command with quotes \"\" and ssh into the Node: ssh DataNode001 \"{command}\" Update packages on the server ssh Datanode001 \"sudo apt-get -y update\" ssh Datanode001 \"sudo apt-get -y install default-jdk\" Notice: You will synchronize your DataNodes with the NameNode later, so you do not need to set up environment variables yet. pdsh pdsh is a variant of the rsh(1) command. Unlike rsh(1), which runs commands on a single remote host, pdsh can run multiple remote commands in parallel. pdsh uses a \"sliding window\" (or fanout) of threads to conserve resources on the initiating host while allowing some connections to time out. sudo apt-get -y install pdsh Add the following variable to your bigdata.sh file: export PDSH_RCMD_TYPE=ssh or run this commands: echo -e \"# PDSH Variables START\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export PDSH_RCMD_TYPE=ssh\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"# PDSH Variables END\" | sudo tee --append $BigDataSH \u003e /dev/null sudo reboot rsync rsync is a utility for efficiently transferring and synchronizing files across computer systems by checking the timestamp and size of files. It is commonly found on Unix-like systems and functions as both a file synchronization and file transfer program. The rsync algorithm is a type of delta encoding and is used for minimizing network usage. Zlib may be used for additional compression, and SSH or stunnel can be used for data security. rsync(1) - Linux man page sudo apt-get -y install rsync sudo reboot ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:6:2","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#java-developers-kit-jdk"},{"categories":null,"content":"Java Developers Kit (JDK) sudo apt-get -y update sudo apt-get -y install default-jdk confirm you have it: cd /usr/lib/jvm/ \u0026\u0026 ls result should be like: default-java java-1.11.0-openjdk-amd64 java-11-openjdk-amd64 openjdk-11 Add Environment Variables to /etc/profile.d/bigdata.sh Open the file /etc/profile.d/bigdata.sh sudo nano $BigDataSH Add these lines to the end of the bigdata.sh file: export JAVA_HOME=/usr/lib/jvm/default-java PATH=$PATH:$JAVA_HOME/bin Or, copy the following code and paste it at the command line. This will echo the entries into the bigdata.sh file echo \"# JAVA Variables START\" | sudo tee --append $BigDataSH /dev/null echo \"export JAVA_HOME=/usr/lib/jvm/default-java\" | sudo tee --append $BigDataSH /dev/null echo \"PATH=\\$PATH:\\$JAVA_HOME/bin\" | sudo tee --append $BigDataSH /dev/null echo \"# JAVA Variables END\" | sudo tee --append $BigDataSH /dev/null Confirm that your Java variables were added, open the /etc/profile.d/bigdata.sh file: cat $BigDataSH Your bigdata.sh file output should look like this now: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=/etc/ssh/ssh_config.d/SSH_keypair.pem export SSHConfigFile=/etc/ssh/ssh_config.d/config.conf # JAVA Variables START export JAVA_HOME=/usr/lib/jvm/default-java PATH=$PATH:$JAVA_HOME/bin # JAVA Variables END sudo reboot Confirm the Java Version and Environment Variables java -version result should be like: openjdk version \"11.0.11\" 2021-04-20 OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04) OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing) confirm you have it: echo $JAVA_HOME result should be like: /usr/lib/jvm/default-java Java Developers Kit (JDK) for DataNodes in a Cluster To send a command to a DateNode using SSH, we wrap the command with quotes \"\" and ssh into the Node: ssh DataNode001 \"{command}\" Update packages on the server ssh Datanode001 \"sudo apt-get -y update\" ssh Datanode001 \"sudo apt-get -y install default-jdk\" Notice: You will synchronize your DataNodes with the NameNode later, so you do not need to set up environment variables yet. pdsh pdsh is a variant of the rsh(1) command. Unlike rsh(1), which runs commands on a single remote host, pdsh can run multiple remote commands in parallel. pdsh uses a \"sliding window\" (or fanout) of threads to conserve resources on the initiating host while allowing some connections to time out. sudo apt-get -y install pdsh Add the following variable to your bigdata.sh file: export PDSH_RCMD_TYPE=ssh or run this commands: echo -e \"# PDSH Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export PDSH_RCMD_TYPE=ssh\" | sudo tee --append $BigDataSH /dev/null echo -e \"# PDSH Variables END\" | sudo tee --append $BigDataSH /dev/null sudo reboot rsync rsync is a utility for efficiently transferring and synchronizing files across computer systems by checking the timestamp and size of files. It is commonly found on Unix-like systems and functions as both a file synchronization and file transfer program. The rsync algorithm is a type of delta encoding and is used for minimizing network usage. Zlib may be used for additional compression, and SSH or stunnel can be used for data security. rsync(1) - Linux man page sudo apt-get -y install rsync sudo reboot ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:6:2","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#add-environment-variables-to-etcprofiledbigdatash"},{"categories":null,"content":"Java Developers Kit (JDK) sudo apt-get -y update sudo apt-get -y install default-jdk confirm you have it: cd /usr/lib/jvm/ \u0026\u0026 ls result should be like: default-java java-1.11.0-openjdk-amd64 java-11-openjdk-amd64 openjdk-11 Add Environment Variables to /etc/profile.d/bigdata.sh Open the file /etc/profile.d/bigdata.sh sudo nano $BigDataSH Add these lines to the end of the bigdata.sh file: export JAVA_HOME=/usr/lib/jvm/default-java PATH=$PATH:$JAVA_HOME/bin Or, copy the following code and paste it at the command line. This will echo the entries into the bigdata.sh file echo \"# JAVA Variables START\" | sudo tee --append $BigDataSH /dev/null echo \"export JAVA_HOME=/usr/lib/jvm/default-java\" | sudo tee --append $BigDataSH /dev/null echo \"PATH=\\$PATH:\\$JAVA_HOME/bin\" | sudo tee --append $BigDataSH /dev/null echo \"# JAVA Variables END\" | sudo tee --append $BigDataSH /dev/null Confirm that your Java variables were added, open the /etc/profile.d/bigdata.sh file: cat $BigDataSH Your bigdata.sh file output should look like this now: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=/etc/ssh/ssh_config.d/SSH_keypair.pem export SSHConfigFile=/etc/ssh/ssh_config.d/config.conf # JAVA Variables START export JAVA_HOME=/usr/lib/jvm/default-java PATH=$PATH:$JAVA_HOME/bin # JAVA Variables END sudo reboot Confirm the Java Version and Environment Variables java -version result should be like: openjdk version \"11.0.11\" 2021-04-20 OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04) OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing) confirm you have it: echo $JAVA_HOME result should be like: /usr/lib/jvm/default-java Java Developers Kit (JDK) for DataNodes in a Cluster To send a command to a DateNode using SSH, we wrap the command with quotes \"\" and ssh into the Node: ssh DataNode001 \"{command}\" Update packages on the server ssh Datanode001 \"sudo apt-get -y update\" ssh Datanode001 \"sudo apt-get -y install default-jdk\" Notice: You will synchronize your DataNodes with the NameNode later, so you do not need to set up environment variables yet. pdsh pdsh is a variant of the rsh(1) command. Unlike rsh(1), which runs commands on a single remote host, pdsh can run multiple remote commands in parallel. pdsh uses a \"sliding window\" (or fanout) of threads to conserve resources on the initiating host while allowing some connections to time out. sudo apt-get -y install pdsh Add the following variable to your bigdata.sh file: export PDSH_RCMD_TYPE=ssh or run this commands: echo -e \"# PDSH Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export PDSH_RCMD_TYPE=ssh\" | sudo tee --append $BigDataSH /dev/null echo -e \"# PDSH Variables END\" | sudo tee --append $BigDataSH /dev/null sudo reboot rsync rsync is a utility for efficiently transferring and synchronizing files across computer systems by checking the timestamp and size of files. It is commonly found on Unix-like systems and functions as both a file synchronization and file transfer program. The rsync algorithm is a type of delta encoding and is used for minimizing network usage. Zlib may be used for additional compression, and SSH or stunnel can be used for data security. rsync(1) - Linux man page sudo apt-get -y install rsync sudo reboot ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:6:2","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#confirm-the-java-version-and-environment-variables"},{"categories":null,"content":"Java Developers Kit (JDK) sudo apt-get -y update sudo apt-get -y install default-jdk confirm you have it: cd /usr/lib/jvm/ \u0026\u0026 ls result should be like: default-java java-1.11.0-openjdk-amd64 java-11-openjdk-amd64 openjdk-11 Add Environment Variables to /etc/profile.d/bigdata.sh Open the file /etc/profile.d/bigdata.sh sudo nano $BigDataSH Add these lines to the end of the bigdata.sh file: export JAVA_HOME=/usr/lib/jvm/default-java PATH=$PATH:$JAVA_HOME/bin Or, copy the following code and paste it at the command line. This will echo the entries into the bigdata.sh file echo \"# JAVA Variables START\" | sudo tee --append $BigDataSH /dev/null echo \"export JAVA_HOME=/usr/lib/jvm/default-java\" | sudo tee --append $BigDataSH /dev/null echo \"PATH=\\$PATH:\\$JAVA_HOME/bin\" | sudo tee --append $BigDataSH /dev/null echo \"# JAVA Variables END\" | sudo tee --append $BigDataSH /dev/null Confirm that your Java variables were added, open the /etc/profile.d/bigdata.sh file: cat $BigDataSH Your bigdata.sh file output should look like this now: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=/etc/ssh/ssh_config.d/SSH_keypair.pem export SSHConfigFile=/etc/ssh/ssh_config.d/config.conf # JAVA Variables START export JAVA_HOME=/usr/lib/jvm/default-java PATH=$PATH:$JAVA_HOME/bin # JAVA Variables END sudo reboot Confirm the Java Version and Environment Variables java -version result should be like: openjdk version \"11.0.11\" 2021-04-20 OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04) OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing) confirm you have it: echo $JAVA_HOME result should be like: /usr/lib/jvm/default-java Java Developers Kit (JDK) for DataNodes in a Cluster To send a command to a DateNode using SSH, we wrap the command with quotes \"\" and ssh into the Node: ssh DataNode001 \"{command}\" Update packages on the server ssh Datanode001 \"sudo apt-get -y update\" ssh Datanode001 \"sudo apt-get -y install default-jdk\" Notice: You will synchronize your DataNodes with the NameNode later, so you do not need to set up environment variables yet. pdsh pdsh is a variant of the rsh(1) command. Unlike rsh(1), which runs commands on a single remote host, pdsh can run multiple remote commands in parallel. pdsh uses a \"sliding window\" (or fanout) of threads to conserve resources on the initiating host while allowing some connections to time out. sudo apt-get -y install pdsh Add the following variable to your bigdata.sh file: export PDSH_RCMD_TYPE=ssh or run this commands: echo -e \"# PDSH Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export PDSH_RCMD_TYPE=ssh\" | sudo tee --append $BigDataSH /dev/null echo -e \"# PDSH Variables END\" | sudo tee --append $BigDataSH /dev/null sudo reboot rsync rsync is a utility for efficiently transferring and synchronizing files across computer systems by checking the timestamp and size of files. It is commonly found on Unix-like systems and functions as both a file synchronization and file transfer program. The rsync algorithm is a type of delta encoding and is used for minimizing network usage. Zlib may be used for additional compression, and SSH or stunnel can be used for data security. rsync(1) - Linux man page sudo apt-get -y install rsync sudo reboot ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:6:2","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#java-developers-kit-jdk-for-datanodes-in-a-cluster"},{"categories":null,"content":"Java Developers Kit (JDK) sudo apt-get -y update sudo apt-get -y install default-jdk confirm you have it: cd /usr/lib/jvm/ \u0026\u0026 ls result should be like: default-java java-1.11.0-openjdk-amd64 java-11-openjdk-amd64 openjdk-11 Add Environment Variables to /etc/profile.d/bigdata.sh Open the file /etc/profile.d/bigdata.sh sudo nano $BigDataSH Add these lines to the end of the bigdata.sh file: export JAVA_HOME=/usr/lib/jvm/default-java PATH=$PATH:$JAVA_HOME/bin Or, copy the following code and paste it at the command line. This will echo the entries into the bigdata.sh file echo \"# JAVA Variables START\" | sudo tee --append $BigDataSH /dev/null echo \"export JAVA_HOME=/usr/lib/jvm/default-java\" | sudo tee --append $BigDataSH /dev/null echo \"PATH=\\$PATH:\\$JAVA_HOME/bin\" | sudo tee --append $BigDataSH /dev/null echo \"# JAVA Variables END\" | sudo tee --append $BigDataSH /dev/null Confirm that your Java variables were added, open the /etc/profile.d/bigdata.sh file: cat $BigDataSH Your bigdata.sh file output should look like this now: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=/etc/ssh/ssh_config.d/SSH_keypair.pem export SSHConfigFile=/etc/ssh/ssh_config.d/config.conf # JAVA Variables START export JAVA_HOME=/usr/lib/jvm/default-java PATH=$PATH:$JAVA_HOME/bin # JAVA Variables END sudo reboot Confirm the Java Version and Environment Variables java -version result should be like: openjdk version \"11.0.11\" 2021-04-20 OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04) OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing) confirm you have it: echo $JAVA_HOME result should be like: /usr/lib/jvm/default-java Java Developers Kit (JDK) for DataNodes in a Cluster To send a command to a DateNode using SSH, we wrap the command with quotes \"\" and ssh into the Node: ssh DataNode001 \"{command}\" Update packages on the server ssh Datanode001 \"sudo apt-get -y update\" ssh Datanode001 \"sudo apt-get -y install default-jdk\" Notice: You will synchronize your DataNodes with the NameNode later, so you do not need to set up environment variables yet. pdsh pdsh is a variant of the rsh(1) command. Unlike rsh(1), which runs commands on a single remote host, pdsh can run multiple remote commands in parallel. pdsh uses a \"sliding window\" (or fanout) of threads to conserve resources on the initiating host while allowing some connections to time out. sudo apt-get -y install pdsh Add the following variable to your bigdata.sh file: export PDSH_RCMD_TYPE=ssh or run this commands: echo -e \"# PDSH Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export PDSH_RCMD_TYPE=ssh\" | sudo tee --append $BigDataSH /dev/null echo -e \"# PDSH Variables END\" | sudo tee --append $BigDataSH /dev/null sudo reboot rsync rsync is a utility for efficiently transferring and synchronizing files across computer systems by checking the timestamp and size of files. It is commonly found on Unix-like systems and functions as both a file synchronization and file transfer program. The rsync algorithm is a type of delta encoding and is used for minimizing network usage. Zlib may be used for additional compression, and SSH or stunnel can be used for data security. rsync(1) - Linux man page sudo apt-get -y install rsync sudo reboot ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:6:2","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#update-packages-on-the-server"},{"categories":null,"content":"Java Developers Kit (JDK) sudo apt-get -y update sudo apt-get -y install default-jdk confirm you have it: cd /usr/lib/jvm/ \u0026\u0026 ls result should be like: default-java java-1.11.0-openjdk-amd64 java-11-openjdk-amd64 openjdk-11 Add Environment Variables to /etc/profile.d/bigdata.sh Open the file /etc/profile.d/bigdata.sh sudo nano $BigDataSH Add these lines to the end of the bigdata.sh file: export JAVA_HOME=/usr/lib/jvm/default-java PATH=$PATH:$JAVA_HOME/bin Or, copy the following code and paste it at the command line. This will echo the entries into the bigdata.sh file echo \"# JAVA Variables START\" | sudo tee --append $BigDataSH /dev/null echo \"export JAVA_HOME=/usr/lib/jvm/default-java\" | sudo tee --append $BigDataSH /dev/null echo \"PATH=\\$PATH:\\$JAVA_HOME/bin\" | sudo tee --append $BigDataSH /dev/null echo \"# JAVA Variables END\" | sudo tee --append $BigDataSH /dev/null Confirm that your Java variables were added, open the /etc/profile.d/bigdata.sh file: cat $BigDataSH Your bigdata.sh file output should look like this now: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=/etc/ssh/ssh_config.d/SSH_keypair.pem export SSHConfigFile=/etc/ssh/ssh_config.d/config.conf # JAVA Variables START export JAVA_HOME=/usr/lib/jvm/default-java PATH=$PATH:$JAVA_HOME/bin # JAVA Variables END sudo reboot Confirm the Java Version and Environment Variables java -version result should be like: openjdk version \"11.0.11\" 2021-04-20 OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04) OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing) confirm you have it: echo $JAVA_HOME result should be like: /usr/lib/jvm/default-java Java Developers Kit (JDK) for DataNodes in a Cluster To send a command to a DateNode using SSH, we wrap the command with quotes \"\" and ssh into the Node: ssh DataNode001 \"{command}\" Update packages on the server ssh Datanode001 \"sudo apt-get -y update\" ssh Datanode001 \"sudo apt-get -y install default-jdk\" Notice: You will synchronize your DataNodes with the NameNode later, so you do not need to set up environment variables yet. pdsh pdsh is a variant of the rsh(1) command. Unlike rsh(1), which runs commands on a single remote host, pdsh can run multiple remote commands in parallel. pdsh uses a \"sliding window\" (or fanout) of threads to conserve resources on the initiating host while allowing some connections to time out. sudo apt-get -y install pdsh Add the following variable to your bigdata.sh file: export PDSH_RCMD_TYPE=ssh or run this commands: echo -e \"# PDSH Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export PDSH_RCMD_TYPE=ssh\" | sudo tee --append $BigDataSH /dev/null echo -e \"# PDSH Variables END\" | sudo tee --append $BigDataSH /dev/null sudo reboot rsync rsync is a utility for efficiently transferring and synchronizing files across computer systems by checking the timestamp and size of files. It is commonly found on Unix-like systems and functions as both a file synchronization and file transfer program. The rsync algorithm is a type of delta encoding and is used for minimizing network usage. Zlib may be used for additional compression, and SSH or stunnel can be used for data security. rsync(1) - Linux man page sudo apt-get -y install rsync sudo reboot ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:6:2","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#pdsh"},{"categories":null,"content":"Java Developers Kit (JDK) sudo apt-get -y update sudo apt-get -y install default-jdk confirm you have it: cd /usr/lib/jvm/ \u0026\u0026 ls result should be like: default-java java-1.11.0-openjdk-amd64 java-11-openjdk-amd64 openjdk-11 Add Environment Variables to /etc/profile.d/bigdata.sh Open the file /etc/profile.d/bigdata.sh sudo nano $BigDataSH Add these lines to the end of the bigdata.sh file: export JAVA_HOME=/usr/lib/jvm/default-java PATH=$PATH:$JAVA_HOME/bin Or, copy the following code and paste it at the command line. This will echo the entries into the bigdata.sh file echo \"# JAVA Variables START\" | sudo tee --append $BigDataSH /dev/null echo \"export JAVA_HOME=/usr/lib/jvm/default-java\" | sudo tee --append $BigDataSH /dev/null echo \"PATH=\\$PATH:\\$JAVA_HOME/bin\" | sudo tee --append $BigDataSH /dev/null echo \"# JAVA Variables END\" | sudo tee --append $BigDataSH /dev/null Confirm that your Java variables were added, open the /etc/profile.d/bigdata.sh file: cat $BigDataSH Your bigdata.sh file output should look like this now: #!/bin/bash # Environment Variables for Big Data tools export BigDataSH=/etc/profile.d/bigdata.sh export IdentityFile=/etc/ssh/ssh_config.d/SSH_keypair.pem export SSHConfigFile=/etc/ssh/ssh_config.d/config.conf # JAVA Variables START export JAVA_HOME=/usr/lib/jvm/default-java PATH=$PATH:$JAVA_HOME/bin # JAVA Variables END sudo reboot Confirm the Java Version and Environment Variables java -version result should be like: openjdk version \"11.0.11\" 2021-04-20 OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04) OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing) confirm you have it: echo $JAVA_HOME result should be like: /usr/lib/jvm/default-java Java Developers Kit (JDK) for DataNodes in a Cluster To send a command to a DateNode using SSH, we wrap the command with quotes \"\" and ssh into the Node: ssh DataNode001 \"{command}\" Update packages on the server ssh Datanode001 \"sudo apt-get -y update\" ssh Datanode001 \"sudo apt-get -y install default-jdk\" Notice: You will synchronize your DataNodes with the NameNode later, so you do not need to set up environment variables yet. pdsh pdsh is a variant of the rsh(1) command. Unlike rsh(1), which runs commands on a single remote host, pdsh can run multiple remote commands in parallel. pdsh uses a \"sliding window\" (or fanout) of threads to conserve resources on the initiating host while allowing some connections to time out. sudo apt-get -y install pdsh Add the following variable to your bigdata.sh file: export PDSH_RCMD_TYPE=ssh or run this commands: echo -e \"# PDSH Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export PDSH_RCMD_TYPE=ssh\" | sudo tee --append $BigDataSH /dev/null echo -e \"# PDSH Variables END\" | sudo tee --append $BigDataSH /dev/null sudo reboot rsync rsync is a utility for efficiently transferring and synchronizing files across computer systems by checking the timestamp and size of files. It is commonly found on Unix-like systems and functions as both a file synchronization and file transfer program. The rsync algorithm is a type of delta encoding and is used for minimizing network usage. Zlib may be used for additional compression, and SSH or stunnel can be used for data security. rsync(1) - Linux man page sudo apt-get -y install rsync sudo reboot ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:6:2","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#rsync"},{"categories":null,"content":"Install and Configure Hadoop as a Single Node Cluster ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#install-and-configure-hadoop-as-a-single-node-cluster"},{"categories":null,"content":"Download Hadoop from Apache wget http://apache.forsale.plus/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz -P ~/Downloads/Hadoop ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:1","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#download-hadoop-from-apache"},{"categories":null,"content":"Uncompress the Hadoop tar file into the /usr/local folder sudo tar -zxvf ~/Downloads/Hadoop/hadoop-*.tar.gz -C /usr/local ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:2","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#uncompress-the-hadoop-tar-file-into-the-usrlocal-folder"},{"categories":null,"content":"Rename the hadoop-* directory to /usr/local/hadoop sudo mv /usr/local/hadoop-* /usr/local/hadoop/ ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:3","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#rename-the-hadoop--directory-to-usrlocalhadoop"},{"categories":null,"content":"Modify permissions on /usr/local/hadoop/ Allow read+write on the /usr/local/hadoop/ directory for anyone in the hadoop user group. Double Check! Before you run this command, confirm that you have created a hadoop user group. See Hadoop User and Group Hadoop User named (hduser) and User Group named (hadoop) Add the hadoop User Group sudo addgroup hadoop sudo adduser hduser Add hduser to User Groups Run this command to add hduser to the hadoop user group: sudo usermod -a -G hadoop hduser Run this command to add hduser to the sudo (superuser) user group: sudo usermod -a -G sudo hduser We will also add the user to the hadoop user group. sudo usermod -a -G hadoop PceWlkr Now you can switch to hduser when you type this command: su - hduser Confirm which groups hduser is a member of: groups hduser The result should look something like this: hduser : hduser sudo hadoop sudo reboot su - hduser Back to Topic This command will set ownership of all files and directories in the /usr/local/hadoop/ directory: sudo chown PceWlkr:PceWlkr -R /usr/local/hadoop/ Set the permissions on the /usr/local/hadoop/ directory to rwxrwxr–: rwxrwxr– User and Group: Read + Write + Execute Other users: read sudo chmod -R 774 /usr/local/hadoop/ Set Environment Variables Use this command to edit the /etc/profile.d/bigdata.sh file: sudo nano $BigDataSH export HADOOP_HOME=\"/usr/local/hadoop\" export HADOOP_CONF_DIR=\"${HADOOP_HOME}/etc/hadoop\" export YARN_EXAMPLES=\"${HADOOP_HOME}/share/hadoop/mapreduce\" PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin or run these: echo -e \"# HADOOP Variables START\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export HADOOP_HOME=\\\"/usr/local/hadoop\\\"\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export HADOOP_CONF_DIR=\\\"\\${HADOOP_HOME}/etc/hadoop\\\"\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"export YARN_EXAMPLES=\\\"\\${HADOOP_HOME}/share/hadoop/mapreduce\\\"\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"PATH=\\$PATH:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbin\" | sudo tee --append $BigDataSH \u003e /dev/null echo -e \"# HADOOP Variables END\" | sudo tee --append $BigDataSH \u003e /dev/null Instantiate Environment Variables The following command will instantiate the new variables available immediately. You can use this method to instantiate variables in any of the modified shell scripts .sh files: source $BigDataSH test if it’s there: echo $HADOOP_HOME result should be: /usr/local/hadoop sudo reboot Test Environment Variables echo $HADOOP_HOME Test Hadoop Version hadoop version ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:4","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#modify-permissions-on-usrlocalhadoop"},{"categories":null,"content":"Modify permissions on /usr/local/hadoop/ Allow read+write on the /usr/local/hadoop/ directory for anyone in the hadoop user group. Double Check! Before you run this command, confirm that you have created a hadoop user group. See Hadoop User and Group Hadoop User named (hduser) and User Group named (hadoop) Add the hadoop User Group sudo addgroup hadoop sudo adduser hduser Add hduser to User Groups Run this command to add hduser to the hadoop user group: sudo usermod -a -G hadoop hduser Run this command to add hduser to the sudo (superuser) user group: sudo usermod -a -G sudo hduser We will also add the user to the hadoop user group. sudo usermod -a -G hadoop PceWlkr Now you can switch to hduser when you type this command: su - hduser Confirm which groups hduser is a member of: groups hduser The result should look something like this: hduser : hduser sudo hadoop sudo reboot su - hduser Back to Topic This command will set ownership of all files and directories in the /usr/local/hadoop/ directory: sudo chown PceWlkr:PceWlkr -R /usr/local/hadoop/ Set the permissions on the /usr/local/hadoop/ directory to rwxrwxr–: rwxrwxr– User and Group: Read + Write + Execute Other users: read sudo chmod -R 774 /usr/local/hadoop/ Set Environment Variables Use this command to edit the /etc/profile.d/bigdata.sh file: sudo nano $BigDataSH export HADOOP_HOME=\"/usr/local/hadoop\" export HADOOP_CONF_DIR=\"${HADOOP_HOME}/etc/hadoop\" export YARN_EXAMPLES=\"${HADOOP_HOME}/share/hadoop/mapreduce\" PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin or run these: echo -e \"# HADOOP Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_HOME=\\\"/usr/local/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_CONF_DIR=\\\"\\${HADOOP_HOME}/etc/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export YARN_EXAMPLES=\\\"\\${HADOOP_HOME}/share/hadoop/mapreduce\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"PATH=\\$PATH:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbin\" | sudo tee --append $BigDataSH /dev/null echo -e \"# HADOOP Variables END\" | sudo tee --append $BigDataSH /dev/null Instantiate Environment Variables The following command will instantiate the new variables available immediately. You can use this method to instantiate variables in any of the modified shell scripts .sh files: source $BigDataSH test if it’s there: echo $HADOOP_HOME result should be: /usr/local/hadoop sudo reboot Test Environment Variables echo $HADOOP_HOME Test Hadoop Version hadoop version ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:4","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#double-check"},{"categories":null,"content":"Modify permissions on /usr/local/hadoop/ Allow read+write on the /usr/local/hadoop/ directory for anyone in the hadoop user group. Double Check! Before you run this command, confirm that you have created a hadoop user group. See Hadoop User and Group Hadoop User named (hduser) and User Group named (hadoop) Add the hadoop User Group sudo addgroup hadoop sudo adduser hduser Add hduser to User Groups Run this command to add hduser to the hadoop user group: sudo usermod -a -G hadoop hduser Run this command to add hduser to the sudo (superuser) user group: sudo usermod -a -G sudo hduser We will also add the user to the hadoop user group. sudo usermod -a -G hadoop PceWlkr Now you can switch to hduser when you type this command: su - hduser Confirm which groups hduser is a member of: groups hduser The result should look something like this: hduser : hduser sudo hadoop sudo reboot su - hduser Back to Topic This command will set ownership of all files and directories in the /usr/local/hadoop/ directory: sudo chown PceWlkr:PceWlkr -R /usr/local/hadoop/ Set the permissions on the /usr/local/hadoop/ directory to rwxrwxr–: rwxrwxr– User and Group: Read + Write + Execute Other users: read sudo chmod -R 774 /usr/local/hadoop/ Set Environment Variables Use this command to edit the /etc/profile.d/bigdata.sh file: sudo nano $BigDataSH export HADOOP_HOME=\"/usr/local/hadoop\" export HADOOP_CONF_DIR=\"${HADOOP_HOME}/etc/hadoop\" export YARN_EXAMPLES=\"${HADOOP_HOME}/share/hadoop/mapreduce\" PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin or run these: echo -e \"# HADOOP Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_HOME=\\\"/usr/local/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_CONF_DIR=\\\"\\${HADOOP_HOME}/etc/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export YARN_EXAMPLES=\\\"\\${HADOOP_HOME}/share/hadoop/mapreduce\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"PATH=\\$PATH:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbin\" | sudo tee --append $BigDataSH /dev/null echo -e \"# HADOOP Variables END\" | sudo tee --append $BigDataSH /dev/null Instantiate Environment Variables The following command will instantiate the new variables available immediately. You can use this method to instantiate variables in any of the modified shell scripts .sh files: source $BigDataSH test if it’s there: echo $HADOOP_HOME result should be: /usr/local/hadoop sudo reboot Test Environment Variables echo $HADOOP_HOME Test Hadoop Version hadoop version ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:4","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#hadoop-user-named-hduser-and-user-group-named-hadoop"},{"categories":null,"content":"Modify permissions on /usr/local/hadoop/ Allow read+write on the /usr/local/hadoop/ directory for anyone in the hadoop user group. Double Check! Before you run this command, confirm that you have created a hadoop user group. See Hadoop User and Group Hadoop User named (hduser) and User Group named (hadoop) Add the hadoop User Group sudo addgroup hadoop sudo adduser hduser Add hduser to User Groups Run this command to add hduser to the hadoop user group: sudo usermod -a -G hadoop hduser Run this command to add hduser to the sudo (superuser) user group: sudo usermod -a -G sudo hduser We will also add the user to the hadoop user group. sudo usermod -a -G hadoop PceWlkr Now you can switch to hduser when you type this command: su - hduser Confirm which groups hduser is a member of: groups hduser The result should look something like this: hduser : hduser sudo hadoop sudo reboot su - hduser Back to Topic This command will set ownership of all files and directories in the /usr/local/hadoop/ directory: sudo chown PceWlkr:PceWlkr -R /usr/local/hadoop/ Set the permissions on the /usr/local/hadoop/ directory to rwxrwxr–: rwxrwxr– User and Group: Read + Write + Execute Other users: read sudo chmod -R 774 /usr/local/hadoop/ Set Environment Variables Use this command to edit the /etc/profile.d/bigdata.sh file: sudo nano $BigDataSH export HADOOP_HOME=\"/usr/local/hadoop\" export HADOOP_CONF_DIR=\"${HADOOP_HOME}/etc/hadoop\" export YARN_EXAMPLES=\"${HADOOP_HOME}/share/hadoop/mapreduce\" PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin or run these: echo -e \"# HADOOP Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_HOME=\\\"/usr/local/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_CONF_DIR=\\\"\\${HADOOP_HOME}/etc/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export YARN_EXAMPLES=\\\"\\${HADOOP_HOME}/share/hadoop/mapreduce\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"PATH=\\$PATH:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbin\" | sudo tee --append $BigDataSH /dev/null echo -e \"# HADOOP Variables END\" | sudo tee --append $BigDataSH /dev/null Instantiate Environment Variables The following command will instantiate the new variables available immediately. You can use this method to instantiate variables in any of the modified shell scripts .sh files: source $BigDataSH test if it’s there: echo $HADOOP_HOME result should be: /usr/local/hadoop sudo reboot Test Environment Variables echo $HADOOP_HOME Test Hadoop Version hadoop version ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:4","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#add-the-hadoop-user-group"},{"categories":null,"content":"Modify permissions on /usr/local/hadoop/ Allow read+write on the /usr/local/hadoop/ directory for anyone in the hadoop user group. Double Check! Before you run this command, confirm that you have created a hadoop user group. See Hadoop User and Group Hadoop User named (hduser) and User Group named (hadoop) Add the hadoop User Group sudo addgroup hadoop sudo adduser hduser Add hduser to User Groups Run this command to add hduser to the hadoop user group: sudo usermod -a -G hadoop hduser Run this command to add hduser to the sudo (superuser) user group: sudo usermod -a -G sudo hduser We will also add the user to the hadoop user group. sudo usermod -a -G hadoop PceWlkr Now you can switch to hduser when you type this command: su - hduser Confirm which groups hduser is a member of: groups hduser The result should look something like this: hduser : hduser sudo hadoop sudo reboot su - hduser Back to Topic This command will set ownership of all files and directories in the /usr/local/hadoop/ directory: sudo chown PceWlkr:PceWlkr -R /usr/local/hadoop/ Set the permissions on the /usr/local/hadoop/ directory to rwxrwxr–: rwxrwxr– User and Group: Read + Write + Execute Other users: read sudo chmod -R 774 /usr/local/hadoop/ Set Environment Variables Use this command to edit the /etc/profile.d/bigdata.sh file: sudo nano $BigDataSH export HADOOP_HOME=\"/usr/local/hadoop\" export HADOOP_CONF_DIR=\"${HADOOP_HOME}/etc/hadoop\" export YARN_EXAMPLES=\"${HADOOP_HOME}/share/hadoop/mapreduce\" PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin or run these: echo -e \"# HADOOP Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_HOME=\\\"/usr/local/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_CONF_DIR=\\\"\\${HADOOP_HOME}/etc/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export YARN_EXAMPLES=\\\"\\${HADOOP_HOME}/share/hadoop/mapreduce\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"PATH=\\$PATH:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbin\" | sudo tee --append $BigDataSH /dev/null echo -e \"# HADOOP Variables END\" | sudo tee --append $BigDataSH /dev/null Instantiate Environment Variables The following command will instantiate the new variables available immediately. You can use this method to instantiate variables in any of the modified shell scripts .sh files: source $BigDataSH test if it’s there: echo $HADOOP_HOME result should be: /usr/local/hadoop sudo reboot Test Environment Variables echo $HADOOP_HOME Test Hadoop Version hadoop version ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:4","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#add-hduser-to-user-groups"},{"categories":null,"content":"Modify permissions on /usr/local/hadoop/ Allow read+write on the /usr/local/hadoop/ directory for anyone in the hadoop user group. Double Check! Before you run this command, confirm that you have created a hadoop user group. See Hadoop User and Group Hadoop User named (hduser) and User Group named (hadoop) Add the hadoop User Group sudo addgroup hadoop sudo adduser hduser Add hduser to User Groups Run this command to add hduser to the hadoop user group: sudo usermod -a -G hadoop hduser Run this command to add hduser to the sudo (superuser) user group: sudo usermod -a -G sudo hduser We will also add the user to the hadoop user group. sudo usermod -a -G hadoop PceWlkr Now you can switch to hduser when you type this command: su - hduser Confirm which groups hduser is a member of: groups hduser The result should look something like this: hduser : hduser sudo hadoop sudo reboot su - hduser Back to Topic This command will set ownership of all files and directories in the /usr/local/hadoop/ directory: sudo chown PceWlkr:PceWlkr -R /usr/local/hadoop/ Set the permissions on the /usr/local/hadoop/ directory to rwxrwxr–: rwxrwxr– User and Group: Read + Write + Execute Other users: read sudo chmod -R 774 /usr/local/hadoop/ Set Environment Variables Use this command to edit the /etc/profile.d/bigdata.sh file: sudo nano $BigDataSH export HADOOP_HOME=\"/usr/local/hadoop\" export HADOOP_CONF_DIR=\"${HADOOP_HOME}/etc/hadoop\" export YARN_EXAMPLES=\"${HADOOP_HOME}/share/hadoop/mapreduce\" PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin or run these: echo -e \"# HADOOP Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_HOME=\\\"/usr/local/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_CONF_DIR=\\\"\\${HADOOP_HOME}/etc/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export YARN_EXAMPLES=\\\"\\${HADOOP_HOME}/share/hadoop/mapreduce\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"PATH=\\$PATH:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbin\" | sudo tee --append $BigDataSH /dev/null echo -e \"# HADOOP Variables END\" | sudo tee --append $BigDataSH /dev/null Instantiate Environment Variables The following command will instantiate the new variables available immediately. You can use this method to instantiate variables in any of the modified shell scripts .sh files: source $BigDataSH test if it’s there: echo $HADOOP_HOME result should be: /usr/local/hadoop sudo reboot Test Environment Variables echo $HADOOP_HOME Test Hadoop Version hadoop version ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:4","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#back-to-topic"},{"categories":null,"content":"Modify permissions on /usr/local/hadoop/ Allow read+write on the /usr/local/hadoop/ directory for anyone in the hadoop user group. Double Check! Before you run this command, confirm that you have created a hadoop user group. See Hadoop User and Group Hadoop User named (hduser) and User Group named (hadoop) Add the hadoop User Group sudo addgroup hadoop sudo adduser hduser Add hduser to User Groups Run this command to add hduser to the hadoop user group: sudo usermod -a -G hadoop hduser Run this command to add hduser to the sudo (superuser) user group: sudo usermod -a -G sudo hduser We will also add the user to the hadoop user group. sudo usermod -a -G hadoop PceWlkr Now you can switch to hduser when you type this command: su - hduser Confirm which groups hduser is a member of: groups hduser The result should look something like this: hduser : hduser sudo hadoop sudo reboot su - hduser Back to Topic This command will set ownership of all files and directories in the /usr/local/hadoop/ directory: sudo chown PceWlkr:PceWlkr -R /usr/local/hadoop/ Set the permissions on the /usr/local/hadoop/ directory to rwxrwxr–: rwxrwxr– User and Group: Read + Write + Execute Other users: read sudo chmod -R 774 /usr/local/hadoop/ Set Environment Variables Use this command to edit the /etc/profile.d/bigdata.sh file: sudo nano $BigDataSH export HADOOP_HOME=\"/usr/local/hadoop\" export HADOOP_CONF_DIR=\"${HADOOP_HOME}/etc/hadoop\" export YARN_EXAMPLES=\"${HADOOP_HOME}/share/hadoop/mapreduce\" PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin or run these: echo -e \"# HADOOP Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_HOME=\\\"/usr/local/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_CONF_DIR=\\\"\\${HADOOP_HOME}/etc/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export YARN_EXAMPLES=\\\"\\${HADOOP_HOME}/share/hadoop/mapreduce\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"PATH=\\$PATH:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbin\" | sudo tee --append $BigDataSH /dev/null echo -e \"# HADOOP Variables END\" | sudo tee --append $BigDataSH /dev/null Instantiate Environment Variables The following command will instantiate the new variables available immediately. You can use this method to instantiate variables in any of the modified shell scripts .sh files: source $BigDataSH test if it’s there: echo $HADOOP_HOME result should be: /usr/local/hadoop sudo reboot Test Environment Variables echo $HADOOP_HOME Test Hadoop Version hadoop version ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:4","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#set-environment-variables"},{"categories":null,"content":"Modify permissions on /usr/local/hadoop/ Allow read+write on the /usr/local/hadoop/ directory for anyone in the hadoop user group. Double Check! Before you run this command, confirm that you have created a hadoop user group. See Hadoop User and Group Hadoop User named (hduser) and User Group named (hadoop) Add the hadoop User Group sudo addgroup hadoop sudo adduser hduser Add hduser to User Groups Run this command to add hduser to the hadoop user group: sudo usermod -a -G hadoop hduser Run this command to add hduser to the sudo (superuser) user group: sudo usermod -a -G sudo hduser We will also add the user to the hadoop user group. sudo usermod -a -G hadoop PceWlkr Now you can switch to hduser when you type this command: su - hduser Confirm which groups hduser is a member of: groups hduser The result should look something like this: hduser : hduser sudo hadoop sudo reboot su - hduser Back to Topic This command will set ownership of all files and directories in the /usr/local/hadoop/ directory: sudo chown PceWlkr:PceWlkr -R /usr/local/hadoop/ Set the permissions on the /usr/local/hadoop/ directory to rwxrwxr–: rwxrwxr– User and Group: Read + Write + Execute Other users: read sudo chmod -R 774 /usr/local/hadoop/ Set Environment Variables Use this command to edit the /etc/profile.d/bigdata.sh file: sudo nano $BigDataSH export HADOOP_HOME=\"/usr/local/hadoop\" export HADOOP_CONF_DIR=\"${HADOOP_HOME}/etc/hadoop\" export YARN_EXAMPLES=\"${HADOOP_HOME}/share/hadoop/mapreduce\" PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin or run these: echo -e \"# HADOOP Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_HOME=\\\"/usr/local/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_CONF_DIR=\\\"\\${HADOOP_HOME}/etc/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export YARN_EXAMPLES=\\\"\\${HADOOP_HOME}/share/hadoop/mapreduce\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"PATH=\\$PATH:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbin\" | sudo tee --append $BigDataSH /dev/null echo -e \"# HADOOP Variables END\" | sudo tee --append $BigDataSH /dev/null Instantiate Environment Variables The following command will instantiate the new variables available immediately. You can use this method to instantiate variables in any of the modified shell scripts .sh files: source $BigDataSH test if it’s there: echo $HADOOP_HOME result should be: /usr/local/hadoop sudo reboot Test Environment Variables echo $HADOOP_HOME Test Hadoop Version hadoop version ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:4","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#instantiate-environment-variables"},{"categories":null,"content":"Modify permissions on /usr/local/hadoop/ Allow read+write on the /usr/local/hadoop/ directory for anyone in the hadoop user group. Double Check! Before you run this command, confirm that you have created a hadoop user group. See Hadoop User and Group Hadoop User named (hduser) and User Group named (hadoop) Add the hadoop User Group sudo addgroup hadoop sudo adduser hduser Add hduser to User Groups Run this command to add hduser to the hadoop user group: sudo usermod -a -G hadoop hduser Run this command to add hduser to the sudo (superuser) user group: sudo usermod -a -G sudo hduser We will also add the user to the hadoop user group. sudo usermod -a -G hadoop PceWlkr Now you can switch to hduser when you type this command: su - hduser Confirm which groups hduser is a member of: groups hduser The result should look something like this: hduser : hduser sudo hadoop sudo reboot su - hduser Back to Topic This command will set ownership of all files and directories in the /usr/local/hadoop/ directory: sudo chown PceWlkr:PceWlkr -R /usr/local/hadoop/ Set the permissions on the /usr/local/hadoop/ directory to rwxrwxr–: rwxrwxr– User and Group: Read + Write + Execute Other users: read sudo chmod -R 774 /usr/local/hadoop/ Set Environment Variables Use this command to edit the /etc/profile.d/bigdata.sh file: sudo nano $BigDataSH export HADOOP_HOME=\"/usr/local/hadoop\" export HADOOP_CONF_DIR=\"${HADOOP_HOME}/etc/hadoop\" export YARN_EXAMPLES=\"${HADOOP_HOME}/share/hadoop/mapreduce\" PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin or run these: echo -e \"# HADOOP Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_HOME=\\\"/usr/local/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_CONF_DIR=\\\"\\${HADOOP_HOME}/etc/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export YARN_EXAMPLES=\\\"\\${HADOOP_HOME}/share/hadoop/mapreduce\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"PATH=\\$PATH:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbin\" | sudo tee --append $BigDataSH /dev/null echo -e \"# HADOOP Variables END\" | sudo tee --append $BigDataSH /dev/null Instantiate Environment Variables The following command will instantiate the new variables available immediately. You can use this method to instantiate variables in any of the modified shell scripts .sh files: source $BigDataSH test if it’s there: echo $HADOOP_HOME result should be: /usr/local/hadoop sudo reboot Test Environment Variables echo $HADOOP_HOME Test Hadoop Version hadoop version ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:4","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#test-environment-variables"},{"categories":null,"content":"Modify permissions on /usr/local/hadoop/ Allow read+write on the /usr/local/hadoop/ directory for anyone in the hadoop user group. Double Check! Before you run this command, confirm that you have created a hadoop user group. See Hadoop User and Group Hadoop User named (hduser) and User Group named (hadoop) Add the hadoop User Group sudo addgroup hadoop sudo adduser hduser Add hduser to User Groups Run this command to add hduser to the hadoop user group: sudo usermod -a -G hadoop hduser Run this command to add hduser to the sudo (superuser) user group: sudo usermod -a -G sudo hduser We will also add the user to the hadoop user group. sudo usermod -a -G hadoop PceWlkr Now you can switch to hduser when you type this command: su - hduser Confirm which groups hduser is a member of: groups hduser The result should look something like this: hduser : hduser sudo hadoop sudo reboot su - hduser Back to Topic This command will set ownership of all files and directories in the /usr/local/hadoop/ directory: sudo chown PceWlkr:PceWlkr -R /usr/local/hadoop/ Set the permissions on the /usr/local/hadoop/ directory to rwxrwxr–: rwxrwxr– User and Group: Read + Write + Execute Other users: read sudo chmod -R 774 /usr/local/hadoop/ Set Environment Variables Use this command to edit the /etc/profile.d/bigdata.sh file: sudo nano $BigDataSH export HADOOP_HOME=\"/usr/local/hadoop\" export HADOOP_CONF_DIR=\"${HADOOP_HOME}/etc/hadoop\" export YARN_EXAMPLES=\"${HADOOP_HOME}/share/hadoop/mapreduce\" PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin or run these: echo -e \"# HADOOP Variables START\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_HOME=\\\"/usr/local/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export HADOOP_CONF_DIR=\\\"\\${HADOOP_HOME}/etc/hadoop\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"export YARN_EXAMPLES=\\\"\\${HADOOP_HOME}/share/hadoop/mapreduce\\\"\" | sudo tee --append $BigDataSH /dev/null echo -e \"PATH=\\$PATH:\\$HADOOP_HOME/bin:\\$HADOOP_HOME/sbin\" | sudo tee --append $BigDataSH /dev/null echo -e \"# HADOOP Variables END\" | sudo tee --append $BigDataSH /dev/null Instantiate Environment Variables The following command will instantiate the new variables available immediately. You can use this method to instantiate variables in any of the modified shell scripts .sh files: source $BigDataSH test if it’s there: echo $HADOOP_HOME result should be: /usr/local/hadoop sudo reboot Test Environment Variables echo $HADOOP_HOME Test Hadoop Version hadoop version ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:4","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#test-hadoop-version"},{"categories":null,"content":"Hadoop Configuration Files Add JAVA_HOME Variable to $HADOOP_CONF_DIR/hadoop-env.sh echo $JAVA_HOME sudo nano $HADOOP_CONF_DIR/hadoop-env.sh Locate the area in the hadoop-env.sh file that indicates the JAVA_HOME variable. For Hadoop 3.3.1, this is on line 54. The line should look something like this: # export JAVA_HOME= Change the line entry to look like this statement; this value should be the same as your JAVA_HOME environment variable: export JAVA_HOME=/usr/lib/jvm/default-java sed -i 's/# export JAVA_HOME=/export JAVA_HOME=\\/usr\\/lib\\/jvm\\/default-java # export JAVA_HOME=/g' $HADOOP_CONF_DIR/hadoop-env.sh cat $BigDataSH result should be: # The java implementation to use. By default, this environment # variable is REQUIRED on ALL platforms except OS X! export JAVA_HOME=/usr/lib/jvm/default-java Modify $HADOOP_CONF_DIR/core-site.xml sudo nano $HADOOP_CONF_DIR/core-site.xml Replace the contents of the core-site.xml file section with the following lines: \u003cconfiguration\u003e \u003c!--Custom Properties--\u003e \u003cproperty\u003e \u003cname\u003ethisnamenode\u003c/name\u003e \u003cvalue\u003elocalhost\u003c/value\u003e \u003cdescription\u003eThis used as a variable throughout the configuration files. localhost may be replaced with a DNS that points to the NameNode. \u003c/description\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003ehomefolder\u003c/name\u003e \u003cvalue\u003e/home/${user.name}\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003efs.defaultFS\u003c/name\u003e \u003cvalue\u003ehdfs://${thisnamenode}:9000\u003c/value\u003e \u003cdescription\u003elocalhost may be replaced with a DNS that points to the NameNode.\u003c/description\u003e \u003c/property\u003e \u003c!-- Do not enable permission check --\u003e \u003cproperty\u003e \u003cname\u003edfs.permissions.enabled\u003c/name\u003e \u003cvalue\u003efalse\u003c/value\u003e \u003cdescription\u003eIf \"true\", enable permission checking in HDFS. If \"false\", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories. \u003c/description\u003e \u003c/property\u003e \u003c!-- The current user is all set to root --\u003e \u003cproperty\u003e \u003cname\u003ehadoop.http.staticuser.user\u003c/name\u003e \u003c!-- For Virtual Machine Instances in the Cloud, use ubuntu --\u003e \u003c!-- \u003cvalue\u003ePcewlkr\u003c/value\u003e --\u003e \u003cvalue\u003ePcewlkr\u003c/value\u003e \u003cdescription\u003ePcewlkr is the default user for our NameNode. This property sets the WebUI user for file browsing. For Virtual Machine Instances in the Cloud, use Pcewlkr \u003c/description\u003e \u003c/property\u003e \u003c/configuration\u003e Modify $HADOOP_CONF_DIR/yarn-site.xml sudo nano $HADOOP_CONF_DIR/yarn-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003eyarn.nodemanager.aux-services\u003c/name\u003e \u003cvalue\u003emapreduce_shuffle\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003emapred.job.tracker\u003c/name\u003e \u003cvalue\u003e${thisnamenode}:9001\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003eyarn.nodemanager.env-whitelist\u003c/name\u003e \u003cvalue\u003eJAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME\u003c/value\u003e \u003c/property\u003e \u003c/configuration\u003e Modify $HADOOP_CONF_DIR/mapred-site.xml sudo nano $HADOOP_CONF_DIR/mapred-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003emapreduce.jobtracker.address\u003c/name\u003e \u003cvalue\u003elocal\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003emapreduce.framework.name\u003c/name\u003e \u003cvalue\u003eyarn\u003c/value\u003e \u003c/property\u003e \u003c/configuration\u003e Modify $HADOOP_CONF_DIR/hdfs-site.xml sudo nano $HADOOP_CONF_DIR/hdfs-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003edfs.replication\u003c/name\u003e \u003c!--\u003cvalue\u003e3\u003c/value\u003e--\u003e \u003cvalue\u003e1\u003c/value\u003e \u003cdescription\u003eDefault block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time. When migrating your cluster to a Fully Distributed Cluster, change this to 3. \u003c/description\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003edfs.permissions.enabled\u003c/name\u003e \u003cvalue\u003efalse\u003c/value\u003e \u003cdescription\u003eIf \"true\", enable permission checking in HDFS. If \"false\", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories. \u003c/description\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003edfs.namenode.name.dir\u003c/name\u003e \u003cvalue\u003efile:///","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:5","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#hadoop-configuration-files"},{"categories":null,"content":"Hadoop Configuration Files Add JAVA_HOME Variable to $HADOOP_CONF_DIR/hadoop-env.sh echo $JAVA_HOME sudo nano $HADOOP_CONF_DIR/hadoop-env.sh Locate the area in the hadoop-env.sh file that indicates the JAVA_HOME variable. For Hadoop 3.3.1, this is on line 54. The line should look something like this: # export JAVA_HOME= Change the line entry to look like this statement; this value should be the same as your JAVA_HOME environment variable: export JAVA_HOME=/usr/lib/jvm/default-java sed -i 's/# export JAVA_HOME=/export JAVA_HOME=\\/usr\\/lib\\/jvm\\/default-java # export JAVA_HOME=/g' $HADOOP_CONF_DIR/hadoop-env.sh cat $BigDataSH result should be: # The java implementation to use. By default, this environment # variable is REQUIRED on ALL platforms except OS X! export JAVA_HOME=/usr/lib/jvm/default-java Modify $HADOOP_CONF_DIR/core-site.xml sudo nano $HADOOP_CONF_DIR/core-site.xml Replace the contents of the core-site.xml file section with the following lines: thisnamenode localhost This used as a variable throughout the configuration files. localhost may be replaced with a DNS that points to the NameNode. homefolder /home/${user.name} fs.defaultFS hdfs://${thisnamenode}:9000 localhost may be replaced with a DNS that points to the NameNode. dfs.permissions.enabled false If \"true\", enable permission checking in HDFS. If \"false\", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories. hadoop.http.staticuser.user Pcewlkr -- Pcewlkr Pcewlkr is the default user for our NameNode. This property sets the WebUI user for file browsing. For Virtual Machine Instances in the Cloud, use Pcewlkr Modify $HADOOP_CONF_DIR/yarn-site.xml sudo nano $HADOOP_CONF_DIR/yarn-site.xml yarn.nodemanager.aux-services mapreduce_shuffle mapred.job.tracker ${thisnamenode}:9001 yarn.nodemanager.env-whitelist JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME Modify $HADOOP_CONF_DIR/mapred-site.xml sudo nano $HADOOP_CONF_DIR/mapred-site.xml mapreduce.jobtracker.address local mapreduce.framework.name yarn Modify $HADOOP_CONF_DIR/hdfs-site.xml sudo nano $HADOOP_CONF_DIR/hdfs-site.xml dfs.replication 3-- 1 Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time. When migrating your cluster to a Fully Distributed Cluster, change this to 3. dfs.permissions.enabled false If \"true\", enable permission checking in HDFS. If \"false\", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories. dfs.namenode.name.dir file:///","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:5","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#add-java_home-variable-to-hadoop_conf_dirhadoop-envsh"},{"categories":null,"content":"Hadoop Configuration Files Add JAVA_HOME Variable to $HADOOP_CONF_DIR/hadoop-env.sh echo $JAVA_HOME sudo nano $HADOOP_CONF_DIR/hadoop-env.sh Locate the area in the hadoop-env.sh file that indicates the JAVA_HOME variable. For Hadoop 3.3.1, this is on line 54. The line should look something like this: # export JAVA_HOME= Change the line entry to look like this statement; this value should be the same as your JAVA_HOME environment variable: export JAVA_HOME=/usr/lib/jvm/default-java sed -i 's/# export JAVA_HOME=/export JAVA_HOME=\\/usr\\/lib\\/jvm\\/default-java # export JAVA_HOME=/g' $HADOOP_CONF_DIR/hadoop-env.sh cat $BigDataSH result should be: # The java implementation to use. By default, this environment # variable is REQUIRED on ALL platforms except OS X! export JAVA_HOME=/usr/lib/jvm/default-java Modify $HADOOP_CONF_DIR/core-site.xml sudo nano $HADOOP_CONF_DIR/core-site.xml Replace the contents of the core-site.xml file section with the following lines: thisnamenode localhost This used as a variable throughout the configuration files. localhost may be replaced with a DNS that points to the NameNode. homefolder /home/${user.name} fs.defaultFS hdfs://${thisnamenode}:9000 localhost may be replaced with a DNS that points to the NameNode. dfs.permissions.enabled false If \"true\", enable permission checking in HDFS. If \"false\", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories. hadoop.http.staticuser.user Pcewlkr -- Pcewlkr Pcewlkr is the default user for our NameNode. This property sets the WebUI user for file browsing. For Virtual Machine Instances in the Cloud, use Pcewlkr Modify $HADOOP_CONF_DIR/yarn-site.xml sudo nano $HADOOP_CONF_DIR/yarn-site.xml yarn.nodemanager.aux-services mapreduce_shuffle mapred.job.tracker ${thisnamenode}:9001 yarn.nodemanager.env-whitelist JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME Modify $HADOOP_CONF_DIR/mapred-site.xml sudo nano $HADOOP_CONF_DIR/mapred-site.xml mapreduce.jobtracker.address local mapreduce.framework.name yarn Modify $HADOOP_CONF_DIR/hdfs-site.xml sudo nano $HADOOP_CONF_DIR/hdfs-site.xml dfs.replication 3-- 1 Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time. When migrating your cluster to a Fully Distributed Cluster, change this to 3. dfs.permissions.enabled false If \"true\", enable permission checking in HDFS. If \"false\", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories. dfs.namenode.name.dir file:///","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:5","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#modify-hadoop_conf_dircore-sitexml"},{"categories":null,"content":"Hadoop Configuration Files Add JAVA_HOME Variable to $HADOOP_CONF_DIR/hadoop-env.sh echo $JAVA_HOME sudo nano $HADOOP_CONF_DIR/hadoop-env.sh Locate the area in the hadoop-env.sh file that indicates the JAVA_HOME variable. For Hadoop 3.3.1, this is on line 54. The line should look something like this: # export JAVA_HOME= Change the line entry to look like this statement; this value should be the same as your JAVA_HOME environment variable: export JAVA_HOME=/usr/lib/jvm/default-java sed -i 's/# export JAVA_HOME=/export JAVA_HOME=\\/usr\\/lib\\/jvm\\/default-java # export JAVA_HOME=/g' $HADOOP_CONF_DIR/hadoop-env.sh cat $BigDataSH result should be: # The java implementation to use. By default, this environment # variable is REQUIRED on ALL platforms except OS X! export JAVA_HOME=/usr/lib/jvm/default-java Modify $HADOOP_CONF_DIR/core-site.xml sudo nano $HADOOP_CONF_DIR/core-site.xml Replace the contents of the core-site.xml file section with the following lines: thisnamenode localhost This used as a variable throughout the configuration files. localhost may be replaced with a DNS that points to the NameNode. homefolder /home/${user.name} fs.defaultFS hdfs://${thisnamenode}:9000 localhost may be replaced with a DNS that points to the NameNode. dfs.permissions.enabled false If \"true\", enable permission checking in HDFS. If \"false\", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories. hadoop.http.staticuser.user Pcewlkr -- Pcewlkr Pcewlkr is the default user for our NameNode. This property sets the WebUI user for file browsing. For Virtual Machine Instances in the Cloud, use Pcewlkr Modify $HADOOP_CONF_DIR/yarn-site.xml sudo nano $HADOOP_CONF_DIR/yarn-site.xml yarn.nodemanager.aux-services mapreduce_shuffle mapred.job.tracker ${thisnamenode}:9001 yarn.nodemanager.env-whitelist JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME Modify $HADOOP_CONF_DIR/mapred-site.xml sudo nano $HADOOP_CONF_DIR/mapred-site.xml mapreduce.jobtracker.address local mapreduce.framework.name yarn Modify $HADOOP_CONF_DIR/hdfs-site.xml sudo nano $HADOOP_CONF_DIR/hdfs-site.xml dfs.replication 3-- 1 Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time. When migrating your cluster to a Fully Distributed Cluster, change this to 3. dfs.permissions.enabled false If \"true\", enable permission checking in HDFS. If \"false\", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories. dfs.namenode.name.dir file:///","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:5","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#modify-hadoop_conf_diryarn-sitexml"},{"categories":null,"content":"Hadoop Configuration Files Add JAVA_HOME Variable to $HADOOP_CONF_DIR/hadoop-env.sh echo $JAVA_HOME sudo nano $HADOOP_CONF_DIR/hadoop-env.sh Locate the area in the hadoop-env.sh file that indicates the JAVA_HOME variable. For Hadoop 3.3.1, this is on line 54. The line should look something like this: # export JAVA_HOME= Change the line entry to look like this statement; this value should be the same as your JAVA_HOME environment variable: export JAVA_HOME=/usr/lib/jvm/default-java sed -i 's/# export JAVA_HOME=/export JAVA_HOME=\\/usr\\/lib\\/jvm\\/default-java # export JAVA_HOME=/g' $HADOOP_CONF_DIR/hadoop-env.sh cat $BigDataSH result should be: # The java implementation to use. By default, this environment # variable is REQUIRED on ALL platforms except OS X! export JAVA_HOME=/usr/lib/jvm/default-java Modify $HADOOP_CONF_DIR/core-site.xml sudo nano $HADOOP_CONF_DIR/core-site.xml Replace the contents of the core-site.xml file section with the following lines: thisnamenode localhost This used as a variable throughout the configuration files. localhost may be replaced with a DNS that points to the NameNode. homefolder /home/${user.name} fs.defaultFS hdfs://${thisnamenode}:9000 localhost may be replaced with a DNS that points to the NameNode. dfs.permissions.enabled false If \"true\", enable permission checking in HDFS. If \"false\", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories. hadoop.http.staticuser.user Pcewlkr -- Pcewlkr Pcewlkr is the default user for our NameNode. This property sets the WebUI user for file browsing. For Virtual Machine Instances in the Cloud, use Pcewlkr Modify $HADOOP_CONF_DIR/yarn-site.xml sudo nano $HADOOP_CONF_DIR/yarn-site.xml yarn.nodemanager.aux-services mapreduce_shuffle mapred.job.tracker ${thisnamenode}:9001 yarn.nodemanager.env-whitelist JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME Modify $HADOOP_CONF_DIR/mapred-site.xml sudo nano $HADOOP_CONF_DIR/mapred-site.xml mapreduce.jobtracker.address local mapreduce.framework.name yarn Modify $HADOOP_CONF_DIR/hdfs-site.xml sudo nano $HADOOP_CONF_DIR/hdfs-site.xml dfs.replication 3-- 1 Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time. When migrating your cluster to a Fully Distributed Cluster, change this to 3. dfs.permissions.enabled false If \"true\", enable permission checking in HDFS. If \"false\", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories. dfs.namenode.name.dir file:///","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:5","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#modify-hadoop_conf_dirmapred-sitexml"},{"categories":null,"content":"Hadoop Configuration Files Add JAVA_HOME Variable to $HADOOP_CONF_DIR/hadoop-env.sh echo $JAVA_HOME sudo nano $HADOOP_CONF_DIR/hadoop-env.sh Locate the area in the hadoop-env.sh file that indicates the JAVA_HOME variable. For Hadoop 3.3.1, this is on line 54. The line should look something like this: # export JAVA_HOME= Change the line entry to look like this statement; this value should be the same as your JAVA_HOME environment variable: export JAVA_HOME=/usr/lib/jvm/default-java sed -i 's/# export JAVA_HOME=/export JAVA_HOME=\\/usr\\/lib\\/jvm\\/default-java # export JAVA_HOME=/g' $HADOOP_CONF_DIR/hadoop-env.sh cat $BigDataSH result should be: # The java implementation to use. By default, this environment # variable is REQUIRED on ALL platforms except OS X! export JAVA_HOME=/usr/lib/jvm/default-java Modify $HADOOP_CONF_DIR/core-site.xml sudo nano $HADOOP_CONF_DIR/core-site.xml Replace the contents of the core-site.xml file section with the following lines: thisnamenode localhost This used as a variable throughout the configuration files. localhost may be replaced with a DNS that points to the NameNode. homefolder /home/${user.name} fs.defaultFS hdfs://${thisnamenode}:9000 localhost may be replaced with a DNS that points to the NameNode. dfs.permissions.enabled false If \"true\", enable permission checking in HDFS. If \"false\", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories. hadoop.http.staticuser.user Pcewlkr -- Pcewlkr Pcewlkr is the default user for our NameNode. This property sets the WebUI user for file browsing. For Virtual Machine Instances in the Cloud, use Pcewlkr Modify $HADOOP_CONF_DIR/yarn-site.xml sudo nano $HADOOP_CONF_DIR/yarn-site.xml yarn.nodemanager.aux-services mapreduce_shuffle mapred.job.tracker ${thisnamenode}:9001 yarn.nodemanager.env-whitelist JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME Modify $HADOOP_CONF_DIR/mapred-site.xml sudo nano $HADOOP_CONF_DIR/mapred-site.xml mapreduce.jobtracker.address local mapreduce.framework.name yarn Modify $HADOOP_CONF_DIR/hdfs-site.xml sudo nano $HADOOP_CONF_DIR/hdfs-site.xml dfs.replication 3-- 1 Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time. When migrating your cluster to a Fully Distributed Cluster, change this to 3. dfs.permissions.enabled false If \"true\", enable permission checking in HDFS. If \"false\", permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories. dfs.namenode.name.dir file:///","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:5","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#modify-hadoop_conf_dirhdfs-sitexml"},{"categories":null,"content":"Format the HDFS hdfs namenode -format ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:6","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#format-the-hdfs"},{"categories":null,"content":"Start up your Hadoop Cluster start-dfs.sh check the namenode http://DNSofNameNode:9870 start-yarn.sh mapred --daemon start historyserver check the history website http://DNSofNameNode:19888 check the resource manager website http://DNSofNameNode:8088 jps will show you the running java processes On windows, edit C:\\Windows\\System32\\drivers\\etc\\hosts, add xxx.xxx.xxx.xxx namenode.internal.cloudapp.net namenode xxx.xxx.xxx.xxx datanode001.internal.cloudapp.net datanode001 xxx.xxx.xxx.xxx datanode002.internal.cloudapp.net datanode002 So that Windows can find the interal website http://namenode:9870 or http://namenode.internal.cloudapp.net:9870 ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:7","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#start-up-your-hadoop-cluster"},{"categories":null,"content":"Test the HDFS hdfs dfsadmin -report hdfs dfs -mkdir -p /user/PceWlkr/hdfs/tests mkdir -p ~/Documents/HDFS/Tests touch ~/Documents/HDFS/Tests/test.txt hdfs dfs -put ~/Documents/HDFS/Tests/test.txt /user/PceWlkr/hdfs/tests hdfs dfs -ls /user/PceWlkr/hdfs/tests ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:7:8","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#test-the-hdfs"},{"categories":null,"content":"Fully Distributed Hadoop Cluster on a Cloud Provider These configuration files include: SSH config SSH_keypair.pem Environment Variables bigdata.sh Environment Setup Java JDK pdsh rsync Hadoop Configuration - the Hadoop entire directory is usually copied directly to the new DataNodes hadoop-env.sh hdfs.site.xml core-site.xml yarn-site.xml mapred-site.xml masters workers ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:8:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#fully-distributed-hadoop-cluster-on-a-cloud-provider"},{"categories":null,"content":"Configure .masters File sudo touch $HADOOP_CONF_DIR/masters Add the DNS of your Master Node: Namenode Datanode001 Datanode002 sudo chown ubuntu $HADOOP_CONF_DIR/masters sudo chmod 0644 $HADOOP_CONF_DIR/masters ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:8:1","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#configure-masters-file"},{"categories":null,"content":"Configure .workers File ‘sudo nano $HADOOP_CONF_DIR/workers’ Add the entries for the Data Nodes: Datanode001 Datanode002 sudo chown ubuntu $HADOOP_CONF_DIR/workers sudo chmod 0644 $HADOOP_CONF_DIR/workers ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:8:2","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#configure-workers-file"},{"categories":null,"content":"Modify configuration files for Fully Distributed Cluster sudo nano $HADOOP_CONF_DIR/core-site.xml Change the thisnamenode property value to NameNode in the core-site.xml file. \u003cproperty\u003e \u003cname\u003ethisnamenode\u003c/name\u003e \u003cvalue\u003eNameNode\u003c/value\u003e \u003cdescription\u003eNameNode is the hostname specified in the config file and etc/hosts file. It may be replaced with a DNS that points to your NameNode.\u003c/description\u003e \u003c/property\u003e sudo nano $HADOOP_CONF_DIR/hdfs-site.xml Edit the dfs.replication property in hdfs-site.xml. \u003cproperty\u003e \u003cname\u003edfs.replication\u003c/name\u003e \u003cvalue\u003e3\u003c/value\u003e \u003cdescription\u003eDefault block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time. \u003c/description\u003e \u003c/property\u003e ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:8:3","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#modify-configuration-files-for-fully-distributed-cluster"},{"categories":null,"content":"Add a DataNode to your Hadoop Cluster stop-dfs.sh scp -p $SSHConfigFile $IdentityFile PceWlkr@Datanode001:~/.ssh/ scp -p $SSHConfigFile $IdentityFile PceWlkr@Datanode002:~/.ssh/ ssh Datanode001 sudo apt -y remove unattended-upgrades \u0026\u0026\rsudo apt-get -y update \u0026\u0026\rsudo apt-get -y upgrade \u0026\u0026\rsudo apt-get -y install default-jdk pdsh rsync exit ssh Datanode002 sudo apt -y remove unattended-upgrades \u0026\u0026\rsudo apt-get -y update \u0026\u0026\rsudo apt-get -y upgrade \u0026\u0026\rsudo apt-get -y install default-jdk pdsh rsync exit If the hadoop directory does not exist on the DataNode, you must first create it. Do this for each DataNode in your Cluster. Create the hadoop directory: ssh PceWlkr@Datanode001 \"sudo mkdir -p ${HADOOP_HOME}/\" ssh PceWlkr@Datanode001 \"sudo chown -R PceWlkr:PceWlkr ${HADOOP_HOME}/\" ssh PceWlkr@Datanode001 \"sudo chmod -R 774 ${HADOOP_HOME}/\" ssh PceWlkr@Datanode002 \"sudo mkdir -p ${HADOOP_HOME}/\" ssh PceWlkr@Datanode002 \"sudo chown -R PceWlkr:PceWlkr ${HADOOP_HOME}/\" ssh PceWlkr@Datanode002 \"sudo chmod -R 774 ${HADOOP_HOME}/\" cat $BigDataSH | ssh PceWlkr@Datanode001 \"sudo tee ${BigDataSH}\" cat $BigDataSH | ssh PceWlkr@Datanode002 \"sudo tee ${BigDataSH}\" sudo rm $HADOOP_HOME/logs/*.* rsync -ravl $HADOOP_HOME PceWlkr@Datanode001:/usr/local/ rsync -ravl $HADOOP_HOME PceWlkr@Datanode002:/usr/local/ Then reboot the Datanode001 and Datanode002 ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:8:4","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#add-a-datanode-to-your-hadoop-cluster"},{"categories":null,"content":"Test the HDFS Again ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:8:5","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#test-the-hdfs-again"},{"categories":null,"content":"Test MapReduce hadoop jar $YARN_EXAMPLES/hadoop-mapreduce-examples-3.3.1.jar hdfs dfs -mkdir -p /user/PceWlkr/wordcount/input hdfs dfs -chmod -R 777 /user hdfs dfs -put $HADOOP_HOME/*.txt /user/PceWlkr/wordcount/input hadoop jar $YARN_EXAMPLES/hadoop-mapreduce-examples-3.3.1.jar wordcount /user/ubPceWlkrntu/wordcount/input /user/PceWlkr/wordcount/output hdfs dfs -cat /user/PceWlkr/wordcount/output/* ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:8:6","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#test-mapreduce"},{"categories":null,"content":"Env Vars The hadoop config directory HADOOP_CONF_DIR The directory of the progtram JAVA_HOME HADOOP_HOME HIVE_HOME PIG_HOME HADOOP_YARN_HOME HADOOP_MAPRED_HOME The path of the files under ~/.ssh/ BigDataSH=/etc/profile.d/bigdata.sh IdentityFile=~/.ssh/SSH_keypair.pem SSHConfigFile=~/.ssh/config The following are DNS and IP for all nodes NameNodeDNS=“Namenode” DataNode001DNS=“Datanode001” DataNode002DNS=“Datanode002” NameNodeIP=“10.0.0.4” DataNode001IP=“10.0.0.5” DataNode002IP=“10.0.0.6” ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:0:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#env-vars"},{"categories":null,"content":"Env Vars The hadoop config directory HADOOP_CONF_DIR The directory of the progtram JAVA_HOME HADOOP_HOME HIVE_HOME PIG_HOME HADOOP_YARN_HOME HADOOP_MAPRED_HOME The path of the files under ~/.ssh/ BigDataSH=/etc/profile.d/bigdata.sh IdentityFile=~/.ssh/SSH_keypair.pem SSHConfigFile=~/.ssh/config The following are DNS and IP for all nodes NameNodeDNS=“Namenode” DataNode001DNS=“Datanode001” DataNode002DNS=“Datanode002” NameNodeIP=“10.0.0.4” DataNode001IP=“10.0.0.5” DataNode002IP=“10.0.0.6” ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:0:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#the-hadoop-config-directory"},{"categories":null,"content":"Env Vars The hadoop config directory HADOOP_CONF_DIR The directory of the progtram JAVA_HOME HADOOP_HOME HIVE_HOME PIG_HOME HADOOP_YARN_HOME HADOOP_MAPRED_HOME The path of the files under ~/.ssh/ BigDataSH=/etc/profile.d/bigdata.sh IdentityFile=~/.ssh/SSH_keypair.pem SSHConfigFile=~/.ssh/config The following are DNS and IP for all nodes NameNodeDNS=“Namenode” DataNode001DNS=“Datanode001” DataNode002DNS=“Datanode002” NameNodeIP=“10.0.0.4” DataNode001IP=“10.0.0.5” DataNode002IP=“10.0.0.6” ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:0:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#the-directory-of-the-progtram"},{"categories":null,"content":"Env Vars The hadoop config directory HADOOP_CONF_DIR The directory of the progtram JAVA_HOME HADOOP_HOME HIVE_HOME PIG_HOME HADOOP_YARN_HOME HADOOP_MAPRED_HOME The path of the files under ~/.ssh/ BigDataSH=/etc/profile.d/bigdata.sh IdentityFile=~/.ssh/SSH_keypair.pem SSHConfigFile=~/.ssh/config The following are DNS and IP for all nodes NameNodeDNS=“Namenode” DataNode001DNS=“Datanode001” DataNode002DNS=“Datanode002” NameNodeIP=“10.0.0.4” DataNode001IP=“10.0.0.5” DataNode002IP=“10.0.0.6” ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:0:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#the-path-of-the-files-under-ssh"},{"categories":null,"content":"Env Vars The hadoop config directory HADOOP_CONF_DIR The directory of the progtram JAVA_HOME HADOOP_HOME HIVE_HOME PIG_HOME HADOOP_YARN_HOME HADOOP_MAPRED_HOME The path of the files under ~/.ssh/ BigDataSH=/etc/profile.d/bigdata.sh IdentityFile=~/.ssh/SSH_keypair.pem SSHConfigFile=~/.ssh/config The following are DNS and IP for all nodes NameNodeDNS=“Namenode” DataNode001DNS=“Datanode001” DataNode002DNS=“Datanode002” NameNodeIP=“10.0.0.4” DataNode001IP=“10.0.0.5” DataNode002IP=“10.0.0.6” ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:0:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#the-following-are-dns-and-ip-for-all-nodes"},{"categories":null,"content":"Configs SSH config –\u003e give a identity to anyone enter through ssh, and give all host a IP and DNS so that ssh can recognize them SSH_keypair.pem –\u003e The ssh key Environment Variables bigdata.sh –\u003e storing env vars for ssh config and files, java vars, pdsh vars, and other vars Hadoop Configuration - the Hadoop entire directory is usually copied directly to the new DataNodes hadoop-env.sh –\u003e some env vars hdfs.site.xml –\u003e config for HDFS core-site.xml –\u003e config for Hadoop and Namenode yarn-site.xml –\u003e config for yarn mapred-site.xml –\u003e config for MapReduce masters –\u003e specify the master nodes workers –\u003e specify the slave nodes ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:0:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#configs"},{"categories":null,"content":"Daemons HDFS Daemons: They are configured by masters and workers and hdfs.site.xml and core-site.xml Name Node Secondary Name Node Data Node YARN Daemons: They are configured by yarn-site.xml Resource Manager Node Manager MapReduce Daemon: They are configured by mapred-site.xml Job History Server ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:0:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#daemons"},{"categories":null,"content":"Cluster Tests ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:0:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#cluster-tests"},{"categories":null,"content":"HDFS Test Result ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:1:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#hdfs-test-result"},{"categories":null,"content":"MapReduce Test Result ","date":"2021-12-12","objectID":"/setup-hadoop-cluster-on-ms-azure/:2:0","series":null,"tags":["Hadoop","Note"],"title":"Hadoop Cluster Setup","uri":"/setup-hadoop-cluster-on-ms-azure/#mapreduce-test-result"},{"categories":null,"content":"JavaSE Environment variable: set JAVA_HOME to the addr of install java Then add %JAVA_HOME%/bin to path. We do so to let Tomcat to recognize the java dir by looking at the JAVA_HOME .java -\u003e javac.exe -\u003e .class -\u003e java.exe -\u003e result bytecode file .class has the name of the class name in .java javac HelloWorld.java then java HelloWorldClassName class HelloWorldClassName { public static void main(String[] args) { System.out.println(\"Hello, World!\"); } } Important You can declare multiple classes in a source code file, but you can only have 1 public class at most in it. if declare a public class, then the source code file name has to be the same name of the class! Depending on the number of class you have in a source file, the compile process may generate 1 or more bytecode files. ","date":"2021-12-12","objectID":"/javase/:0:0","series":null,"tags":["Java","Note"],"title":"Java Note","uri":"/javase/#javase"},{"categories":null,"content":"JavaSE Environment variable: set JAVA_HOME to the addr of install java Then add %JAVA_HOME%/bin to path. We do so to let Tomcat to recognize the java dir by looking at the JAVA_HOME .java - javac.exe - .class - java.exe - result bytecode file .class has the name of the class name in .java javac HelloWorld.java then java HelloWorldClassName class HelloWorldClassName { public static void main(String[] args) { System.out.println(\"Hello, World!\"); } } Important You can declare multiple classes in a source code file, but you can only have 1 public class at most in it. if declare a public class, then the source code file name has to be the same name of the class! Depending on the number of class you have in a source file, the compile process may generate 1 or more bytecode files. ","date":"2021-12-12","objectID":"/javase/:0:0","series":null,"tags":["Java","Note"],"title":"Java Note","uri":"/javase/#important"},{"categories":null,"content":"Comment // /* */ /** @author nameOfAuthor @version verOfSourceCode */ Notice this can be extracted by javadoc and produce a document. `javadoc -d docName -author -version HelloWorld.java` Find the index.html in the docName. Maybe need -encoding UTF-8 Also notice that if you want to illustrate the func(), you should write the /** xxx */ out of the func() not inside it. ### Java API Document ### Coding Style ### String Sting is not a primitive data type 而是引用数据类型 When print function has multiple data types doing concatination using +, one String in it will make everything a string and concatinate to one big String. ```java System.out.println('*' + '\\t'); System.out.println('*' + \"\\t\"); System.out.println(\"*\" + '\\t'); System.out.println(\"*\" + \"\\t\"); ","date":"2021-12-12","objectID":"/javase/:0:1","series":null,"tags":["Java","Note"],"title":"Java Note","uri":"/javase/#comment"},{"categories":null,"content":"Var byte b = 5; b = b - 2; //wrong! here 2 is int ","date":"2021-12-12","objectID":"/javase/:0:2","series":null,"tags":["Java","Note"],"title":"Java Note","uri":"/javase/#var"},{"categories":null,"content":"Base 0b or 0B 0-7 0xA or 0Xa int num1 = 0b1001; int num2 = 0123; int num3 = 0x110A; System.out.println(num1); System.out.println(num2); System.out.println(num3); // all in base of 10 after printing out ","date":"2021-12-12","objectID":"/javase/:0:3","series":null,"tags":["Java","Note"],"title":"Java Note","uri":"/javase/#base"},{"categories":null,"content":"Operator int res = m % n res has the same sign of m! -12 % 5 == -12 % -5 ","date":"2021-12-12","objectID":"/javase/:0:4","series":null,"tags":["Java","Note"],"title":"Java Note","uri":"/javase/#operator"},{"categories":null,"content":"Scanner import java.util.Scanner; class ScannerTest{ public static void main(String[] args){ Scanner scan = new Scanner(System.in); String str = scan.nextLine(); int i = scan.nextInt(); } } ","date":"2021-12-12","objectID":"/javase/:0:5","series":null,"tags":["Java","Note"],"title":"Java Note","uri":"/javase/#scanner"},{"categories":null,"content":"Control Flow with Label label: for (int i = 1; i \u003c 4; i++){ for (int j = 1; j \u003c= 10; j++){ if(j % 4 === 0) continue label; } System.out.println(j); } System.out.println(); ","date":"2021-12-12","objectID":"/javase/:0:6","series":null,"tags":["Java","Note"],"title":"Java Note","uri":"/javase/#control-flow-with-label"},{"categories":null,"content":"Variable Number of args public static void test(String … strs); The number of String passed in can be any amount including no string at all. 看到226 ","date":"2021-12-12","objectID":"/javase/:0:7","series":null,"tags":["Java","Note"],"title":"Java Note","uri":"/javase/#variable-number-of-args"},{"categories":null,"content":"JS ","date":"2021-12-12","objectID":"/javascript/:0:0","series":null,"tags":["JavaScript","Note"],"title":"JS Note","uri":"/javascript/#js"},{"categories":null,"content":"Let/Const function sayHello() { for (var i = 0; i \u003c 5; i++) { console.log(i); } console.log(i); } sayHello(); problem with var: the scope! To solve this problem, we have let. Scope var -\u003e function let -\u003e block const -\u003e block const person = { name: 'Mosh', walk: function() {}, talk() {} //neat way }; person.talk(); person['name'] = 'John'; const targetMember = 'name'; // think about this can be user input person[targetMember] = 'John'; // dynamically manage the access ","date":"2021-12-12","objectID":"/javascript/:1:0","series":null,"tags":["JavaScript","Note"],"title":"JS Note","uri":"/javascript/#letconst"},{"categories":null,"content":"Let/Const function sayHello() { for (var i = 0; i function let - block const - block const person = { name: 'Mosh', walk: function() {}, talk() {} //neat way }; person.talk(); person['name'] = 'John'; const targetMember = 'name'; // think about this can be user input person[targetMember] = 'John'; // dynamically manage the access ","date":"2021-12-12","objectID":"/javascript/:1:0","series":null,"tags":["JavaScript","Note"],"title":"JS Note","uri":"/javascript/#scope"},{"categories":null,"content":"Objects ","date":"2021-12-12","objectID":"/javascript/:2:0","series":null,"tags":["JavaScript","Note"],"title":"JS Note","uri":"/javascript/#objects"},{"categories":null,"content":"this const person = { name: \"Mosh\", walk() { console.log(this); } }; person.walk(); // {name: \"Mosh\", walk: f} const walk = person.walk; walk(); // undefined since the browser runs it in the strict mode What is ‘this’? ‘this’ always return the current object. The value of ‘this’ is determined by how the function is called. If we call a function as a method of a object, ‘this’ returns the ref of the obj. However, if we call a function as a standalone object or outside of an obj, ‘this’ will return the global obj – the browser. const walk = person.walk.bind(person); // now the walk will always be binded to person obj for the new function walk created permanently. walk(); ","date":"2021-12-12","objectID":"/javascript/:3:0","series":null,"tags":["JavaScript","Note"],"title":"JS Note","uri":"/javascript/#this"},{"categories":null,"content":"Building this How to fix the prob: no matter how we call the function, ‘this’ always returns the person obj. notice that in JS, functions are objs So, person.walk is actually an obj. person.walk.bind(person); will bind the it to the person obj, and ‘this’ will return the person obj. ","date":"2021-12-12","objectID":"/javascript/:3:1","series":null,"tags":["JavaScript","Note"],"title":"JS Note","uri":"/javascript/#building-this"},{"categories":null,"content":"Arrow Funcs const square = function(number) { return number * number; } const square = number =\u003e { return number * number; } const square = number =\u003e number * number; console.log(square(5)); const jobs = { {id: 1, isActive: true}, {id: 2, isActive: true}, {id: 3, isActive: false} }; const activeJobs = jobs.filter(function(job){ return job.isActive;}); const activeJobs = jobs.filter(job =\u003e job.isActive); ","date":"2021-12-12","objectID":"/javascript/:4:0","series":null,"tags":["JavaScript","Note"],"title":"JS Note","uri":"/javascript/#arrow-funcs"},{"categories":null,"content":"Arow Funcs and this ","date":"2021-12-12","objectID":"/javascript/:4:1","series":null,"tags":["JavaScript","Note"],"title":"JS Note","uri":"/javascript/#arow-funcs-and-this"},{"categories":null,"content":"Destructuring ","date":"2021-12-12","objectID":"/javascript/:5:0","series":null,"tags":["JavaScript","Note"],"title":"JS Note","uri":"/javascript/#destructuring"},{"categories":null,"content":"Spread ","date":"2021-12-12","objectID":"/javascript/:6:0","series":null,"tags":["JavaScript","Note"],"title":"JS Note","uri":"/javascript/#spread"},{"categories":null,"content":"Classes ","date":"2021-12-12","objectID":"/javascript/:7:0","series":null,"tags":["JavaScript","Note"],"title":"JS Note","uri":"/javascript/#classes"},{"categories":null,"content":"Modules ","date":"2021-12-12","objectID":"/javascript/:8:0","series":null,"tags":["JavaScript","Note"],"title":"JS Note","uri":"/javascript/#modules"},{"categories":null,"content":"von Neumann Architecture (realization of Turing Machine) ","date":"2021-12-12","objectID":"/os_review/:0:0","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#"},{"categories":null,"content":"What are programs? Process – a running program To allow for running multiple processes, a running program must be able to be interrupted OS provides a virtualization of CPUs, so each program can assume they have their “own” virtual CPU at its disposal ","date":"2021-12-12","objectID":"/os_review/:0:1","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#what-are-programs"},{"categories":null,"content":"Memory Space of a Program A variable of a program corresponds to a location in memory Code address, e.g., function entrance addresses, corresponds to a location in memory ","date":"2021-12-12","objectID":"/os_review/:0:2","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#memory-space-of-a-program"},{"categories":null,"content":"Concurrency Two scenarios of concurrency Interleaving Parallel execution Concurrency when happening on shared data May lead to undefined result under thread semantics Have to apply synchronization to ensure consistent semantics ","date":"2021-12-12","objectID":"/os_review/:0:3","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#concurrency"},{"categories":null,"content":"Three Easy Pieces Virtualization Concurrency Persistence ","date":"2021-12-12","objectID":"/os_review/:0:4","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#three-easy-pieces"},{"categories":null,"content":"How CPU virtualization can be realized One Straightforward Approach OS emulates the real CPU Each instruction is interpreted by the OS, as opposed to running on the bare metal. OS has full control of a process, and can interrupt it at will. OS can refuse to run an instruction that directly accesses I/O. What is the drawback? Very slow. Does not fully utilize the hardware’s capability Direct Execution Let processes’ code run directly on CPU How to preempt a running process? Timer interrupt. Need to be frequent enough, e.g., every a few ms. Interrupt handler is run upon an interrupt. Interrupt is widely used to handle device I/O. Exception can also interrupt a running process. How to protect OS from the processes? What about kernel code in memory? How to protect processes from one another? Kernel needs to do A LOT OF book keeping. Let’s introduce a few important concepts about processes Let’s look at some important UNIX process APIs ","date":"2021-12-12","objectID":"/os_review/:0:5","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#how-cpu-virtualization-can-be-realized"},{"categories":null,"content":"What are the main problems to be solved? How to preempt a process How to protect shared resources, i.e., I/O –I/O shall not be directly accessed by processes. All access to I/O must be mediated by OS. The CPU must be able to distinguish two different modes: in one mode, the instructions running are those of the processes; in the other mode, the instructions running are those of the OS. We call the prior mode user mode, and the second mode kernel mode. Notion of OS Kernel: An OS kernel consists of code that runs on behalf of the OS N.B., this does not include OS library code In user mode, certain instructions are not permitted. Examples? If a process attempts to execute one such instruction, the CPU will raise an exception and the process will be interrupted In kernel mode, all instructions are allowed. When the CPU starts up, it is normally in kernel mode. Transition between the two modes. from kernel to user – simpler, since OS is in control from user to kernel – care must be taken to ensure protection How to protect OS from the processes? – wait until Virtual Memory How to protect processes from one another. – wait until Virtual Memory Performance \u0026 Efficiency Other considerations Parallel execution ","date":"2021-12-12","objectID":"/os_review/:0:6","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#what-are-the-main-problems-to-be-solved"},{"categories":null,"content":"Context Switches Always between user mode and kernel mode Can a process directly switch to another process without going through kernel? to do so the first process would need to access the other process’s context information, and change the process’s status to running it will also need to switch to the other process’s virtual memory. It will be very hard to grant these privileges, without giving out too much. Reasons for context switches/Cases of Trapping into Kernel Hardware interrupt Timer Device I/O etc. Exception Illegal instruction Non-permitted instruction System calls Requesting kernel to preform privileged operations on the process’s behalf ","date":"2021-12-12","objectID":"/os_review/:0:7","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#context-switches"},{"categories":null,"content":"Kernel vs. User Mode Kernel code runs at full privilege (a.k.a. privileged execution) All instructions are available All physical memory is available Direct access to hardware I/O is available Process code runs at restricted privilege Certain instructions are not permitted – we will encounter quite a few Direct access to physical memory is not possible – more to be discussed later Direct access to hardware I/O is prohibited Don’t take it for granted! ","date":"2021-12-12","objectID":"/os_review/:0:8","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#kernel-vs-user-mode"},{"categories":null,"content":"How context switches can be implemented How to trap into kernel hardware interrupt e.g., timer interrupt… Save the context of the process interrupted CPU saves PC value at the interrupt CPU saves SR at the interrupt CPU/OS saves GRs at the interrupt Transfer control to kernel CPU saves the current SP, find out the current process’s kernel stack through a special register, and changes SP to point to that kernel stack CPU finds out the address of the IDT through a special register IDTR CPU indexes into IDT, finds out the entrance address of the interrupt handler, and populates PC with the address CPU changes from user mode to kernel mode Update book-keeping OS changes the interrupted process from ”running” to “ready” state OS populates the relevant field in PCB regarding the interrupted process’s context info How to return from a trap return from a special return instruction How to switch to a different process while returning from a trap basic notions: PC (program counter): CPU register pointing to the next instruction to fetch SP (stack pointer): CPU register pointing to the current top of stack GR (general registers): CPU general registers for storing temporary results SR (status register): Current CPU status, e.g., arithmetic operation results A device notifies a CPU for an interrupting event through hard-wired signal The CPU can tell which device has sent an interrupt Interrupts from different devices are assigned different numbers, i.e., interrupt no. CPU checks whether there is interrupt waiting, before fetching the next instruction ","date":"2021-12-12","objectID":"/os_review/:0:9","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#how-context-switches-can-be-implemented"},{"categories":null,"content":"Case Study: x86 Interrupt Mechanism Basic PC (program counter): EIP SP (stack pointer): ESP GR (general registers): EAX, EBX, … SR (status register): EFLAGS x86 adopts segmentation in addressing, so each address also needs to have a segment For code, need to use the CS segment register Thus program counter is CS:EIP For stack, need to use the SS segment register Thus stack pointer is SS:ESP ","date":"2021-12-12","objectID":"/os_review/:0:10","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#case-study-x86-interrupt-mechanism"},{"categories":null,"content":"Resume execution of the interrupted process Revert the operating at the beginning of the interrupt Restore the GRs to the value at interrupt Restore SP/SR/PC to the value at interrupt Change CPU mode from kernel to user Part of this is done through a “trap return” instruction The semantics of the trap return instruction is the contract between the hardware and OS The kernel can decide how to fulfill the rest ","date":"2021-12-12","objectID":"/os_review/:0:11","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#resume-execution-of-the-interrupted-process"},{"categories":null,"content":"Resume execution of a different process Similar to before, but using another process’s context Also need to change CPU register pointing to the new process’s kernel stack OS needs to maintain each process’s context consistently, and be able to find the context when needed. ","date":"2021-12-12","objectID":"/os_review/:0:12","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#resume-execution-of-a-different-process"},{"categories":null,"content":"Maintaining each process’s context It is extremely important that kernel maintains each process’s context consistently The biggest part of an OS kernel’s job is book-keeping! Typically the kernel maintains a process control block (PCB) data structure for each active process. The process’s current status The process’s context to resume running The process’s kernel stack (interrupt stack) location ","date":"2021-12-12","objectID":"/os_review/:0:13","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#maintaining-each-processs-context"},{"categories":null,"content":"Other ways of trapping into kernel Exception Similar to interrupt, except that when exception happens, the PC points to the offending instruction, as opposed to the next instruction The hardware may inform CPU the specific nature of the exception through an error number. System call User process explicitly traps into kernel, by invoking a specific interrupt number through “software interrupt” In x86: the “int N” instruction System call number is passed through a register, in x86 it is %EAX All parameters of a system call must be copied into kernel and checked ","date":"2021-12-12","objectID":"/os_review/:0:14","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#other-ways-of-trapping-into-kernel"},{"categories":null,"content":"How system call can be implemented ","date":"2021-12-12","objectID":"/os_review/:0:15","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#how-system-call-can-be-implemented"},{"categories":null,"content":"Important UNIX process APIs Unix History Developed by Dennis Ritchie and Ken Thompson at AT\u0026T Bell Labs Adapted some ideas from the Multics project in 1969 Use man command to read manuals of commands and system calls “man -a topic”: returns all entries on topic Type “q” to exit an entry and move on to the next one UNIX Process APIs fork() Memory space, open files, environmental variables wait() exec() Redirecting input/output An open file is assigned a numeric number as its “file descriptor” 0: standard input 1: standard output 2: standard error UNIX pipes: pipe() returns a unidirectional message queue between processes for inter-process communication Returns two file descriptors, d0 and d1. Bytes written on d1 will be read from d0 in the same order. Use the dup() system call to redirect standard input and output ","date":"2021-12-12","objectID":"/os_review/:0:16","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#important-unix-process-apis"},{"categories":null,"content":"CPU Scheduling ","date":"2021-12-12","objectID":"/os_review/:1:0","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#cpu-scheduling"},{"categories":null,"content":"Policy vs. Mechanisms We have introduced the mechanism of context switches Before returning from kernel space to user space, OS needs to decide which user level process to resume The determination of the choice of which process to resume is the “Policy” ","date":"2021-12-12","objectID":"/os_review/:1:1","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#policy-vs-mechanisms"},{"categories":null,"content":"Desired Properties of the Scheduling Policy What do you think are good outcomes (so we try to achieve them) Fair: Each process occupies about the same amount of CPU time This needs to be modulated through priority I/O intensive processes shall have higher priority How to achieve fairness when every process has the same priority? Two different notions of time: elapsed time vs. virtual time OS can keep track of virtual time of all processes Scheduler always picks the one with the lowest virtual time to run What do you think are bad outcomes (so we try to avoid them) What if a new process joins later? – needs to be adjusted Approach 1: initialize a new process’s virtual time to that of the minimum of the currently ready processes What if a process was blocked in I/O, and is now ready to run. It will have a lower virtual time compared with the other ready processes. Adversarial processes can potentially game the system to deprive other processes opportunity to make progress ","date":"2021-12-12","objectID":"/os_review/:1:2","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#desired-properties-of-the-scheduling-policy"},{"categories":null,"content":"Goals of the Scheduling Policy Metrics on which the algorithm tries to optimize Turnaround time Average interval length from task arrival till completion Response time (fairness) Average interval length from task arrival till “response” For simplicity, we interpret “response” as the first time a task is scheduled The two metrics are trade-offs Optimizing on one will jeopardize the other ","date":"2021-12-12","objectID":"/os_review/:1:3","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#goals-of-the-scheduling-policy"},{"categories":null,"content":"The FIFO policy ","date":"2021-12-12","objectID":"/os_review/:1:4","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#the-fifo-policy"},{"categories":null,"content":"Shortest Time-to-Completion First (STCF) ","date":"2021-12-12","objectID":"/os_review/:1:5","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#shortest-time-to-completion-first-stcf"},{"categories":null,"content":"Shortest Job First (SJF) –Historical Coverage ","date":"2021-12-12","objectID":"/os_review/:1:6","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#shortest-job-first-sjf-historical-coverage"},{"categories":null,"content":"Round Robin ","date":"2021-12-12","objectID":"/os_review/:1:7","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#round-robin"},{"categories":null,"content":"When a Process is Blocked by I/O It cannot be scheduled until it is woken up What shall the scheduling policy be? If a process is more interactive with users, it will more often be blocked by I/O. ","date":"2021-12-12","objectID":"/os_review/:1:8","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#when-a-process-is-blocked-by-io"},{"categories":null,"content":"Two Well Known Scheduling Algorithms ","date":"2021-12-12","objectID":"/os_review/:2:0","series":null,"tags":["OS","Note"],"title":"OS Review Note","uri":"/os_review/#two-well-known-scheduling-algorithms"},{"categories":null,"content":"persistence Closer to the CPU, faster CPU–Mem–Graphics– ","date":"2021-12-12","objectID":"/persistence/:0:0","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#persistence"},{"categories":null,"content":"canonical protocol for programmed I/O Interface: statue: busy or not command data internal: CPU MEM other chips ","date":"2021-12-12","objectID":"/persistence/:0:1","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#canonical-protocol-for-programmed-io"},{"categories":null,"content":"Q should the program running in kernel or user mode? in kernel. you don’t want the user level program to touch the devices directly, since devices are shared bewteen processes. Device doesn’t not have any protection from other processes. It can change the data on behalf of anyone. ","date":"2021-12-12","objectID":"/persistence/:0:2","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#q"},{"categories":null,"content":"key features of programmed IO CPU is involved every step of the way A process invkes a system call, which results in device access Under PIO, the process is spinning on the CPU waiting for the IO to complete cons: kernel involved everywhere since devices are so slow compareing with cpu, cpu has to do a lot of unuseful spinning to do status check. just like what we do for the spinning lock. pros: simple and straight forward cpu will keep checking so no time is wasted once the devices are available, so the responsive time is good. ","date":"2021-12-12","objectID":"/persistence/:0:3","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#key-features-of-programmed-io"},{"categories":null,"content":"Scheduling Picture ","date":"2021-12-12","objectID":"/persistence/:0:4","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#scheduling-picture"},{"categories":null,"content":"Other approach: interrupt you don’t need to keep pooling, the device will raise the exception to interrupt, and the handler will deal with it. pro: no wasting of cpu source con: if there are a lot of i/o devices, and they wake up one by one? the interrupts will happen consecutively. So interrupt does have overhead. canonical protocol for programmed io very low latencty, it’s watching. the moment it’s ready, it leaves interrupt need to go wait and switch back. overhead always there. Latency for some device it’s ok, but not for everyone. use programmed io if fast response is needed Q: if a process si running in the interrupt mode in a system call, and it goes to sleep. which mode is the process in? (running ready blocked?) it’s blocked. instead in programmed io mode, the process is running. ","date":"2021-12-12","objectID":"/persistence/:0:5","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#other-approach-interrupt"},{"categories":null,"content":"Other approach: interrupt you don’t need to keep pooling, the device will raise the exception to interrupt, and the handler will deal with it. pro: no wasting of cpu source con: if there are a lot of i/o devices, and they wake up one by one? the interrupts will happen consecutively. So interrupt does have overhead. canonical protocol for programmed io very low latencty, it’s watching. the moment it’s ready, it leaves interrupt need to go wait and switch back. overhead always there. Latency for some device it’s ok, but not for everyone. use programmed io if fast response is needed Q: if a process si running in the interrupt mode in a system call, and it goes to sleep. which mode is the process in? (running ready blocked?) it’s blocked. instead in programmed io mode, the process is running. ","date":"2021-12-12","objectID":"/persistence/:0:5","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#canonical-protocol-for-programmed-io-1"},{"categories":null,"content":"Other approach: interrupt you don’t need to keep pooling, the device will raise the exception to interrupt, and the handler will deal with it. pro: no wasting of cpu source con: if there are a lot of i/o devices, and they wake up one by one? the interrupts will happen consecutively. So interrupt does have overhead. canonical protocol for programmed io very low latencty, it’s watching. the moment it’s ready, it leaves interrupt need to go wait and switch back. overhead always there. Latency for some device it’s ok, but not for everyone. use programmed io if fast response is needed Q: if a process si running in the interrupt mode in a system call, and it goes to sleep. which mode is the process in? (running ready blocked?) it’s blocked. instead in programmed io mode, the process is running. ","date":"2021-12-12","objectID":"/persistence/:0:5","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#interrupt"},{"categories":null,"content":"Other approach: interrupt you don’t need to keep pooling, the device will raise the exception to interrupt, and the handler will deal with it. pro: no wasting of cpu source con: if there are a lot of i/o devices, and they wake up one by one? the interrupts will happen consecutively. So interrupt does have overhead. canonical protocol for programmed io very low latencty, it’s watching. the moment it’s ready, it leaves interrupt need to go wait and switch back. overhead always there. Latency for some device it’s ok, but not for everyone. use programmed io if fast response is needed Q: if a process si running in the interrupt mode in a system call, and it goes to sleep. which mode is the process in? (running ready blocked?) it’s blocked. instead in programmed io mode, the process is running. ","date":"2021-12-12","objectID":"/persistence/:0:5","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#use-programmed-io-if-fast-response-is-needed"},{"categories":null,"content":"Further improvement: DMA even with interrupt, cpu is still involved in all data transfers Direct Memory Access allows devices to transfer data to/from memory without CPU involvement. CPU sets up DMA, and context switch to run other process when mem transfer finishes, interrupt raised Kernel interrupt handler runs and wakes up the sleeping process DMA uses kernel page table there are some space reserved for dma so that kernel know where they are, just like how driver files are set. the kernel page table can have physical mem 1-1 matched scheduling pic ","date":"2021-12-12","objectID":"/persistence/:0:6","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#further-improvement-dma"},{"categories":null,"content":"Further improvement: DMA even with interrupt, cpu is still involved in all data transfers Direct Memory Access allows devices to transfer data to/from memory without CPU involvement. CPU sets up DMA, and context switch to run other process when mem transfer finishes, interrupt raised Kernel interrupt handler runs and wakes up the sleeping process DMA uses kernel page table there are some space reserved for dma so that kernel know where they are, just like how driver files are set. the kernel page table can have physical mem 1-1 matched scheduling pic ","date":"2021-12-12","objectID":"/persistence/:0:6","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#dma-uses-kernel-page-table"},{"categories":null,"content":"Further improvement: DMA even with interrupt, cpu is still involved in all data transfers Direct Memory Access allows devices to transfer data to/from memory without CPU involvement. CPU sets up DMA, and context switch to run other process when mem transfer finishes, interrupt raised Kernel interrupt handler runs and wakes up the sleeping process DMA uses kernel page table there are some space reserved for dma so that kernel know where they are, just like how driver files are set. the kernel page table can have physical mem 1-1 matched scheduling pic ","date":"2021-12-12","objectID":"/persistence/:0:6","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#scheduling-pic"},{"categories":null,"content":"Device Driver software dealing with devices part of kernel (a big portion) provide uniform abstraction for OS to interact with various types of devices ","date":"2021-12-12","objectID":"/persistence/:0:7","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#device-driver"},{"categories":null,"content":"Linux software stack regarding File System Assuming we have 4 disks!!! ","date":"2021-12-12","objectID":"/persistence/:0:8","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#linux-software-stack-regarding-file-system"},{"categories":null,"content":"Linux software stack regarding File System Assuming we have 4 disks!!! ","date":"2021-12-12","objectID":"/persistence/:0:8","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#assuming-we-have-4-disks"},{"categories":null,"content":"RAID: redundant arrays of inexpensive disks RAID0: no replication –\u003e Striping capacity: 4 * S bytes, where each disk store S bytes performance: sequential read/write: 4 * times bandwidth random read/write: same as snigle disk reliability: no improvement on reliability, any disk failure means the sys fails RAID1: mirror data across two or more disks –\u003e mirroring RAID1-0: stripe of mirror –\u003e 0011 2233 4455 6677 mirror of strip –\u003e 0101 2323 4545 6767 capacity: performance: 2 * S sequential read: 2 times since the disk head moving take time to skip blocks sequential write: 2 * bandwidth as a single disk random read: same as single random write: write both copys at same time, about the same as single disk, slightly higher Reliability: Tolerate one total disk failure RAID4/5: split data across disks bit-wise XOR all blocks in the same stripe, and store the result in the parity block Can reconstruct any missing block from the others RAID 4/5 differ on how parity blocks are distributed read is like raid 0 with one fewer disk write will additionly write the parity disk! for example, if we only change 2, then we can change P0 by (old_p0 XOR old_2) XOR new_2 so To write one block: read old data read old p write new data write new p = (old_p0 XOR old_2) XOR new_2 RAID4 0123 P0 4567 P2 … Performance: the writing process is bottle neck RAID5: rotating parity p0 0 1 2 3 4 p1 5 6 7 … Raid4/5: capacity: 4 * S bytes performance: sequential read: 4 sequential write: bottle necked by parity disk for raid 4, improve for raid5 Reliability: any single disk failure can be tolerated ","date":"2021-12-12","objectID":"/persistence/:0:9","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#raid-redundant-arrays-of-inexpensive-disks"},{"categories":null,"content":"Unix File System to understand the design principles of a very important file system A few main considerations in Unix File sys Design provide a uniform structure for naming and accessing data objects throughout the system Enable data sharing between different programs Ensure consistency with concurrent access acheve good performance/reliability balance hard drive is slow, buffer si important care need to be taken to account for failure Naming file file are organized in a directory tree a dir is also a file —\u003e mapping from name to inode inode is the Unix sys internal data structure for a file ​ inode number uniquely identifies a file across the system FIle sys Metadata Inode: stored in persistent storage, and cached in mem for faster access Directory: special file that contains mapping from names in that dir to their inode numbers Operating on Files ","date":"2021-12-12","objectID":"/persistence/:0:10","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#unix-file-system"},{"categories":null,"content":"Unix File System to understand the design principles of a very important file system A few main considerations in Unix File sys Design provide a uniform structure for naming and accessing data objects throughout the system Enable data sharing between different programs Ensure consistency with concurrent access acheve good performance/reliability balance hard drive is slow, buffer si important care need to be taken to account for failure Naming file file are organized in a directory tree a dir is also a file — mapping from name to inode inode is the Unix sys internal data structure for a file ​ inode number uniquely identifies a file across the system FIle sys Metadata Inode: stored in persistent storage, and cached in mem for faster access Directory: special file that contains mapping from names in that dir to their inode numbers Operating on Files ","date":"2021-12-12","objectID":"/persistence/:0:10","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#a-few-main-considerations-in-unix-file-sys-design"},{"categories":null,"content":"Unix File System to understand the design principles of a very important file system A few main considerations in Unix File sys Design provide a uniform structure for naming and accessing data objects throughout the system Enable data sharing between different programs Ensure consistency with concurrent access acheve good performance/reliability balance hard drive is slow, buffer si important care need to be taken to account for failure Naming file file are organized in a directory tree a dir is also a file — mapping from name to inode inode is the Unix sys internal data structure for a file ​ inode number uniquely identifies a file across the system FIle sys Metadata Inode: stored in persistent storage, and cached in mem for faster access Directory: special file that contains mapping from names in that dir to their inode numbers Operating on Files ","date":"2021-12-12","objectID":"/persistence/:0:10","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#naming-file"},{"categories":null,"content":"Unix File System to understand the design principles of a very important file system A few main considerations in Unix File sys Design provide a uniform structure for naming and accessing data objects throughout the system Enable data sharing between different programs Ensure consistency with concurrent access acheve good performance/reliability balance hard drive is slow, buffer si important care need to be taken to account for failure Naming file file are organized in a directory tree a dir is also a file — mapping from name to inode inode is the Unix sys internal data structure for a file ​ inode number uniquely identifies a file across the system FIle sys Metadata Inode: stored in persistent storage, and cached in mem for faster access Directory: special file that contains mapping from names in that dir to their inode numbers Operating on Files ","date":"2021-12-12","objectID":"/persistence/:0:10","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#file-sys-metadata"},{"categories":null,"content":"Unix File System to understand the design principles of a very important file system A few main considerations in Unix File sys Design provide a uniform structure for naming and accessing data objects throughout the system Enable data sharing between different programs Ensure consistency with concurrent access acheve good performance/reliability balance hard drive is slow, buffer si important care need to be taken to account for failure Naming file file are organized in a directory tree a dir is also a file — mapping from name to inode inode is the Unix sys internal data structure for a file ​ inode number uniquely identifies a file across the system FIle sys Metadata Inode: stored in persistent storage, and cached in mem for faster access Directory: special file that contains mapping from names in that dir to their inode numbers Operating on Files ","date":"2021-12-12","objectID":"/persistence/:0:10","series":null,"tags":["OS","Note"],"title":"Persistence Note","uri":"/persistence/#operating-on-files"},{"categories":null,"content":"UML Class Diagrams ","date":"2021-12-12","objectID":"/uml-class-diagrams/:0:0","series":null,"tags":["UML","Note"],"title":"UML Note","uri":"/uml-class-diagrams/#uml-class-diagrams"},{"categories":null,"content":"Introduction In object-oriented programming, it’s not uncommon to visualize classes and their relationships using the Unified Modeling Language (UML). While UML has many different types of diagrams, this reading focusses only on UML class diagrams. ","date":"2021-12-12","objectID":"/uml-class-diagrams/:1:0","series":null,"tags":["UML","Note"],"title":"UML Note","uri":"/uml-class-diagrams/#introduction"},{"categories":null,"content":"Basic Class Diagram Consider the following UML class diagram: The diagram consists of a rectangle broken up vertically into three sections: Class name. In this case, the fully qualified name of the class is given. Sometimes, if multiple classes are given and assumed to be in the same package, then the simple name is used here instead. Variables. In UML, the variables of a class are called attributes. In this example, there are two private instance variables (indicated with a -) and one private static variable. In UML, attributes and methods are shown to be public with a + symbol to the left of the name. Static variables are underlined. Methods. In UML, the methods of a class are called operations. In this example, there is one public constructor, two public (indicated by +) instance methods, and one public static method. In UML, attributes and methods are shown to be public with a + symbol to the left of the name. Static methods are underlined. The diagram above gives enough information for a programmer (or a program) to generate the following Person.java file: package cs1302.example; public class Person { private String name; private int age; private static int personCounter; public Person(String name, int age) { ... } // Person public String getName() { ... } // getName public int getAge() { ... } // getAge public static int getPersonCounter() { ... } // getPersonCounter } // Person ","date":"2021-12-12","objectID":"/uml-class-diagrams/:1:1","series":null,"tags":["UML","Note"],"title":"UML Note","uri":"/uml-class-diagrams/#basic-class-diagram"},{"categories":null,"content":"Visibilities UML supports the standard four visibilities: Visibility Name Modifier Keyword UML Symbol private private - package private ~ protected protected # public public + For an in-depth discussion on visibilities, see the Visibility Reading. ","date":"2021-12-12","objectID":"/uml-class-diagrams/:1:2","series":null,"tags":["UML","Note"],"title":"UML Note","uri":"/uml-class-diagrams/#visibilities"},{"categories":null,"content":"Atributes and Parameters (Variables) In UML, attributes and parameters are written in the following format: visibility name : type ","date":"2021-12-12","objectID":"/uml-class-diagrams/:1:3","series":null,"tags":["UML","Note"],"title":"UML Note","uri":"/uml-class-diagrams/#atributes-and-parameters-variables"},{"categories":null,"content":"Operations (Methods) In UML, operations are written in the following format: visibility [\u003c\u003cstereotype\u003e\u003e] methodName(param1: type, param2: type): returnType ","date":"2021-12-12","objectID":"/uml-class-diagrams/:1:4","series":null,"tags":["UML","Note"],"title":"UML Note","uri":"/uml-class-diagrams/#operations-methods"},{"categories":null,"content":"Stereotypes A stereotype is optional and is used to convey additional information. For example, you might use the following stereotypes in different situations: Stereotype Description \u003c\u003cnew\u003e\u003e Denotes constructor. \u003c\u003cabstract\u003e\u003e Denotes abstract. \u003c\u003coverride\u003e\u003e Denotes override. \u003c\u003cinterface\u003e\u003e Denotes interface. \u003c\u003cfinal\u003e\u003e Denotes final. ","date":"2021-12-12","objectID":"/uml-class-diagrams/:1:5","series":null,"tags":["UML","Note"],"title":"UML Note","uri":"/uml-class-diagrams/#stereotypes"},{"categories":null,"content":"Abstract Classes and Operations The usual way to denote that a class or operation is abstract is to italicize the name of class or operation. Sometimes this is impractical in situations where it’s difficult to discern the difference between the normal lettering of a font and its italicized version. In such casses, the names might also be prefixed with an \u003c\u003cabstract\u003e\u003e stereotype in order to better communicate the intention to the viewer of the diagram. To illustrate the differences, consider the following three Shape classes: In the diagram, the first class on the left is not abstract and the other two are. While the italics in the middle class indicate that its abstract, it can be easily confused as a non-abstract class if viewed quickly. For this reason, we suggest you italicize and use a stereotype to indicate that a class is abstract just as is done with the third class in the diagram. ","date":"2021-12-12","objectID":"/uml-class-diagrams/:1:6","series":null,"tags":["UML","Note"],"title":"UML Note","uri":"/uml-class-diagrams/#abstract-classes-and-operations"},{"categories":null,"content":"Associations When you have more than one class in a diagram, you often want to express how they are associated. In UML, this is done with association arrows. While UML supports many different kinds of association arrows, the following are arguably the most common: Association Description Usage Solid line; open arrowhead. ClassA uses ClassB Solid line; unfilled triangle arrowhead. Child extends Parent Dashed line; unfilled triangle arrowhead. SomeClass implements SomeInterface It may seem nit picky, but each of these arrows is visually different with respect to its line and its arrowhead! ","date":"2021-12-12","objectID":"/uml-class-diagrams/:1:7","series":null,"tags":["UML","Note"],"title":"UML Note","uri":"/uml-class-diagrams/#associations"},{"categories":null,"content":"UML Software and Tools Before you continue reading, it’s important to note that you do not need a special program to work with UML. It’s quite possible (and encouraged) that you practice drawing UML digrams by hand either on paper or in your note-taking software. The purpose of a UML class diagram is to help visualize multiple classes and their relationships. Over the years, we’ve seen many students skip directly to using UML software and get frustrated. Since these programs have a learning curve, students spend too much time trying to figure out, for example, how to mark something as protected in the program when they could have simply written # had they done it by hand. Don’t get us wrong, these tools are great, and if you want to learn them, then you should. Just try to make sure it doesn’t impact your productivity, especially near a deadline. Below is a list of popular UML software programs. You are not required to have access to one for this course. While many of these programs do require a paid license, you are encouraged to seek out a free community edition or a free/reduced-price student license before making any purchases. If you find a tool that’s not in the list, then please share it on Piazza! Astah UML Diagrams.net StarUML Umbrello UML Designer Visual Paradigm You may be surprised by how much a regular license costs for some of the programs above. In practice, if you need a program like this for your job, then it’s not uncommon for your company to pay for the license just as many companies do for programs like Microsoft Word, Adobe Photoshop, etc. Copyright © Michael E. Cotterell, Bradley J. Barnes, and the University of Georgia. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License to students and the public. The content and opinions expressed on this Web page do not necessarily reflect the views of nor are they endorsed by the University of Georgia or the University System of Georgia. ","date":"2021-12-12","objectID":"/uml-class-diagrams/:2:0","series":null,"tags":["UML","Note"],"title":"UML Note","uri":"/uml-class-diagrams/#uml-software-and-tools"},{"categories":null,"content":"Termial Oriented Programming ","date":"2021-12-12","objectID":"/vimnotes/:0:0","series":null,"tags":["Vim","Note"],"title":"Vim Note","uri":"/vimnotes/#termial-oriented-programming"},{"categories":null,"content":"Some Useful Info https://vim-adventures.com/ This is a game for vim beginner. https://www.destroyallsoftware.com/screencasts This is an amazing guy introducing the power of the terminal. Doing everything in one terminal window is powerful. ","date":"2021-12-12","objectID":"/vimnotes/:0:1","series":null,"tags":["Vim","Note"],"title":"Vim Note","uri":"/vimnotes/#some-useful-info"},{"categories":null,"content":"Jobs Control jobs ps fg bg fg %1 kill %2 kill -9 24153 ./some_codes.sh \u0026 ctrl + z to suspend the current job including when you are in the Vim ctrl + c to kill the current job ","date":"2021-12-12","objectID":"/vimnotes/:0:2","series":null,"tags":["Vim","Note"],"title":"Vim Note","uri":"/vimnotes/#jobs-control"},{"categories":null,"content":"Map :map ,l :!g++ std=c++11 % -Wall -o name :map :map ,l :! ls -a :! command1 \u0026\u0026 command2 means that command 1 has to be suceeded before the command 2 runs :! c1 || c2 means that no matter c1 runs successfully or not the c2 will run :map ,l :w \u0026\u0026 :!g++ std=c++11 % -Wall -o name \u0026\u0026 name This is actually not a good practice since you may want to manually control when you save the file :vs :map move to the right vim session :! echo % prints out the current file name :!echo %:r prints out the current file name without extension, actually it just remove the very last extension, but you can use it multiple times. :!echo %:h header, it will give you just a . since the current folder is presented as . :!echo %:p:h this will give you the current folder’s full path :!echo %:p this is full path :!echo %:~ this is relative path to home, will stay unmodified if it’s not under home dir :!echo %:. reduced path e.g. test.txt :!mv currentPath newPath the way to rename a file or maybe add a plugin :map .. :e %:r_spec.rb it lets you jump to corresponding spec file And we also have many other map ways :nm[ap] :vm[ap] :xm[ap] and etc. which let you map the keys for diff modes :unmap .l :noremap lhs rhs means the map cannot be mapped recursively :remap works the oppsite :!! repeat the last one :ter Opens a terminal ","date":"2021-12-12","objectID":"/vimnotes/:0:3","series":null,"tags":["Vim","Note"],"title":"Vim Note","uri":"/vimnotes/#map"},{"categories":null,"content":"Commands verbs mm =\u003e mark here as m `m =\u003e go back to the mark m d =\u003e cut dw means delete a word c =\u003e change (delete and enter the insert mode) =\u003e indent v =\u003e visually select y =\u003e Yank (copy) u =\u003e undo ctrl+r =\u003e redo . =\u003e repeat the last operation motions w =\u003e word (forward by a “word”) b =\u003e back (back by a “word”) 2j =\u003e down 2 lines yy =\u003e copy the entire lines text object iw =\u003e “inner word” (works from anywhere in a word) it =\u003e “inner tag” (the contents of an HTML tag) i\" =\u003e “inner quotes” ip =\u003e “inner paragraph” as =\u003e “a sentence” Nouns –Parameterized Text Objects f =\u003e find the next character F reverse the direction t =\u003e find the next char but not include that char T reverse / search the next match ? reverse Combinations of Commands combination of these commands are already powerful and there are more to explore Relative Numbers :set relativenumber =\u003e this is so powerful!! Visual Mode is a Smell visual breaks repatability! be mindful of this, don;t use it unless you have to Custom Operators (https://www.youtube.com/watch?v=wlR5gYd6um0) Surround Commentary ReplaceWithRegister Titlecase Sort-motion System-copy Custom Text Objects Indent Entire Line Ruby block ","date":"2021-12-12","objectID":"/vimnotes/:0:4","series":null,"tags":["Vim","Note"],"title":"Vim Note","uri":"/vimnotes/#commands"},{"categories":null,"content":"Commands verbs mm = mark here as m `m = go back to the mark m d = cut dw means delete a word c = change (delete and enter the insert mode) = indent v = visually select y = Yank (copy) u = undo ctrl+r = redo . = repeat the last operation motions w = word (forward by a “word”) b = back (back by a “word”) 2j = down 2 lines yy = copy the entire lines text object iw = “inner word” (works from anywhere in a word) it = “inner tag” (the contents of an HTML tag) i\" = “inner quotes” ip = “inner paragraph” as = “a sentence” Nouns –Parameterized Text Objects f = find the next character F reverse the direction t = find the next char but not include that char T reverse / search the next match ? reverse Combinations of Commands combination of these commands are already powerful and there are more to explore Relative Numbers :set relativenumber = this is so powerful!! Visual Mode is a Smell visual breaks repatability! be mindful of this, don;t use it unless you have to Custom Operators (https://www.youtube.com/watch?v=wlR5gYd6um0) Surround Commentary ReplaceWithRegister Titlecase Sort-motion System-copy Custom Text Objects Indent Entire Line Ruby block ","date":"2021-12-12","objectID":"/vimnotes/:0:4","series":null,"tags":["Vim","Note"],"title":"Vim Note","uri":"/vimnotes/#verbs"},{"categories":null,"content":"Commands verbs mm = mark here as m `m = go back to the mark m d = cut dw means delete a word c = change (delete and enter the insert mode) = indent v = visually select y = Yank (copy) u = undo ctrl+r = redo . = repeat the last operation motions w = word (forward by a “word”) b = back (back by a “word”) 2j = down 2 lines yy = copy the entire lines text object iw = “inner word” (works from anywhere in a word) it = “inner tag” (the contents of an HTML tag) i\" = “inner quotes” ip = “inner paragraph” as = “a sentence” Nouns –Parameterized Text Objects f = find the next character F reverse the direction t = find the next char but not include that char T reverse / search the next match ? reverse Combinations of Commands combination of these commands are already powerful and there are more to explore Relative Numbers :set relativenumber = this is so powerful!! Visual Mode is a Smell visual breaks repatability! be mindful of this, don;t use it unless you have to Custom Operators (https://www.youtube.com/watch?v=wlR5gYd6um0) Surround Commentary ReplaceWithRegister Titlecase Sort-motion System-copy Custom Text Objects Indent Entire Line Ruby block ","date":"2021-12-12","objectID":"/vimnotes/:0:4","series":null,"tags":["Vim","Note"],"title":"Vim Note","uri":"/vimnotes/#motions"},{"categories":null,"content":"Commands verbs mm = mark here as m `m = go back to the mark m d = cut dw means delete a word c = change (delete and enter the insert mode) = indent v = visually select y = Yank (copy) u = undo ctrl+r = redo . = repeat the last operation motions w = word (forward by a “word”) b = back (back by a “word”) 2j = down 2 lines yy = copy the entire lines text object iw = “inner word” (works from anywhere in a word) it = “inner tag” (the contents of an HTML tag) i\" = “inner quotes” ip = “inner paragraph” as = “a sentence” Nouns –Parameterized Text Objects f = find the next character F reverse the direction t = find the next char but not include that char T reverse / search the next match ? reverse Combinations of Commands combination of these commands are already powerful and there are more to explore Relative Numbers :set relativenumber = this is so powerful!! Visual Mode is a Smell visual breaks repatability! be mindful of this, don;t use it unless you have to Custom Operators (https://www.youtube.com/watch?v=wlR5gYd6um0) Surround Commentary ReplaceWithRegister Titlecase Sort-motion System-copy Custom Text Objects Indent Entire Line Ruby block ","date":"2021-12-12","objectID":"/vimnotes/:0:4","series":null,"tags":["Vim","Note"],"title":"Vim Note","uri":"/vimnotes/#text-object"},{"categories":null,"content":"Commands verbs mm = mark here as m `m = go back to the mark m d = cut dw means delete a word c = change (delete and enter the insert mode) = indent v = visually select y = Yank (copy) u = undo ctrl+r = redo . = repeat the last operation motions w = word (forward by a “word”) b = back (back by a “word”) 2j = down 2 lines yy = copy the entire lines text object iw = “inner word” (works from anywhere in a word) it = “inner tag” (the contents of an HTML tag) i\" = “inner quotes” ip = “inner paragraph” as = “a sentence” Nouns –Parameterized Text Objects f = find the next character F reverse the direction t = find the next char but not include that char T reverse / search the next match ? reverse Combinations of Commands combination of these commands are already powerful and there are more to explore Relative Numbers :set relativenumber = this is so powerful!! Visual Mode is a Smell visual breaks repatability! be mindful of this, don;t use it unless you have to Custom Operators (https://www.youtube.com/watch?v=wlR5gYd6um0) Surround Commentary ReplaceWithRegister Titlecase Sort-motion System-copy Custom Text Objects Indent Entire Line Ruby block ","date":"2021-12-12","objectID":"/vimnotes/:0:4","series":null,"tags":["Vim","Note"],"title":"Vim Note","uri":"/vimnotes/#nouns---parameterized-text-objects"},{"categories":null,"content":"Commands verbs mm = mark here as m `m = go back to the mark m d = cut dw means delete a word c = change (delete and enter the insert mode) = indent v = visually select y = Yank (copy) u = undo ctrl+r = redo . = repeat the last operation motions w = word (forward by a “word”) b = back (back by a “word”) 2j = down 2 lines yy = copy the entire lines text object iw = “inner word” (works from anywhere in a word) it = “inner tag” (the contents of an HTML tag) i\" = “inner quotes” ip = “inner paragraph” as = “a sentence” Nouns –Parameterized Text Objects f = find the next character F reverse the direction t = find the next char but not include that char T reverse / search the next match ? reverse Combinations of Commands combination of these commands are already powerful and there are more to explore Relative Numbers :set relativenumber = this is so powerful!! Visual Mode is a Smell visual breaks repatability! be mindful of this, don;t use it unless you have to Custom Operators (https://www.youtube.com/watch?v=wlR5gYd6um0) Surround Commentary ReplaceWithRegister Titlecase Sort-motion System-copy Custom Text Objects Indent Entire Line Ruby block ","date":"2021-12-12","objectID":"/vimnotes/:0:4","series":null,"tags":["Vim","Note"],"title":"Vim Note","uri":"/vimnotes/#combinations-of-commands"},{"categories":null,"content":"Commands verbs mm = mark here as m `m = go back to the mark m d = cut dw means delete a word c = change (delete and enter the insert mode) = indent v = visually select y = Yank (copy) u = undo ctrl+r = redo . = repeat the last operation motions w = word (forward by a “word”) b = back (back by a “word”) 2j = down 2 lines yy = copy the entire lines text object iw = “inner word” (works from anywhere in a word) it = “inner tag” (the contents of an HTML tag) i\" = “inner quotes” ip = “inner paragraph” as = “a sentence” Nouns –Parameterized Text Objects f = find the next character F reverse the direction t = find the next char but not include that char T reverse / search the next match ? reverse Combinations of Commands combination of these commands are already powerful and there are more to explore Relative Numbers :set relativenumber = this is so powerful!! Visual Mode is a Smell visual breaks repatability! be mindful of this, don;t use it unless you have to Custom Operators (https://www.youtube.com/watch?v=wlR5gYd6um0) Surround Commentary ReplaceWithRegister Titlecase Sort-motion System-copy Custom Text Objects Indent Entire Line Ruby block ","date":"2021-12-12","objectID":"/vimnotes/:0:4","series":null,"tags":["Vim","Note"],"title":"Vim Note","uri":"/vimnotes/#relative-numbers"},{"categories":null,"content":"Commands verbs mm = mark here as m `m = go back to the mark m d = cut dw means delete a word c = change (delete and enter the insert mode) = indent v = visually select y = Yank (copy) u = undo ctrl+r = redo . = repeat the last operation motions w = word (forward by a “word”) b = back (back by a “word”) 2j = down 2 lines yy = copy the entire lines text object iw = “inner word” (works from anywhere in a word) it = “inner tag” (the contents of an HTML tag) i\" = “inner quotes” ip = “inner paragraph” as = “a sentence” Nouns –Parameterized Text Objects f = find the next character F reverse the direction t = find the next char but not include that char T reverse / search the next match ? reverse Combinations of Commands combination of these commands are already powerful and there are more to explore Relative Numbers :set relativenumber = this is so powerful!! Visual Mode is a Smell visual breaks repatability! be mindful of this, don;t use it unless you have to Custom Operators (https://www.youtube.com/watch?v=wlR5gYd6um0) Surround Commentary ReplaceWithRegister Titlecase Sort-motion System-copy Custom Text Objects Indent Entire Line Ruby block ","date":"2021-12-12","objectID":"/vimnotes/:0:4","series":null,"tags":["Vim","Note"],"title":"Vim Note","uri":"/vimnotes/#visual-mode-is-a-smell"},{"categories":null,"content":"Commands verbs mm = mark here as m `m = go back to the mark m d = cut dw means delete a word c = change (delete and enter the insert mode) = indent v = visually select y = Yank (copy) u = undo ctrl+r = redo . = repeat the last operation motions w = word (forward by a “word”) b = back (back by a “word”) 2j = down 2 lines yy = copy the entire lines text object iw = “inner word” (works from anywhere in a word) it = “inner tag” (the contents of an HTML tag) i\" = “inner quotes” ip = “inner paragraph” as = “a sentence” Nouns –Parameterized Text Objects f = find the next character F reverse the direction t = find the next char but not include that char T reverse / search the next match ? reverse Combinations of Commands combination of these commands are already powerful and there are more to explore Relative Numbers :set relativenumber = this is so powerful!! Visual Mode is a Smell visual breaks repatability! be mindful of this, don;t use it unless you have to Custom Operators (https://www.youtube.com/watch?v=wlR5gYd6um0) Surround Commentary ReplaceWithRegister Titlecase Sort-motion System-copy Custom Text Objects Indent Entire Line Ruby block ","date":"2021-12-12","objectID":"/vimnotes/:0:4","series":null,"tags":["Vim","Note"],"title":"Vim Note","uri":"/vimnotes/#custom-operators-httpswwwyoutubecomwatchvwlr5gyd6um0"},{"categories":null,"content":"Commands verbs mm = mark here as m `m = go back to the mark m d = cut dw means delete a word c = change (delete and enter the insert mode) = indent v = visually select y = Yank (copy) u = undo ctrl+r = redo . = repeat the last operation motions w = word (forward by a “word”) b = back (back by a “word”) 2j = down 2 lines yy = copy the entire lines text object iw = “inner word” (works from anywhere in a word) it = “inner tag” (the contents of an HTML tag) i\" = “inner quotes” ip = “inner paragraph” as = “a sentence” Nouns –Parameterized Text Objects f = find the next character F reverse the direction t = find the next char but not include that char T reverse / search the next match ? reverse Combinations of Commands combination of these commands are already powerful and there are more to explore Relative Numbers :set relativenumber = this is so powerful!! Visual Mode is a Smell visual breaks repatability! be mindful of this, don;t use it unless you have to Custom Operators (https://www.youtube.com/watch?v=wlR5gYd6um0) Surround Commentary ReplaceWithRegister Titlecase Sort-motion System-copy Custom Text Objects Indent Entire Line Ruby block ","date":"2021-12-12","objectID":"/vimnotes/:0:4","series":null,"tags":["Vim","Note"],"title":"Vim Note","uri":"/vimnotes/#custom-text-objects"},{"categories":null,"content":"OS_notes_for book_reading ","date":"2021-12-12","objectID":"/os_reading_note/:0:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#os_notes_for-book_reading"},{"categories":null,"content":"Virtual Memory ","date":"2021-12-12","objectID":"/os_reading_note/:1:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#virtual-memory"},{"categories":null,"content":"Paging Segmentation leads to fragmentation so that we need the new idea of paging. Pages are memory chunks with fixed size, we can view physical memory as an array of fixed-sized slots called page frames. Paging gives us flexibility so that we don’t need to whether the way a process uses to address space will cause external fragmentation. Paging also gives us the simplicity of free-space management that paging affords. The OS keeps a free list of all free pages. ","date":"2021-12-12","objectID":"/os_reading_note/:1:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#paging"},{"categories":null,"content":"Page Table The major role of page table is to store address translations for each of the virtual pages of the address space, thus letting us know where in physical memory each page resides. It’s important to know that page table is a per-process data structure. An exception of this is the inverted page table. Notice Notice that two same virtual addresses in different processes may lead to a same physical frame. Translation VA –\u003e VPN(Virtual Address Number) + offset For example, the page size is 16 bytes in a 64-byte address space. Then we need 4 pages, the virtual page number will be indicated by the first 2-bit. VPN –\u003e PFN(Physical Frame Number) Physical Address = VFN + offset Where are page tables stored? Page tables can get terribly large, much larger than the small segment table or base/bounds pair. For example, for a typical 32-bit address space, with 4KB pages. This VA splits into 20-bit VPN and 12-bit offset. 20 bits implies 2^20 translations that the OS would have to manage for each process. Assuming we need 4 bytes per PTE to hold the PFN + control bits, then we need an immense 4MB of memory for each page table. And this is only for one process. What’s in the page table? The simplest one is the linear page table, which is just an array. The OS indexes the array by the virtual page number, and looks up the page-table entry at that VPN to find the desired PFN. We also have control bits such as: Valid bit ==\u003e indicating whether the particular translation is valid. All unused space will be marked invalid. Protection bit ==\u003e indicating whether the page can be read, written, or executed. Present bit ==\u003e indicating whether this page is in physical memory or on disk. Dirty bit ==\u003e indicating whether the page has been modified Reference bit(accessed bit) ==\u003e track whether a page has been accessed, and this is useful to tell which page is popular so that we should keep in memory. Problem: paging is so slow To fetch the desired data: fetch the proper page table entry from the process’s page table. Perform the translation. Load the data from ther physical memory. To do so, the hardware must know where to find the page table of the current running process. Thus we need a page-table base register holding the location. However, implementing paging without care will lead to a slower machine as well as memory waste. ","date":"2021-12-12","objectID":"/os_reading_note/:1:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#page-table"},{"categories":null,"content":"Page Table The major role of page table is to store address translations for each of the virtual pages of the address space, thus letting us know where in physical memory each page resides. It’s important to know that page table is a per-process data structure. An exception of this is the inverted page table. Notice Notice that two same virtual addresses in different processes may lead to a same physical frame. Translation VA – VPN(Virtual Address Number) + offset For example, the page size is 16 bytes in a 64-byte address space. Then we need 4 pages, the virtual page number will be indicated by the first 2-bit. VPN – PFN(Physical Frame Number) Physical Address = VFN + offset Where are page tables stored? Page tables can get terribly large, much larger than the small segment table or base/bounds pair. For example, for a typical 32-bit address space, with 4KB pages. This VA splits into 20-bit VPN and 12-bit offset. 20 bits implies 2^20 translations that the OS would have to manage for each process. Assuming we need 4 bytes per PTE to hold the PFN + control bits, then we need an immense 4MB of memory for each page table. And this is only for one process. What’s in the page table? The simplest one is the linear page table, which is just an array. The OS indexes the array by the virtual page number, and looks up the page-table entry at that VPN to find the desired PFN. We also have control bits such as: Valid bit == indicating whether the particular translation is valid. All unused space will be marked invalid. Protection bit == indicating whether the page can be read, written, or executed. Present bit == indicating whether this page is in physical memory or on disk. Dirty bit == indicating whether the page has been modified Reference bit(accessed bit) == track whether a page has been accessed, and this is useful to tell which page is popular so that we should keep in memory. Problem: paging is so slow To fetch the desired data: fetch the proper page table entry from the process’s page table. Perform the translation. Load the data from ther physical memory. To do so, the hardware must know where to find the page table of the current running process. Thus we need a page-table base register holding the location. However, implementing paging without care will lead to a slower machine as well as memory waste. ","date":"2021-12-12","objectID":"/os_reading_note/:1:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#notice"},{"categories":null,"content":"Page Table The major role of page table is to store address translations for each of the virtual pages of the address space, thus letting us know where in physical memory each page resides. It’s important to know that page table is a per-process data structure. An exception of this is the inverted page table. Notice Notice that two same virtual addresses in different processes may lead to a same physical frame. Translation VA – VPN(Virtual Address Number) + offset For example, the page size is 16 bytes in a 64-byte address space. Then we need 4 pages, the virtual page number will be indicated by the first 2-bit. VPN – PFN(Physical Frame Number) Physical Address = VFN + offset Where are page tables stored? Page tables can get terribly large, much larger than the small segment table or base/bounds pair. For example, for a typical 32-bit address space, with 4KB pages. This VA splits into 20-bit VPN and 12-bit offset. 20 bits implies 2^20 translations that the OS would have to manage for each process. Assuming we need 4 bytes per PTE to hold the PFN + control bits, then we need an immense 4MB of memory for each page table. And this is only for one process. What’s in the page table? The simplest one is the linear page table, which is just an array. The OS indexes the array by the virtual page number, and looks up the page-table entry at that VPN to find the desired PFN. We also have control bits such as: Valid bit == indicating whether the particular translation is valid. All unused space will be marked invalid. Protection bit == indicating whether the page can be read, written, or executed. Present bit == indicating whether this page is in physical memory or on disk. Dirty bit == indicating whether the page has been modified Reference bit(accessed bit) == track whether a page has been accessed, and this is useful to tell which page is popular so that we should keep in memory. Problem: paging is so slow To fetch the desired data: fetch the proper page table entry from the process’s page table. Perform the translation. Load the data from ther physical memory. To do so, the hardware must know where to find the page table of the current running process. Thus we need a page-table base register holding the location. However, implementing paging without care will lead to a slower machine as well as memory waste. ","date":"2021-12-12","objectID":"/os_reading_note/:1:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#translation"},{"categories":null,"content":"Page Table The major role of page table is to store address translations for each of the virtual pages of the address space, thus letting us know where in physical memory each page resides. It’s important to know that page table is a per-process data structure. An exception of this is the inverted page table. Notice Notice that two same virtual addresses in different processes may lead to a same physical frame. Translation VA – VPN(Virtual Address Number) + offset For example, the page size is 16 bytes in a 64-byte address space. Then we need 4 pages, the virtual page number will be indicated by the first 2-bit. VPN – PFN(Physical Frame Number) Physical Address = VFN + offset Where are page tables stored? Page tables can get terribly large, much larger than the small segment table or base/bounds pair. For example, for a typical 32-bit address space, with 4KB pages. This VA splits into 20-bit VPN and 12-bit offset. 20 bits implies 2^20 translations that the OS would have to manage for each process. Assuming we need 4 bytes per PTE to hold the PFN + control bits, then we need an immense 4MB of memory for each page table. And this is only for one process. What’s in the page table? The simplest one is the linear page table, which is just an array. The OS indexes the array by the virtual page number, and looks up the page-table entry at that VPN to find the desired PFN. We also have control bits such as: Valid bit == indicating whether the particular translation is valid. All unused space will be marked invalid. Protection bit == indicating whether the page can be read, written, or executed. Present bit == indicating whether this page is in physical memory or on disk. Dirty bit == indicating whether the page has been modified Reference bit(accessed bit) == track whether a page has been accessed, and this is useful to tell which page is popular so that we should keep in memory. Problem: paging is so slow To fetch the desired data: fetch the proper page table entry from the process’s page table. Perform the translation. Load the data from ther physical memory. To do so, the hardware must know where to find the page table of the current running process. Thus we need a page-table base register holding the location. However, implementing paging without care will lead to a slower machine as well as memory waste. ","date":"2021-12-12","objectID":"/os_reading_note/:1:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#where-are-page-tables-stored"},{"categories":null,"content":"Page Table The major role of page table is to store address translations for each of the virtual pages of the address space, thus letting us know where in physical memory each page resides. It’s important to know that page table is a per-process data structure. An exception of this is the inverted page table. Notice Notice that two same virtual addresses in different processes may lead to a same physical frame. Translation VA – VPN(Virtual Address Number) + offset For example, the page size is 16 bytes in a 64-byte address space. Then we need 4 pages, the virtual page number will be indicated by the first 2-bit. VPN – PFN(Physical Frame Number) Physical Address = VFN + offset Where are page tables stored? Page tables can get terribly large, much larger than the small segment table or base/bounds pair. For example, for a typical 32-bit address space, with 4KB pages. This VA splits into 20-bit VPN and 12-bit offset. 20 bits implies 2^20 translations that the OS would have to manage for each process. Assuming we need 4 bytes per PTE to hold the PFN + control bits, then we need an immense 4MB of memory for each page table. And this is only for one process. What’s in the page table? The simplest one is the linear page table, which is just an array. The OS indexes the array by the virtual page number, and looks up the page-table entry at that VPN to find the desired PFN. We also have control bits such as: Valid bit == indicating whether the particular translation is valid. All unused space will be marked invalid. Protection bit == indicating whether the page can be read, written, or executed. Present bit == indicating whether this page is in physical memory or on disk. Dirty bit == indicating whether the page has been modified Reference bit(accessed bit) == track whether a page has been accessed, and this is useful to tell which page is popular so that we should keep in memory. Problem: paging is so slow To fetch the desired data: fetch the proper page table entry from the process’s page table. Perform the translation. Load the data from ther physical memory. To do so, the hardware must know where to find the page table of the current running process. Thus we need a page-table base register holding the location. However, implementing paging without care will lead to a slower machine as well as memory waste. ","date":"2021-12-12","objectID":"/os_reading_note/:1:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#whats-in-the-page-table"},{"categories":null,"content":"Page Table The major role of page table is to store address translations for each of the virtual pages of the address space, thus letting us know where in physical memory each page resides. It’s important to know that page table is a per-process data structure. An exception of this is the inverted page table. Notice Notice that two same virtual addresses in different processes may lead to a same physical frame. Translation VA – VPN(Virtual Address Number) + offset For example, the page size is 16 bytes in a 64-byte address space. Then we need 4 pages, the virtual page number will be indicated by the first 2-bit. VPN – PFN(Physical Frame Number) Physical Address = VFN + offset Where are page tables stored? Page tables can get terribly large, much larger than the small segment table or base/bounds pair. For example, for a typical 32-bit address space, with 4KB pages. This VA splits into 20-bit VPN and 12-bit offset. 20 bits implies 2^20 translations that the OS would have to manage for each process. Assuming we need 4 bytes per PTE to hold the PFN + control bits, then we need an immense 4MB of memory for each page table. And this is only for one process. What’s in the page table? The simplest one is the linear page table, which is just an array. The OS indexes the array by the virtual page number, and looks up the page-table entry at that VPN to find the desired PFN. We also have control bits such as: Valid bit == indicating whether the particular translation is valid. All unused space will be marked invalid. Protection bit == indicating whether the page can be read, written, or executed. Present bit == indicating whether this page is in physical memory or on disk. Dirty bit == indicating whether the page has been modified Reference bit(accessed bit) == track whether a page has been accessed, and this is useful to tell which page is popular so that we should keep in memory. Problem: paging is so slow To fetch the desired data: fetch the proper page table entry from the process’s page table. Perform the translation. Load the data from ther physical memory. To do so, the hardware must know where to find the page table of the current running process. Thus we need a page-table base register holding the location. However, implementing paging without care will lead to a slower machine as well as memory waste. ","date":"2021-12-12","objectID":"/os_reading_note/:1:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#problem-paging-is-so-slow"},{"categories":null,"content":"TLBs Translation-lookaside Buffer(TLB) A TLB is a part of memory-management unit(MMU), and is a hardware cache. TLB Basic Algorithm Extract the VPN from the VA and check if the TLB holds it. If yes, then we have a TLB hit. We can now extract the PFN from the relevant TLB entry, concatenate that onto the offset from the original VA, and form the PA we want. If protection checks do not fail, then we can perform the operation correctly. If no, then we have a TLB miss. Then we have to walk the page table, the memory, or even copy the data from disk. And then update the TLB. Note that page fault can be triggered by both TLB hit and TLB miss. The TLB hit rate will greatly affect the performance, so we need to find the sweet spot for the TLB size. Also, if the TLB size is too large, walk the TLB will consume more time. And also, if you want a fast cache, it has to be small. This is the physical limit. TLB also improves the performance due to spatial locality even if it’s the first time to visit that memory space. Who handle the TLB miss? Could be hardware or software. Hardware: the hardware has to know where exactly the page tables are located in memory(page-table base register), as well as the format. On a miss, the hardware will walk through the page table, find the right PTE and extract the desired translation, update the TLB with the translation, and retry the instruction. For example, the Intel x86 architecture uses a fixed multi-level page table, and the current table is pointed to by the CR3 register. Software: More modern architecture such as RISC, has a software-managed TLB. On a TLB miss, the hardware will raise an exception to pause the current instruction stream. Then it traps into kernel mode and jumps to a trap handler dealing with TLB misses. The code will look up the page table and update the TLB and return from the trap. The hardware will retry the instruction. Notice that the return-from-trap behaves differently from that in a system call. In the system call case, it should resume execution at the instruction after the trap into the OS. However, in this case, the hardware must resume execution at the instruction that cause the trap and retry it so that it will has a TLB hit this time. We need be aware of the OS need to save different PC in these two cases. Also, when running the TLB miss-handling code, the OS need to be extra careful not to cause an infinite chain of TLB misses to occur. The solution would be to keep TLB miss handlers in physical memory that are unmapped and not subject to address translation. Or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself, these wired translations always hit in the TLB. Aside: TLB valid bit != page table valid bit TLB valid bit indicates whether a TLB entry has a valid translation in it. The page table valid bit indicates the page has not been allocated by the process, and should not be accessed by a correctly working program. The TLB valid bit is quite useful when performing a context switch too. By setting all TLB entries to invalid, the system can ensure that the about-to-run process does not accidentally use a virtual-to-physical translation from a previous process. IMPORTANT: The dirty bit in PTE indicates where the PTE is modified rather than whether the memory location is modified, which is indicated by the dirty bit of that memory location. Based on this fact, we can know that the content of PTE in TLB does not need to be the same as the one in the page table since whether the memory is modified is indicated by the memory itself rather than the PTE. In addition, TLB PTE doesn’t need to be the as same as the one in the page table because the OS can keep just TLB updated rather than both. Think of about this, if we have the PTE in TLB, when we want to go to that PA, we always get a TLB hit and will never walk the page table. The additional work would be to update the PTE in th","date":"2021-12-12","objectID":"/os_reading_note/:1:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#tlbs"},{"categories":null,"content":"TLBs Translation-lookaside Buffer(TLB) A TLB is a part of memory-management unit(MMU), and is a hardware cache. TLB Basic Algorithm Extract the VPN from the VA and check if the TLB holds it. If yes, then we have a TLB hit. We can now extract the PFN from the relevant TLB entry, concatenate that onto the offset from the original VA, and form the PA we want. If protection checks do not fail, then we can perform the operation correctly. If no, then we have a TLB miss. Then we have to walk the page table, the memory, or even copy the data from disk. And then update the TLB. Note that page fault can be triggered by both TLB hit and TLB miss. The TLB hit rate will greatly affect the performance, so we need to find the sweet spot for the TLB size. Also, if the TLB size is too large, walk the TLB will consume more time. And also, if you want a fast cache, it has to be small. This is the physical limit. TLB also improves the performance due to spatial locality even if it’s the first time to visit that memory space. Who handle the TLB miss? Could be hardware or software. Hardware: the hardware has to know where exactly the page tables are located in memory(page-table base register), as well as the format. On a miss, the hardware will walk through the page table, find the right PTE and extract the desired translation, update the TLB with the translation, and retry the instruction. For example, the Intel x86 architecture uses a fixed multi-level page table, and the current table is pointed to by the CR3 register. Software: More modern architecture such as RISC, has a software-managed TLB. On a TLB miss, the hardware will raise an exception to pause the current instruction stream. Then it traps into kernel mode and jumps to a trap handler dealing with TLB misses. The code will look up the page table and update the TLB and return from the trap. The hardware will retry the instruction. Notice that the return-from-trap behaves differently from that in a system call. In the system call case, it should resume execution at the instruction after the trap into the OS. However, in this case, the hardware must resume execution at the instruction that cause the trap and retry it so that it will has a TLB hit this time. We need be aware of the OS need to save different PC in these two cases. Also, when running the TLB miss-handling code, the OS need to be extra careful not to cause an infinite chain of TLB misses to occur. The solution would be to keep TLB miss handlers in physical memory that are unmapped and not subject to address translation. Or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself, these wired translations always hit in the TLB. Aside: TLB valid bit != page table valid bit TLB valid bit indicates whether a TLB entry has a valid translation in it. The page table valid bit indicates the page has not been allocated by the process, and should not be accessed by a correctly working program. The TLB valid bit is quite useful when performing a context switch too. By setting all TLB entries to invalid, the system can ensure that the about-to-run process does not accidentally use a virtual-to-physical translation from a previous process. IMPORTANT: The dirty bit in PTE indicates where the PTE is modified rather than whether the memory location is modified, which is indicated by the dirty bit of that memory location. Based on this fact, we can know that the content of PTE in TLB does not need to be the same as the one in the page table since whether the memory is modified is indicated by the memory itself rather than the PTE. In addition, TLB PTE doesn’t need to be the as same as the one in the page table because the OS can keep just TLB updated rather than both. Think of about this, if we have the PTE in TLB, when we want to go to that PA, we always get a TLB hit and will never walk the page table. The additional work would be to update the PTE in th","date":"2021-12-12","objectID":"/os_reading_note/:1:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#translation-lookaside-buffertlb"},{"categories":null,"content":"TLBs Translation-lookaside Buffer(TLB) A TLB is a part of memory-management unit(MMU), and is a hardware cache. TLB Basic Algorithm Extract the VPN from the VA and check if the TLB holds it. If yes, then we have a TLB hit. We can now extract the PFN from the relevant TLB entry, concatenate that onto the offset from the original VA, and form the PA we want. If protection checks do not fail, then we can perform the operation correctly. If no, then we have a TLB miss. Then we have to walk the page table, the memory, or even copy the data from disk. And then update the TLB. Note that page fault can be triggered by both TLB hit and TLB miss. The TLB hit rate will greatly affect the performance, so we need to find the sweet spot for the TLB size. Also, if the TLB size is too large, walk the TLB will consume more time. And also, if you want a fast cache, it has to be small. This is the physical limit. TLB also improves the performance due to spatial locality even if it’s the first time to visit that memory space. Who handle the TLB miss? Could be hardware or software. Hardware: the hardware has to know where exactly the page tables are located in memory(page-table base register), as well as the format. On a miss, the hardware will walk through the page table, find the right PTE and extract the desired translation, update the TLB with the translation, and retry the instruction. For example, the Intel x86 architecture uses a fixed multi-level page table, and the current table is pointed to by the CR3 register. Software: More modern architecture such as RISC, has a software-managed TLB. On a TLB miss, the hardware will raise an exception to pause the current instruction stream. Then it traps into kernel mode and jumps to a trap handler dealing with TLB misses. The code will look up the page table and update the TLB and return from the trap. The hardware will retry the instruction. Notice that the return-from-trap behaves differently from that in a system call. In the system call case, it should resume execution at the instruction after the trap into the OS. However, in this case, the hardware must resume execution at the instruction that cause the trap and retry it so that it will has a TLB hit this time. We need be aware of the OS need to save different PC in these two cases. Also, when running the TLB miss-handling code, the OS need to be extra careful not to cause an infinite chain of TLB misses to occur. The solution would be to keep TLB miss handlers in physical memory that are unmapped and not subject to address translation. Or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself, these wired translations always hit in the TLB. Aside: TLB valid bit != page table valid bit TLB valid bit indicates whether a TLB entry has a valid translation in it. The page table valid bit indicates the page has not been allocated by the process, and should not be accessed by a correctly working program. The TLB valid bit is quite useful when performing a context switch too. By setting all TLB entries to invalid, the system can ensure that the about-to-run process does not accidentally use a virtual-to-physical translation from a previous process. IMPORTANT: The dirty bit in PTE indicates where the PTE is modified rather than whether the memory location is modified, which is indicated by the dirty bit of that memory location. Based on this fact, we can know that the content of PTE in TLB does not need to be the same as the one in the page table since whether the memory is modified is indicated by the memory itself rather than the PTE. In addition, TLB PTE doesn’t need to be the as same as the one in the page table because the OS can keep just TLB updated rather than both. Think of about this, if we have the PTE in TLB, when we want to go to that PA, we always get a TLB hit and will never walk the page table. The additional work would be to update the PTE in th","date":"2021-12-12","objectID":"/os_reading_note/:1:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#tlb-basic-algorithm"},{"categories":null,"content":"TLBs Translation-lookaside Buffer(TLB) A TLB is a part of memory-management unit(MMU), and is a hardware cache. TLB Basic Algorithm Extract the VPN from the VA and check if the TLB holds it. If yes, then we have a TLB hit. We can now extract the PFN from the relevant TLB entry, concatenate that onto the offset from the original VA, and form the PA we want. If protection checks do not fail, then we can perform the operation correctly. If no, then we have a TLB miss. Then we have to walk the page table, the memory, or even copy the data from disk. And then update the TLB. Note that page fault can be triggered by both TLB hit and TLB miss. The TLB hit rate will greatly affect the performance, so we need to find the sweet spot for the TLB size. Also, if the TLB size is too large, walk the TLB will consume more time. And also, if you want a fast cache, it has to be small. This is the physical limit. TLB also improves the performance due to spatial locality even if it’s the first time to visit that memory space. Who handle the TLB miss? Could be hardware or software. Hardware: the hardware has to know where exactly the page tables are located in memory(page-table base register), as well as the format. On a miss, the hardware will walk through the page table, find the right PTE and extract the desired translation, update the TLB with the translation, and retry the instruction. For example, the Intel x86 architecture uses a fixed multi-level page table, and the current table is pointed to by the CR3 register. Software: More modern architecture such as RISC, has a software-managed TLB. On a TLB miss, the hardware will raise an exception to pause the current instruction stream. Then it traps into kernel mode and jumps to a trap handler dealing with TLB misses. The code will look up the page table and update the TLB and return from the trap. The hardware will retry the instruction. Notice that the return-from-trap behaves differently from that in a system call. In the system call case, it should resume execution at the instruction after the trap into the OS. However, in this case, the hardware must resume execution at the instruction that cause the trap and retry it so that it will has a TLB hit this time. We need be aware of the OS need to save different PC in these two cases. Also, when running the TLB miss-handling code, the OS need to be extra careful not to cause an infinite chain of TLB misses to occur. The solution would be to keep TLB miss handlers in physical memory that are unmapped and not subject to address translation. Or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself, these wired translations always hit in the TLB. Aside: TLB valid bit != page table valid bit TLB valid bit indicates whether a TLB entry has a valid translation in it. The page table valid bit indicates the page has not been allocated by the process, and should not be accessed by a correctly working program. The TLB valid bit is quite useful when performing a context switch too. By setting all TLB entries to invalid, the system can ensure that the about-to-run process does not accidentally use a virtual-to-physical translation from a previous process. IMPORTANT: The dirty bit in PTE indicates where the PTE is modified rather than whether the memory location is modified, which is indicated by the dirty bit of that memory location. Based on this fact, we can know that the content of PTE in TLB does not need to be the same as the one in the page table since whether the memory is modified is indicated by the memory itself rather than the PTE. In addition, TLB PTE doesn’t need to be the as same as the one in the page table because the OS can keep just TLB updated rather than both. Think of about this, if we have the PTE in TLB, when we want to go to that PA, we always get a TLB hit and will never walk the page table. The additional work would be to update the PTE in th","date":"2021-12-12","objectID":"/os_reading_note/:1:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#who-handle-the-tlb-miss"},{"categories":null,"content":"TLBs Translation-lookaside Buffer(TLB) A TLB is a part of memory-management unit(MMU), and is a hardware cache. TLB Basic Algorithm Extract the VPN from the VA and check if the TLB holds it. If yes, then we have a TLB hit. We can now extract the PFN from the relevant TLB entry, concatenate that onto the offset from the original VA, and form the PA we want. If protection checks do not fail, then we can perform the operation correctly. If no, then we have a TLB miss. Then we have to walk the page table, the memory, or even copy the data from disk. And then update the TLB. Note that page fault can be triggered by both TLB hit and TLB miss. The TLB hit rate will greatly affect the performance, so we need to find the sweet spot for the TLB size. Also, if the TLB size is too large, walk the TLB will consume more time. And also, if you want a fast cache, it has to be small. This is the physical limit. TLB also improves the performance due to spatial locality even if it’s the first time to visit that memory space. Who handle the TLB miss? Could be hardware or software. Hardware: the hardware has to know where exactly the page tables are located in memory(page-table base register), as well as the format. On a miss, the hardware will walk through the page table, find the right PTE and extract the desired translation, update the TLB with the translation, and retry the instruction. For example, the Intel x86 architecture uses a fixed multi-level page table, and the current table is pointed to by the CR3 register. Software: More modern architecture such as RISC, has a software-managed TLB. On a TLB miss, the hardware will raise an exception to pause the current instruction stream. Then it traps into kernel mode and jumps to a trap handler dealing with TLB misses. The code will look up the page table and update the TLB and return from the trap. The hardware will retry the instruction. Notice that the return-from-trap behaves differently from that in a system call. In the system call case, it should resume execution at the instruction after the trap into the OS. However, in this case, the hardware must resume execution at the instruction that cause the trap and retry it so that it will has a TLB hit this time. We need be aware of the OS need to save different PC in these two cases. Also, when running the TLB miss-handling code, the OS need to be extra careful not to cause an infinite chain of TLB misses to occur. The solution would be to keep TLB miss handlers in physical memory that are unmapped and not subject to address translation. Or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself, these wired translations always hit in the TLB. Aside: TLB valid bit != page table valid bit TLB valid bit indicates whether a TLB entry has a valid translation in it. The page table valid bit indicates the page has not been allocated by the process, and should not be accessed by a correctly working program. The TLB valid bit is quite useful when performing a context switch too. By setting all TLB entries to invalid, the system can ensure that the about-to-run process does not accidentally use a virtual-to-physical translation from a previous process. IMPORTANT: The dirty bit in PTE indicates where the PTE is modified rather than whether the memory location is modified, which is indicated by the dirty bit of that memory location. Based on this fact, we can know that the content of PTE in TLB does not need to be the same as the one in the page table since whether the memory is modified is indicated by the memory itself rather than the PTE. In addition, TLB PTE doesn’t need to be the as same as the one in the page table because the OS can keep just TLB updated rather than both. Think of about this, if we have the PTE in TLB, when we want to go to that PA, we always get a TLB hit and will never walk the page table. The additional work would be to update the PTE in th","date":"2021-12-12","objectID":"/os_reading_note/:1:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#aside-tlb-valid-bit--page-table-valid-bit"},{"categories":null,"content":"TLBs Translation-lookaside Buffer(TLB) A TLB is a part of memory-management unit(MMU), and is a hardware cache. TLB Basic Algorithm Extract the VPN from the VA and check if the TLB holds it. If yes, then we have a TLB hit. We can now extract the PFN from the relevant TLB entry, concatenate that onto the offset from the original VA, and form the PA we want. If protection checks do not fail, then we can perform the operation correctly. If no, then we have a TLB miss. Then we have to walk the page table, the memory, or even copy the data from disk. And then update the TLB. Note that page fault can be triggered by both TLB hit and TLB miss. The TLB hit rate will greatly affect the performance, so we need to find the sweet spot for the TLB size. Also, if the TLB size is too large, walk the TLB will consume more time. And also, if you want a fast cache, it has to be small. This is the physical limit. TLB also improves the performance due to spatial locality even if it’s the first time to visit that memory space. Who handle the TLB miss? Could be hardware or software. Hardware: the hardware has to know where exactly the page tables are located in memory(page-table base register), as well as the format. On a miss, the hardware will walk through the page table, find the right PTE and extract the desired translation, update the TLB with the translation, and retry the instruction. For example, the Intel x86 architecture uses a fixed multi-level page table, and the current table is pointed to by the CR3 register. Software: More modern architecture such as RISC, has a software-managed TLB. On a TLB miss, the hardware will raise an exception to pause the current instruction stream. Then it traps into kernel mode and jumps to a trap handler dealing with TLB misses. The code will look up the page table and update the TLB and return from the trap. The hardware will retry the instruction. Notice that the return-from-trap behaves differently from that in a system call. In the system call case, it should resume execution at the instruction after the trap into the OS. However, in this case, the hardware must resume execution at the instruction that cause the trap and retry it so that it will has a TLB hit this time. We need be aware of the OS need to save different PC in these two cases. Also, when running the TLB miss-handling code, the OS need to be extra careful not to cause an infinite chain of TLB misses to occur. The solution would be to keep TLB miss handlers in physical memory that are unmapped and not subject to address translation. Or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself, these wired translations always hit in the TLB. Aside: TLB valid bit != page table valid bit TLB valid bit indicates whether a TLB entry has a valid translation in it. The page table valid bit indicates the page has not been allocated by the process, and should not be accessed by a correctly working program. The TLB valid bit is quite useful when performing a context switch too. By setting all TLB entries to invalid, the system can ensure that the about-to-run process does not accidentally use a virtual-to-physical translation from a previous process. IMPORTANT: The dirty bit in PTE indicates where the PTE is modified rather than whether the memory location is modified, which is indicated by the dirty bit of that memory location. Based on this fact, we can know that the content of PTE in TLB does not need to be the same as the one in the page table since whether the memory is modified is indicated by the memory itself rather than the PTE. In addition, TLB PTE doesn’t need to be the as same as the one in the page table because the OS can keep just TLB updated rather than both. Think of about this, if we have the PTE in TLB, when we want to go to that PA, we always get a TLB hit and will never walk the page table. The additional work would be to update the PTE in th","date":"2021-12-12","objectID":"/os_reading_note/:1:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#important"},{"categories":null,"content":"TLBs Translation-lookaside Buffer(TLB) A TLB is a part of memory-management unit(MMU), and is a hardware cache. TLB Basic Algorithm Extract the VPN from the VA and check if the TLB holds it. If yes, then we have a TLB hit. We can now extract the PFN from the relevant TLB entry, concatenate that onto the offset from the original VA, and form the PA we want. If protection checks do not fail, then we can perform the operation correctly. If no, then we have a TLB miss. Then we have to walk the page table, the memory, or even copy the data from disk. And then update the TLB. Note that page fault can be triggered by both TLB hit and TLB miss. The TLB hit rate will greatly affect the performance, so we need to find the sweet spot for the TLB size. Also, if the TLB size is too large, walk the TLB will consume more time. And also, if you want a fast cache, it has to be small. This is the physical limit. TLB also improves the performance due to spatial locality even if it’s the first time to visit that memory space. Who handle the TLB miss? Could be hardware or software. Hardware: the hardware has to know where exactly the page tables are located in memory(page-table base register), as well as the format. On a miss, the hardware will walk through the page table, find the right PTE and extract the desired translation, update the TLB with the translation, and retry the instruction. For example, the Intel x86 architecture uses a fixed multi-level page table, and the current table is pointed to by the CR3 register. Software: More modern architecture such as RISC, has a software-managed TLB. On a TLB miss, the hardware will raise an exception to pause the current instruction stream. Then it traps into kernel mode and jumps to a trap handler dealing with TLB misses. The code will look up the page table and update the TLB and return from the trap. The hardware will retry the instruction. Notice that the return-from-trap behaves differently from that in a system call. In the system call case, it should resume execution at the instruction after the trap into the OS. However, in this case, the hardware must resume execution at the instruction that cause the trap and retry it so that it will has a TLB hit this time. We need be aware of the OS need to save different PC in these two cases. Also, when running the TLB miss-handling code, the OS need to be extra careful not to cause an infinite chain of TLB misses to occur. The solution would be to keep TLB miss handlers in physical memory that are unmapped and not subject to address translation. Or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself, these wired translations always hit in the TLB. Aside: TLB valid bit != page table valid bit TLB valid bit indicates whether a TLB entry has a valid translation in it. The page table valid bit indicates the page has not been allocated by the process, and should not be accessed by a correctly working program. The TLB valid bit is quite useful when performing a context switch too. By setting all TLB entries to invalid, the system can ensure that the about-to-run process does not accidentally use a virtual-to-physical translation from a previous process. IMPORTANT: The dirty bit in PTE indicates where the PTE is modified rather than whether the memory location is modified, which is indicated by the dirty bit of that memory location. Based on this fact, we can know that the content of PTE in TLB does not need to be the same as the one in the page table since whether the memory is modified is indicated by the memory itself rather than the PTE. In addition, TLB PTE doesn’t need to be the as same as the one in the page table because the OS can keep just TLB updated rather than both. Think of about this, if we have the PTE in TLB, when we want to go to that PA, we always get a TLB hit and will never walk the page table. The additional work would be to update the PTE in th","date":"2021-12-12","objectID":"/os_reading_note/:1:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#my-question-here-is-whether-the-same-tlb-is-shared-by-processes-or-even-cores"},{"categories":null,"content":"TLBs Translation-lookaside Buffer(TLB) A TLB is a part of memory-management unit(MMU), and is a hardware cache. TLB Basic Algorithm Extract the VPN from the VA and check if the TLB holds it. If yes, then we have a TLB hit. We can now extract the PFN from the relevant TLB entry, concatenate that onto the offset from the original VA, and form the PA we want. If protection checks do not fail, then we can perform the operation correctly. If no, then we have a TLB miss. Then we have to walk the page table, the memory, or even copy the data from disk. And then update the TLB. Note that page fault can be triggered by both TLB hit and TLB miss. The TLB hit rate will greatly affect the performance, so we need to find the sweet spot for the TLB size. Also, if the TLB size is too large, walk the TLB will consume more time. And also, if you want a fast cache, it has to be small. This is the physical limit. TLB also improves the performance due to spatial locality even if it’s the first time to visit that memory space. Who handle the TLB miss? Could be hardware or software. Hardware: the hardware has to know where exactly the page tables are located in memory(page-table base register), as well as the format. On a miss, the hardware will walk through the page table, find the right PTE and extract the desired translation, update the TLB with the translation, and retry the instruction. For example, the Intel x86 architecture uses a fixed multi-level page table, and the current table is pointed to by the CR3 register. Software: More modern architecture such as RISC, has a software-managed TLB. On a TLB miss, the hardware will raise an exception to pause the current instruction stream. Then it traps into kernel mode and jumps to a trap handler dealing with TLB misses. The code will look up the page table and update the TLB and return from the trap. The hardware will retry the instruction. Notice that the return-from-trap behaves differently from that in a system call. In the system call case, it should resume execution at the instruction after the trap into the OS. However, in this case, the hardware must resume execution at the instruction that cause the trap and retry it so that it will has a TLB hit this time. We need be aware of the OS need to save different PC in these two cases. Also, when running the TLB miss-handling code, the OS need to be extra careful not to cause an infinite chain of TLB misses to occur. The solution would be to keep TLB miss handlers in physical memory that are unmapped and not subject to address translation. Or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself, these wired translations always hit in the TLB. Aside: TLB valid bit != page table valid bit TLB valid bit indicates whether a TLB entry has a valid translation in it. The page table valid bit indicates the page has not been allocated by the process, and should not be accessed by a correctly working program. The TLB valid bit is quite useful when performing a context switch too. By setting all TLB entries to invalid, the system can ensure that the about-to-run process does not accidentally use a virtual-to-physical translation from a previous process. IMPORTANT: The dirty bit in PTE indicates where the PTE is modified rather than whether the memory location is modified, which is indicated by the dirty bit of that memory location. Based on this fact, we can know that the content of PTE in TLB does not need to be the same as the one in the page table since whether the memory is modified is indicated by the memory itself rather than the PTE. In addition, TLB PTE doesn’t need to be the as same as the one in the page table because the OS can keep just TLB updated rather than both. Think of about this, if we have the PTE in TLB, when we want to go to that PA, we always get a TLB hit and will never walk the page table. The additional work would be to update the PTE in th","date":"2021-12-12","objectID":"/os_reading_note/:1:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#whats-in-the-tlb"},{"categories":null,"content":"TLBs Translation-lookaside Buffer(TLB) A TLB is a part of memory-management unit(MMU), and is a hardware cache. TLB Basic Algorithm Extract the VPN from the VA and check if the TLB holds it. If yes, then we have a TLB hit. We can now extract the PFN from the relevant TLB entry, concatenate that onto the offset from the original VA, and form the PA we want. If protection checks do not fail, then we can perform the operation correctly. If no, then we have a TLB miss. Then we have to walk the page table, the memory, or even copy the data from disk. And then update the TLB. Note that page fault can be triggered by both TLB hit and TLB miss. The TLB hit rate will greatly affect the performance, so we need to find the sweet spot for the TLB size. Also, if the TLB size is too large, walk the TLB will consume more time. And also, if you want a fast cache, it has to be small. This is the physical limit. TLB also improves the performance due to spatial locality even if it’s the first time to visit that memory space. Who handle the TLB miss? Could be hardware or software. Hardware: the hardware has to know where exactly the page tables are located in memory(page-table base register), as well as the format. On a miss, the hardware will walk through the page table, find the right PTE and extract the desired translation, update the TLB with the translation, and retry the instruction. For example, the Intel x86 architecture uses a fixed multi-level page table, and the current table is pointed to by the CR3 register. Software: More modern architecture such as RISC, has a software-managed TLB. On a TLB miss, the hardware will raise an exception to pause the current instruction stream. Then it traps into kernel mode and jumps to a trap handler dealing with TLB misses. The code will look up the page table and update the TLB and return from the trap. The hardware will retry the instruction. Notice that the return-from-trap behaves differently from that in a system call. In the system call case, it should resume execution at the instruction after the trap into the OS. However, in this case, the hardware must resume execution at the instruction that cause the trap and retry it so that it will has a TLB hit this time. We need be aware of the OS need to save different PC in these two cases. Also, when running the TLB miss-handling code, the OS need to be extra careful not to cause an infinite chain of TLB misses to occur. The solution would be to keep TLB miss handlers in physical memory that are unmapped and not subject to address translation. Or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself, these wired translations always hit in the TLB. Aside: TLB valid bit != page table valid bit TLB valid bit indicates whether a TLB entry has a valid translation in it. The page table valid bit indicates the page has not been allocated by the process, and should not be accessed by a correctly working program. The TLB valid bit is quite useful when performing a context switch too. By setting all TLB entries to invalid, the system can ensure that the about-to-run process does not accidentally use a virtual-to-physical translation from a previous process. IMPORTANT: The dirty bit in PTE indicates where the PTE is modified rather than whether the memory location is modified, which is indicated by the dirty bit of that memory location. Based on this fact, we can know that the content of PTE in TLB does not need to be the same as the one in the page table since whether the memory is modified is indicated by the memory itself rather than the PTE. In addition, TLB PTE doesn’t need to be the as same as the one in the page table because the OS can keep just TLB updated rather than both. Think of about this, if we have the PTE in TLB, when we want to go to that PA, we always get a TLB hit and will never walk the page table. The additional work would be to update the PTE in th","date":"2021-12-12","objectID":"/os_reading_note/:1:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#tlb-issue-context-switches"},{"categories":null,"content":"TLBs Translation-lookaside Buffer(TLB) A TLB is a part of memory-management unit(MMU), and is a hardware cache. TLB Basic Algorithm Extract the VPN from the VA and check if the TLB holds it. If yes, then we have a TLB hit. We can now extract the PFN from the relevant TLB entry, concatenate that onto the offset from the original VA, and form the PA we want. If protection checks do not fail, then we can perform the operation correctly. If no, then we have a TLB miss. Then we have to walk the page table, the memory, or even copy the data from disk. And then update the TLB. Note that page fault can be triggered by both TLB hit and TLB miss. The TLB hit rate will greatly affect the performance, so we need to find the sweet spot for the TLB size. Also, if the TLB size is too large, walk the TLB will consume more time. And also, if you want a fast cache, it has to be small. This is the physical limit. TLB also improves the performance due to spatial locality even if it’s the first time to visit that memory space. Who handle the TLB miss? Could be hardware or software. Hardware: the hardware has to know where exactly the page tables are located in memory(page-table base register), as well as the format. On a miss, the hardware will walk through the page table, find the right PTE and extract the desired translation, update the TLB with the translation, and retry the instruction. For example, the Intel x86 architecture uses a fixed multi-level page table, and the current table is pointed to by the CR3 register. Software: More modern architecture such as RISC, has a software-managed TLB. On a TLB miss, the hardware will raise an exception to pause the current instruction stream. Then it traps into kernel mode and jumps to a trap handler dealing with TLB misses. The code will look up the page table and update the TLB and return from the trap. The hardware will retry the instruction. Notice that the return-from-trap behaves differently from that in a system call. In the system call case, it should resume execution at the instruction after the trap into the OS. However, in this case, the hardware must resume execution at the instruction that cause the trap and retry it so that it will has a TLB hit this time. We need be aware of the OS need to save different PC in these two cases. Also, when running the TLB miss-handling code, the OS need to be extra careful not to cause an infinite chain of TLB misses to occur. The solution would be to keep TLB miss handlers in physical memory that are unmapped and not subject to address translation. Or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself, these wired translations always hit in the TLB. Aside: TLB valid bit != page table valid bit TLB valid bit indicates whether a TLB entry has a valid translation in it. The page table valid bit indicates the page has not been allocated by the process, and should not be accessed by a correctly working program. The TLB valid bit is quite useful when performing a context switch too. By setting all TLB entries to invalid, the system can ensure that the about-to-run process does not accidentally use a virtual-to-physical translation from a previous process. IMPORTANT: The dirty bit in PTE indicates where the PTE is modified rather than whether the memory location is modified, which is indicated by the dirty bit of that memory location. Based on this fact, we can know that the content of PTE in TLB does not need to be the same as the one in the page table since whether the memory is modified is indicated by the memory itself rather than the PTE. In addition, TLB PTE doesn’t need to be the as same as the one in the page table because the OS can keep just TLB updated rather than both. Think of about this, if we have the PTE in TLB, when we want to go to that PA, we always get a TLB hit and will never walk the page table. The additional work would be to update the PTE in th","date":"2021-12-12","objectID":"/os_reading_note/:1:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#issue-replacement-policy"},{"categories":null,"content":"TLBs Translation-lookaside Buffer(TLB) A TLB is a part of memory-management unit(MMU), and is a hardware cache. TLB Basic Algorithm Extract the VPN from the VA and check if the TLB holds it. If yes, then we have a TLB hit. We can now extract the PFN from the relevant TLB entry, concatenate that onto the offset from the original VA, and form the PA we want. If protection checks do not fail, then we can perform the operation correctly. If no, then we have a TLB miss. Then we have to walk the page table, the memory, or even copy the data from disk. And then update the TLB. Note that page fault can be triggered by both TLB hit and TLB miss. The TLB hit rate will greatly affect the performance, so we need to find the sweet spot for the TLB size. Also, if the TLB size is too large, walk the TLB will consume more time. And also, if you want a fast cache, it has to be small. This is the physical limit. TLB also improves the performance due to spatial locality even if it’s the first time to visit that memory space. Who handle the TLB miss? Could be hardware or software. Hardware: the hardware has to know where exactly the page tables are located in memory(page-table base register), as well as the format. On a miss, the hardware will walk through the page table, find the right PTE and extract the desired translation, update the TLB with the translation, and retry the instruction. For example, the Intel x86 architecture uses a fixed multi-level page table, and the current table is pointed to by the CR3 register. Software: More modern architecture such as RISC, has a software-managed TLB. On a TLB miss, the hardware will raise an exception to pause the current instruction stream. Then it traps into kernel mode and jumps to a trap handler dealing with TLB misses. The code will look up the page table and update the TLB and return from the trap. The hardware will retry the instruction. Notice that the return-from-trap behaves differently from that in a system call. In the system call case, it should resume execution at the instruction after the trap into the OS. However, in this case, the hardware must resume execution at the instruction that cause the trap and retry it so that it will has a TLB hit this time. We need be aware of the OS need to save different PC in these two cases. Also, when running the TLB miss-handling code, the OS need to be extra careful not to cause an infinite chain of TLB misses to occur. The solution would be to keep TLB miss handlers in physical memory that are unmapped and not subject to address translation. Or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself, these wired translations always hit in the TLB. Aside: TLB valid bit != page table valid bit TLB valid bit indicates whether a TLB entry has a valid translation in it. The page table valid bit indicates the page has not been allocated by the process, and should not be accessed by a correctly working program. The TLB valid bit is quite useful when performing a context switch too. By setting all TLB entries to invalid, the system can ensure that the about-to-run process does not accidentally use a virtual-to-physical translation from a previous process. IMPORTANT: The dirty bit in PTE indicates where the PTE is modified rather than whether the memory location is modified, which is indicated by the dirty bit of that memory location. Based on this fact, we can know that the content of PTE in TLB does not need to be the same as the one in the page table since whether the memory is modified is indicated by the memory itself rather than the PTE. In addition, TLB PTE doesn’t need to be the as same as the one in the page table because the OS can keep just TLB updated rather than both. Think of about this, if we have the PTE in TLB, when we want to go to that PA, we always get a TLB hit and will never walk the page table. The additional work would be to update the PTE in th","date":"2021-12-12","objectID":"/os_reading_note/:1:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#how-to-design-tlb-replacement-policy"},{"categories":null,"content":"TLBs Translation-lookaside Buffer(TLB) A TLB is a part of memory-management unit(MMU), and is a hardware cache. TLB Basic Algorithm Extract the VPN from the VA and check if the TLB holds it. If yes, then we have a TLB hit. We can now extract the PFN from the relevant TLB entry, concatenate that onto the offset from the original VA, and form the PA we want. If protection checks do not fail, then we can perform the operation correctly. If no, then we have a TLB miss. Then we have to walk the page table, the memory, or even copy the data from disk. And then update the TLB. Note that page fault can be triggered by both TLB hit and TLB miss. The TLB hit rate will greatly affect the performance, so we need to find the sweet spot for the TLB size. Also, if the TLB size is too large, walk the TLB will consume more time. And also, if you want a fast cache, it has to be small. This is the physical limit. TLB also improves the performance due to spatial locality even if it’s the first time to visit that memory space. Who handle the TLB miss? Could be hardware or software. Hardware: the hardware has to know where exactly the page tables are located in memory(page-table base register), as well as the format. On a miss, the hardware will walk through the page table, find the right PTE and extract the desired translation, update the TLB with the translation, and retry the instruction. For example, the Intel x86 architecture uses a fixed multi-level page table, and the current table is pointed to by the CR3 register. Software: More modern architecture such as RISC, has a software-managed TLB. On a TLB miss, the hardware will raise an exception to pause the current instruction stream. Then it traps into kernel mode and jumps to a trap handler dealing with TLB misses. The code will look up the page table and update the TLB and return from the trap. The hardware will retry the instruction. Notice that the return-from-trap behaves differently from that in a system call. In the system call case, it should resume execution at the instruction after the trap into the OS. However, in this case, the hardware must resume execution at the instruction that cause the trap and retry it so that it will has a TLB hit this time. We need be aware of the OS need to save different PC in these two cases. Also, when running the TLB miss-handling code, the OS need to be extra careful not to cause an infinite chain of TLB misses to occur. The solution would be to keep TLB miss handlers in physical memory that are unmapped and not subject to address translation. Or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself, these wired translations always hit in the TLB. Aside: TLB valid bit != page table valid bit TLB valid bit indicates whether a TLB entry has a valid translation in it. The page table valid bit indicates the page has not been allocated by the process, and should not be accessed by a correctly working program. The TLB valid bit is quite useful when performing a context switch too. By setting all TLB entries to invalid, the system can ensure that the about-to-run process does not accidentally use a virtual-to-physical translation from a previous process. IMPORTANT: The dirty bit in PTE indicates where the PTE is modified rather than whether the memory location is modified, which is indicated by the dirty bit of that memory location. Based on this fact, we can know that the content of PTE in TLB does not need to be the same as the one in the page table since whether the memory is modified is indicated by the memory itself rather than the PTE. In addition, TLB PTE doesn’t need to be the as same as the one in the page table because the OS can keep just TLB updated rather than both. Think of about this, if we have the PTE in TLB, when we want to go to that PA, we always get a TLB hit and will never walk the page table. The additional work would be to update the PTE in th","date":"2021-12-12","objectID":"/os_reading_note/:1:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#a-real-tlb-entry"},{"categories":null,"content":"TLBs Translation-lookaside Buffer(TLB) A TLB is a part of memory-management unit(MMU), and is a hardware cache. TLB Basic Algorithm Extract the VPN from the VA and check if the TLB holds it. If yes, then we have a TLB hit. We can now extract the PFN from the relevant TLB entry, concatenate that onto the offset from the original VA, and form the PA we want. If protection checks do not fail, then we can perform the operation correctly. If no, then we have a TLB miss. Then we have to walk the page table, the memory, or even copy the data from disk. And then update the TLB. Note that page fault can be triggered by both TLB hit and TLB miss. The TLB hit rate will greatly affect the performance, so we need to find the sweet spot for the TLB size. Also, if the TLB size is too large, walk the TLB will consume more time. And also, if you want a fast cache, it has to be small. This is the physical limit. TLB also improves the performance due to spatial locality even if it’s the first time to visit that memory space. Who handle the TLB miss? Could be hardware or software. Hardware: the hardware has to know where exactly the page tables are located in memory(page-table base register), as well as the format. On a miss, the hardware will walk through the page table, find the right PTE and extract the desired translation, update the TLB with the translation, and retry the instruction. For example, the Intel x86 architecture uses a fixed multi-level page table, and the current table is pointed to by the CR3 register. Software: More modern architecture such as RISC, has a software-managed TLB. On a TLB miss, the hardware will raise an exception to pause the current instruction stream. Then it traps into kernel mode and jumps to a trap handler dealing with TLB misses. The code will look up the page table and update the TLB and return from the trap. The hardware will retry the instruction. Notice that the return-from-trap behaves differently from that in a system call. In the system call case, it should resume execution at the instruction after the trap into the OS. However, in this case, the hardware must resume execution at the instruction that cause the trap and retry it so that it will has a TLB hit this time. We need be aware of the OS need to save different PC in these two cases. Also, when running the TLB miss-handling code, the OS need to be extra careful not to cause an infinite chain of TLB misses to occur. The solution would be to keep TLB miss handlers in physical memory that are unmapped and not subject to address translation. Or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself, these wired translations always hit in the TLB. Aside: TLB valid bit != page table valid bit TLB valid bit indicates whether a TLB entry has a valid translation in it. The page table valid bit indicates the page has not been allocated by the process, and should not be accessed by a correctly working program. The TLB valid bit is quite useful when performing a context switch too. By setting all TLB entries to invalid, the system can ensure that the about-to-run process does not accidentally use a virtual-to-physical translation from a previous process. IMPORTANT: The dirty bit in PTE indicates where the PTE is modified rather than whether the memory location is modified, which is indicated by the dirty bit of that memory location. Based on this fact, we can know that the content of PTE in TLB does not need to be the same as the one in the page table since whether the memory is modified is indicated by the memory itself rather than the PTE. In addition, TLB PTE doesn’t need to be the as same as the one in the page table because the OS can keep just TLB updated rather than both. Think of about this, if we have the PTE in TLB, when we want to go to that PA, we always get a TLB hit and will never walk the page table. The additional work would be to update the PTE in th","date":"2021-12-12","objectID":"/os_reading_note/:1:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#ram-isnt-always-ram"},{"categories":null,"content":"Smaller Tables Page tables are too big Linear page tables are pretty big. Assuming a 32-bit address space, with 4KB pages and a 4-byte page-table size. There are roughly one million virtual pages in it, which will occupy 4MB memory. Simple Solution: Bigger Pages Side note: many archs now support multiple page sizes. Bigger pages may cause internal fragmentation. Hybrid Approach: Paging and Segments Most of the page table is unused, full of invalid entries. Instead of havin a single page table for the entris, why not one per logical segment. Base tells us the address of each segment. It holds the pysical address of the page table of that segment. Bound tells us the size of each segment. Thus the VA looks like: Seg | VPN | Offset The hardware use segment bit(SN) to determine which base and bound pair to use. IDEA: When you have two good and seemly opposite ideas, you should always see if you can combine them into hybrid that manage to achieve the best of both worlds. So if the code segment just have three entries, then the bound will be set to 3. This greatly reduces the waste of space for page tables. However, the problem is that if we have a large but sparsely-used heap. This will cause the external fragmentation again. Multi-level Page Tables It turns the linear page table like a tree. We use page directory to find the valid page tables. ![Multi-level Page Tables](img/Multi-level Page Tables.png) The page directory, in a simple two-level table, contains of a number of PDE(page directory entry). A PDE has a valid bit and a PFN, similar to PTE. However, the valid bit in PDE indcates whether at least one of the PTEs on the page pointed to by this PDE is valid. If the PDE is not valid, then the rest of the PDE is not defined. (The valid bit is the first bit in PDE) Advantage: Better supports sparse address spaces. If carefully constructed, each portion of the page table fits neatly within a page, so it’s easier to manage memory to avoid internal fragmentation. Disadvantage: If we have a TLB miss, two loads from memory will be required to get the right translation, one for the page directory and one for the PTE itself. So there is a time-space trade-off. Smaller cache is faster but not for free. The TLB miss suffers from higher cost with this smaller table. ![Page Directory and Page of PT](img/Page Directory and Page of PT.png) More Than Two Levels ![More than two levels](img/More than two levels.png) IMPORTANT: The PDT’s PA will be stored in a register, but it does not mean that the addr of PDT has to be in a fixed PA, it just has to be memorized. Inverted Page Tables Inverted page tables are even more space saving. We keep a single page table that has an entry for each physical page of the system. The entry tells use which prcess is using this page, and which virtual page of that process maps to this physical page. ","date":"2021-12-12","objectID":"/os_reading_note/:1:4","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#smaller-tables"},{"categories":null,"content":"Smaller Tables Page tables are too big Linear page tables are pretty big. Assuming a 32-bit address space, with 4KB pages and a 4-byte page-table size. There are roughly one million virtual pages in it, which will occupy 4MB memory. Simple Solution: Bigger Pages Side note: many archs now support multiple page sizes. Bigger pages may cause internal fragmentation. Hybrid Approach: Paging and Segments Most of the page table is unused, full of invalid entries. Instead of havin a single page table for the entris, why not one per logical segment. Base tells us the address of each segment. It holds the pysical address of the page table of that segment. Bound tells us the size of each segment. Thus the VA looks like: Seg | VPN | Offset The hardware use segment bit(SN) to determine which base and bound pair to use. IDEA: When you have two good and seemly opposite ideas, you should always see if you can combine them into hybrid that manage to achieve the best of both worlds. So if the code segment just have three entries, then the bound will be set to 3. This greatly reduces the waste of space for page tables. However, the problem is that if we have a large but sparsely-used heap. This will cause the external fragmentation again. Multi-level Page Tables It turns the linear page table like a tree. We use page directory to find the valid page tables. ![Multi-level Page Tables](img/Multi-level Page Tables.png) The page directory, in a simple two-level table, contains of a number of PDE(page directory entry). A PDE has a valid bit and a PFN, similar to PTE. However, the valid bit in PDE indcates whether at least one of the PTEs on the page pointed to by this PDE is valid. If the PDE is not valid, then the rest of the PDE is not defined. (The valid bit is the first bit in PDE) Advantage: Better supports sparse address spaces. If carefully constructed, each portion of the page table fits neatly within a page, so it’s easier to manage memory to avoid internal fragmentation. Disadvantage: If we have a TLB miss, two loads from memory will be required to get the right translation, one for the page directory and one for the PTE itself. So there is a time-space trade-off. Smaller cache is faster but not for free. The TLB miss suffers from higher cost with this smaller table. ![Page Directory and Page of PT](img/Page Directory and Page of PT.png) More Than Two Levels ![More than two levels](img/More than two levels.png) IMPORTANT: The PDT’s PA will be stored in a register, but it does not mean that the addr of PDT has to be in a fixed PA, it just has to be memorized. Inverted Page Tables Inverted page tables are even more space saving. We keep a single page table that has an entry for each physical page of the system. The entry tells use which prcess is using this page, and which virtual page of that process maps to this physical page. ","date":"2021-12-12","objectID":"/os_reading_note/:1:4","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#page-tables-are-too-big"},{"categories":null,"content":"Smaller Tables Page tables are too big Linear page tables are pretty big. Assuming a 32-bit address space, with 4KB pages and a 4-byte page-table size. There are roughly one million virtual pages in it, which will occupy 4MB memory. Simple Solution: Bigger Pages Side note: many archs now support multiple page sizes. Bigger pages may cause internal fragmentation. Hybrid Approach: Paging and Segments Most of the page table is unused, full of invalid entries. Instead of havin a single page table for the entris, why not one per logical segment. Base tells us the address of each segment. It holds the pysical address of the page table of that segment. Bound tells us the size of each segment. Thus the VA looks like: Seg | VPN | Offset The hardware use segment bit(SN) to determine which base and bound pair to use. IDEA: When you have two good and seemly opposite ideas, you should always see if you can combine them into hybrid that manage to achieve the best of both worlds. So if the code segment just have three entries, then the bound will be set to 3. This greatly reduces the waste of space for page tables. However, the problem is that if we have a large but sparsely-used heap. This will cause the external fragmentation again. Multi-level Page Tables It turns the linear page table like a tree. We use page directory to find the valid page tables. ![Multi-level Page Tables](img/Multi-level Page Tables.png) The page directory, in a simple two-level table, contains of a number of PDE(page directory entry). A PDE has a valid bit and a PFN, similar to PTE. However, the valid bit in PDE indcates whether at least one of the PTEs on the page pointed to by this PDE is valid. If the PDE is not valid, then the rest of the PDE is not defined. (The valid bit is the first bit in PDE) Advantage: Better supports sparse address spaces. If carefully constructed, each portion of the page table fits neatly within a page, so it’s easier to manage memory to avoid internal fragmentation. Disadvantage: If we have a TLB miss, two loads from memory will be required to get the right translation, one for the page directory and one for the PTE itself. So there is a time-space trade-off. Smaller cache is faster but not for free. The TLB miss suffers from higher cost with this smaller table. ![Page Directory and Page of PT](img/Page Directory and Page of PT.png) More Than Two Levels ![More than two levels](img/More than two levels.png) IMPORTANT: The PDT’s PA will be stored in a register, but it does not mean that the addr of PDT has to be in a fixed PA, it just has to be memorized. Inverted Page Tables Inverted page tables are even more space saving. We keep a single page table that has an entry for each physical page of the system. The entry tells use which prcess is using this page, and which virtual page of that process maps to this physical page. ","date":"2021-12-12","objectID":"/os_reading_note/:1:4","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#simple-solution-bigger-pages"},{"categories":null,"content":"Smaller Tables Page tables are too big Linear page tables are pretty big. Assuming a 32-bit address space, with 4KB pages and a 4-byte page-table size. There are roughly one million virtual pages in it, which will occupy 4MB memory. Simple Solution: Bigger Pages Side note: many archs now support multiple page sizes. Bigger pages may cause internal fragmentation. Hybrid Approach: Paging and Segments Most of the page table is unused, full of invalid entries. Instead of havin a single page table for the entris, why not one per logical segment. Base tells us the address of each segment. It holds the pysical address of the page table of that segment. Bound tells us the size of each segment. Thus the VA looks like: Seg | VPN | Offset The hardware use segment bit(SN) to determine which base and bound pair to use. IDEA: When you have two good and seemly opposite ideas, you should always see if you can combine them into hybrid that manage to achieve the best of both worlds. So if the code segment just have three entries, then the bound will be set to 3. This greatly reduces the waste of space for page tables. However, the problem is that if we have a large but sparsely-used heap. This will cause the external fragmentation again. Multi-level Page Tables It turns the linear page table like a tree. We use page directory to find the valid page tables. ![Multi-level Page Tables](img/Multi-level Page Tables.png) The page directory, in a simple two-level table, contains of a number of PDE(page directory entry). A PDE has a valid bit and a PFN, similar to PTE. However, the valid bit in PDE indcates whether at least one of the PTEs on the page pointed to by this PDE is valid. If the PDE is not valid, then the rest of the PDE is not defined. (The valid bit is the first bit in PDE) Advantage: Better supports sparse address spaces. If carefully constructed, each portion of the page table fits neatly within a page, so it’s easier to manage memory to avoid internal fragmentation. Disadvantage: If we have a TLB miss, two loads from memory will be required to get the right translation, one for the page directory and one for the PTE itself. So there is a time-space trade-off. Smaller cache is faster but not for free. The TLB miss suffers from higher cost with this smaller table. ![Page Directory and Page of PT](img/Page Directory and Page of PT.png) More Than Two Levels ![More than two levels](img/More than two levels.png) IMPORTANT: The PDT’s PA will be stored in a register, but it does not mean that the addr of PDT has to be in a fixed PA, it just has to be memorized. Inverted Page Tables Inverted page tables are even more space saving. We keep a single page table that has an entry for each physical page of the system. The entry tells use which prcess is using this page, and which virtual page of that process maps to this physical page. ","date":"2021-12-12","objectID":"/os_reading_note/:1:4","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#side-note-many-archs-now-support-multiple-page-sizes"},{"categories":null,"content":"Smaller Tables Page tables are too big Linear page tables are pretty big. Assuming a 32-bit address space, with 4KB pages and a 4-byte page-table size. There are roughly one million virtual pages in it, which will occupy 4MB memory. Simple Solution: Bigger Pages Side note: many archs now support multiple page sizes. Bigger pages may cause internal fragmentation. Hybrid Approach: Paging and Segments Most of the page table is unused, full of invalid entries. Instead of havin a single page table for the entris, why not one per logical segment. Base tells us the address of each segment. It holds the pysical address of the page table of that segment. Bound tells us the size of each segment. Thus the VA looks like: Seg | VPN | Offset The hardware use segment bit(SN) to determine which base and bound pair to use. IDEA: When you have two good and seemly opposite ideas, you should always see if you can combine them into hybrid that manage to achieve the best of both worlds. So if the code segment just have three entries, then the bound will be set to 3. This greatly reduces the waste of space for page tables. However, the problem is that if we have a large but sparsely-used heap. This will cause the external fragmentation again. Multi-level Page Tables It turns the linear page table like a tree. We use page directory to find the valid page tables. ![Multi-level Page Tables](img/Multi-level Page Tables.png) The page directory, in a simple two-level table, contains of a number of PDE(page directory entry). A PDE has a valid bit and a PFN, similar to PTE. However, the valid bit in PDE indcates whether at least one of the PTEs on the page pointed to by this PDE is valid. If the PDE is not valid, then the rest of the PDE is not defined. (The valid bit is the first bit in PDE) Advantage: Better supports sparse address spaces. If carefully constructed, each portion of the page table fits neatly within a page, so it’s easier to manage memory to avoid internal fragmentation. Disadvantage: If we have a TLB miss, two loads from memory will be required to get the right translation, one for the page directory and one for the PTE itself. So there is a time-space trade-off. Smaller cache is faster but not for free. The TLB miss suffers from higher cost with this smaller table. ![Page Directory and Page of PT](img/Page Directory and Page of PT.png) More Than Two Levels ![More than two levels](img/More than two levels.png) IMPORTANT: The PDT’s PA will be stored in a register, but it does not mean that the addr of PDT has to be in a fixed PA, it just has to be memorized. Inverted Page Tables Inverted page tables are even more space saving. We keep a single page table that has an entry for each physical page of the system. The entry tells use which prcess is using this page, and which virtual page of that process maps to this physical page. ","date":"2021-12-12","objectID":"/os_reading_note/:1:4","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#hybrid-approach-paging-and-segments"},{"categories":null,"content":"Smaller Tables Page tables are too big Linear page tables are pretty big. Assuming a 32-bit address space, with 4KB pages and a 4-byte page-table size. There are roughly one million virtual pages in it, which will occupy 4MB memory. Simple Solution: Bigger Pages Side note: many archs now support multiple page sizes. Bigger pages may cause internal fragmentation. Hybrid Approach: Paging and Segments Most of the page table is unused, full of invalid entries. Instead of havin a single page table for the entris, why not one per logical segment. Base tells us the address of each segment. It holds the pysical address of the page table of that segment. Bound tells us the size of each segment. Thus the VA looks like: Seg | VPN | Offset The hardware use segment bit(SN) to determine which base and bound pair to use. IDEA: When you have two good and seemly opposite ideas, you should always see if you can combine them into hybrid that manage to achieve the best of both worlds. So if the code segment just have three entries, then the bound will be set to 3. This greatly reduces the waste of space for page tables. However, the problem is that if we have a large but sparsely-used heap. This will cause the external fragmentation again. Multi-level Page Tables It turns the linear page table like a tree. We use page directory to find the valid page tables. ![Multi-level Page Tables](img/Multi-level Page Tables.png) The page directory, in a simple two-level table, contains of a number of PDE(page directory entry). A PDE has a valid bit and a PFN, similar to PTE. However, the valid bit in PDE indcates whether at least one of the PTEs on the page pointed to by this PDE is valid. If the PDE is not valid, then the rest of the PDE is not defined. (The valid bit is the first bit in PDE) Advantage: Better supports sparse address spaces. If carefully constructed, each portion of the page table fits neatly within a page, so it’s easier to manage memory to avoid internal fragmentation. Disadvantage: If we have a TLB miss, two loads from memory will be required to get the right translation, one for the page directory and one for the PTE itself. So there is a time-space trade-off. Smaller cache is faster but not for free. The TLB miss suffers from higher cost with this smaller table. ![Page Directory and Page of PT](img/Page Directory and Page of PT.png) More Than Two Levels ![More than two levels](img/More than two levels.png) IMPORTANT: The PDT’s PA will be stored in a register, but it does not mean that the addr of PDT has to be in a fixed PA, it just has to be memorized. Inverted Page Tables Inverted page tables are even more space saving. We keep a single page table that has an entry for each physical page of the system. The entry tells use which prcess is using this page, and which virtual page of that process maps to this physical page. ","date":"2021-12-12","objectID":"/os_reading_note/:1:4","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#idea-when-you-have-two-good-and-seemly-opposite-ideas-you-should-always-see-if-you-can-combine-them-into-hybrid-that-manage-to-achieve-the-best-of-both-worlds"},{"categories":null,"content":"Smaller Tables Page tables are too big Linear page tables are pretty big. Assuming a 32-bit address space, with 4KB pages and a 4-byte page-table size. There are roughly one million virtual pages in it, which will occupy 4MB memory. Simple Solution: Bigger Pages Side note: many archs now support multiple page sizes. Bigger pages may cause internal fragmentation. Hybrid Approach: Paging and Segments Most of the page table is unused, full of invalid entries. Instead of havin a single page table for the entris, why not one per logical segment. Base tells us the address of each segment. It holds the pysical address of the page table of that segment. Bound tells us the size of each segment. Thus the VA looks like: Seg | VPN | Offset The hardware use segment bit(SN) to determine which base and bound pair to use. IDEA: When you have two good and seemly opposite ideas, you should always see if you can combine them into hybrid that manage to achieve the best of both worlds. So if the code segment just have three entries, then the bound will be set to 3. This greatly reduces the waste of space for page tables. However, the problem is that if we have a large but sparsely-used heap. This will cause the external fragmentation again. Multi-level Page Tables It turns the linear page table like a tree. We use page directory to find the valid page tables. ![Multi-level Page Tables](img/Multi-level Page Tables.png) The page directory, in a simple two-level table, contains of a number of PDE(page directory entry). A PDE has a valid bit and a PFN, similar to PTE. However, the valid bit in PDE indcates whether at least one of the PTEs on the page pointed to by this PDE is valid. If the PDE is not valid, then the rest of the PDE is not defined. (The valid bit is the first bit in PDE) Advantage: Better supports sparse address spaces. If carefully constructed, each portion of the page table fits neatly within a page, so it’s easier to manage memory to avoid internal fragmentation. Disadvantage: If we have a TLB miss, two loads from memory will be required to get the right translation, one for the page directory and one for the PTE itself. So there is a time-space trade-off. Smaller cache is faster but not for free. The TLB miss suffers from higher cost with this smaller table. ![Page Directory and Page of PT](img/Page Directory and Page of PT.png) More Than Two Levels ![More than two levels](img/More than two levels.png) IMPORTANT: The PDT’s PA will be stored in a register, but it does not mean that the addr of PDT has to be in a fixed PA, it just has to be memorized. Inverted Page Tables Inverted page tables are even more space saving. We keep a single page table that has an entry for each physical page of the system. The entry tells use which prcess is using this page, and which virtual page of that process maps to this physical page. ","date":"2021-12-12","objectID":"/os_reading_note/:1:4","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#multi-level-page-tables"},{"categories":null,"content":"Smaller Tables Page tables are too big Linear page tables are pretty big. Assuming a 32-bit address space, with 4KB pages and a 4-byte page-table size. There are roughly one million virtual pages in it, which will occupy 4MB memory. Simple Solution: Bigger Pages Side note: many archs now support multiple page sizes. Bigger pages may cause internal fragmentation. Hybrid Approach: Paging and Segments Most of the page table is unused, full of invalid entries. Instead of havin a single page table for the entris, why not one per logical segment. Base tells us the address of each segment. It holds the pysical address of the page table of that segment. Bound tells us the size of each segment. Thus the VA looks like: Seg | VPN | Offset The hardware use segment bit(SN) to determine which base and bound pair to use. IDEA: When you have two good and seemly opposite ideas, you should always see if you can combine them into hybrid that manage to achieve the best of both worlds. So if the code segment just have three entries, then the bound will be set to 3. This greatly reduces the waste of space for page tables. However, the problem is that if we have a large but sparsely-used heap. This will cause the external fragmentation again. Multi-level Page Tables It turns the linear page table like a tree. We use page directory to find the valid page tables. ![Multi-level Page Tables](img/Multi-level Page Tables.png) The page directory, in a simple two-level table, contains of a number of PDE(page directory entry). A PDE has a valid bit and a PFN, similar to PTE. However, the valid bit in PDE indcates whether at least one of the PTEs on the page pointed to by this PDE is valid. If the PDE is not valid, then the rest of the PDE is not defined. (The valid bit is the first bit in PDE) Advantage: Better supports sparse address spaces. If carefully constructed, each portion of the page table fits neatly within a page, so it’s easier to manage memory to avoid internal fragmentation. Disadvantage: If we have a TLB miss, two loads from memory will be required to get the right translation, one for the page directory and one for the PTE itself. So there is a time-space trade-off. Smaller cache is faster but not for free. The TLB miss suffers from higher cost with this smaller table. ![Page Directory and Page of PT](img/Page Directory and Page of PT.png) More Than Two Levels ![More than two levels](img/More than two levels.png) IMPORTANT: The PDT’s PA will be stored in a register, but it does not mean that the addr of PDT has to be in a fixed PA, it just has to be memorized. Inverted Page Tables Inverted page tables are even more space saving. We keep a single page table that has an entry for each physical page of the system. The entry tells use which prcess is using this page, and which virtual page of that process maps to this physical page. ","date":"2021-12-12","objectID":"/os_reading_note/:1:4","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#advantage"},{"categories":null,"content":"Smaller Tables Page tables are too big Linear page tables are pretty big. Assuming a 32-bit address space, with 4KB pages and a 4-byte page-table size. There are roughly one million virtual pages in it, which will occupy 4MB memory. Simple Solution: Bigger Pages Side note: many archs now support multiple page sizes. Bigger pages may cause internal fragmentation. Hybrid Approach: Paging and Segments Most of the page table is unused, full of invalid entries. Instead of havin a single page table for the entris, why not one per logical segment. Base tells us the address of each segment. It holds the pysical address of the page table of that segment. Bound tells us the size of each segment. Thus the VA looks like: Seg | VPN | Offset The hardware use segment bit(SN) to determine which base and bound pair to use. IDEA: When you have two good and seemly opposite ideas, you should always see if you can combine them into hybrid that manage to achieve the best of both worlds. So if the code segment just have three entries, then the bound will be set to 3. This greatly reduces the waste of space for page tables. However, the problem is that if we have a large but sparsely-used heap. This will cause the external fragmentation again. Multi-level Page Tables It turns the linear page table like a tree. We use page directory to find the valid page tables. ![Multi-level Page Tables](img/Multi-level Page Tables.png) The page directory, in a simple two-level table, contains of a number of PDE(page directory entry). A PDE has a valid bit and a PFN, similar to PTE. However, the valid bit in PDE indcates whether at least one of the PTEs on the page pointed to by this PDE is valid. If the PDE is not valid, then the rest of the PDE is not defined. (The valid bit is the first bit in PDE) Advantage: Better supports sparse address spaces. If carefully constructed, each portion of the page table fits neatly within a page, so it’s easier to manage memory to avoid internal fragmentation. Disadvantage: If we have a TLB miss, two loads from memory will be required to get the right translation, one for the page directory and one for the PTE itself. So there is a time-space trade-off. Smaller cache is faster but not for free. The TLB miss suffers from higher cost with this smaller table. ![Page Directory and Page of PT](img/Page Directory and Page of PT.png) More Than Two Levels ![More than two levels](img/More than two levels.png) IMPORTANT: The PDT’s PA will be stored in a register, but it does not mean that the addr of PDT has to be in a fixed PA, it just has to be memorized. Inverted Page Tables Inverted page tables are even more space saving. We keep a single page table that has an entry for each physical page of the system. The entry tells use which prcess is using this page, and which virtual page of that process maps to this physical page. ","date":"2021-12-12","objectID":"/os_reading_note/:1:4","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#disadvantage"},{"categories":null,"content":"Smaller Tables Page tables are too big Linear page tables are pretty big. Assuming a 32-bit address space, with 4KB pages and a 4-byte page-table size. There are roughly one million virtual pages in it, which will occupy 4MB memory. Simple Solution: Bigger Pages Side note: many archs now support multiple page sizes. Bigger pages may cause internal fragmentation. Hybrid Approach: Paging and Segments Most of the page table is unused, full of invalid entries. Instead of havin a single page table for the entris, why not one per logical segment. Base tells us the address of each segment. It holds the pysical address of the page table of that segment. Bound tells us the size of each segment. Thus the VA looks like: Seg | VPN | Offset The hardware use segment bit(SN) to determine which base and bound pair to use. IDEA: When you have two good and seemly opposite ideas, you should always see if you can combine them into hybrid that manage to achieve the best of both worlds. So if the code segment just have three entries, then the bound will be set to 3. This greatly reduces the waste of space for page tables. However, the problem is that if we have a large but sparsely-used heap. This will cause the external fragmentation again. Multi-level Page Tables It turns the linear page table like a tree. We use page directory to find the valid page tables. ![Multi-level Page Tables](img/Multi-level Page Tables.png) The page directory, in a simple two-level table, contains of a number of PDE(page directory entry). A PDE has a valid bit and a PFN, similar to PTE. However, the valid bit in PDE indcates whether at least one of the PTEs on the page pointed to by this PDE is valid. If the PDE is not valid, then the rest of the PDE is not defined. (The valid bit is the first bit in PDE) Advantage: Better supports sparse address spaces. If carefully constructed, each portion of the page table fits neatly within a page, so it’s easier to manage memory to avoid internal fragmentation. Disadvantage: If we have a TLB miss, two loads from memory will be required to get the right translation, one for the page directory and one for the PTE itself. So there is a time-space trade-off. Smaller cache is faster but not for free. The TLB miss suffers from higher cost with this smaller table. ![Page Directory and Page of PT](img/Page Directory and Page of PT.png) More Than Two Levels ![More than two levels](img/More than two levels.png) IMPORTANT: The PDT’s PA will be stored in a register, but it does not mean that the addr of PDT has to be in a fixed PA, it just has to be memorized. Inverted Page Tables Inverted page tables are even more space saving. We keep a single page table that has an entry for each physical page of the system. The entry tells use which prcess is using this page, and which virtual page of that process maps to this physical page. ","date":"2021-12-12","objectID":"/os_reading_note/:1:4","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#more-than-two-levels"},{"categories":null,"content":"Smaller Tables Page tables are too big Linear page tables are pretty big. Assuming a 32-bit address space, with 4KB pages and a 4-byte page-table size. There are roughly one million virtual pages in it, which will occupy 4MB memory. Simple Solution: Bigger Pages Side note: many archs now support multiple page sizes. Bigger pages may cause internal fragmentation. Hybrid Approach: Paging and Segments Most of the page table is unused, full of invalid entries. Instead of havin a single page table for the entris, why not one per logical segment. Base tells us the address of each segment. It holds the pysical address of the page table of that segment. Bound tells us the size of each segment. Thus the VA looks like: Seg | VPN | Offset The hardware use segment bit(SN) to determine which base and bound pair to use. IDEA: When you have two good and seemly opposite ideas, you should always see if you can combine them into hybrid that manage to achieve the best of both worlds. So if the code segment just have three entries, then the bound will be set to 3. This greatly reduces the waste of space for page tables. However, the problem is that if we have a large but sparsely-used heap. This will cause the external fragmentation again. Multi-level Page Tables It turns the linear page table like a tree. We use page directory to find the valid page tables. ![Multi-level Page Tables](img/Multi-level Page Tables.png) The page directory, in a simple two-level table, contains of a number of PDE(page directory entry). A PDE has a valid bit and a PFN, similar to PTE. However, the valid bit in PDE indcates whether at least one of the PTEs on the page pointed to by this PDE is valid. If the PDE is not valid, then the rest of the PDE is not defined. (The valid bit is the first bit in PDE) Advantage: Better supports sparse address spaces. If carefully constructed, each portion of the page table fits neatly within a page, so it’s easier to manage memory to avoid internal fragmentation. Disadvantage: If we have a TLB miss, two loads from memory will be required to get the right translation, one for the page directory and one for the PTE itself. So there is a time-space trade-off. Smaller cache is faster but not for free. The TLB miss suffers from higher cost with this smaller table. ![Page Directory and Page of PT](img/Page Directory and Page of PT.png) More Than Two Levels ![More than two levels](img/More than two levels.png) IMPORTANT: The PDT’s PA will be stored in a register, but it does not mean that the addr of PDT has to be in a fixed PA, it just has to be memorized. Inverted Page Tables Inverted page tables are even more space saving. We keep a single page table that has an entry for each physical page of the system. The entry tells use which prcess is using this page, and which virtual page of that process maps to this physical page. ","date":"2021-12-12","objectID":"/os_reading_note/:1:4","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#important-the-pdts-pa-will-be-stored-in-a-register-but-it-does-not-mean-that-the-addr-of-pdt-has-to-be-in-a-fixed-pa-it-just-has-to-be-memorized"},{"categories":null,"content":"Smaller Tables Page tables are too big Linear page tables are pretty big. Assuming a 32-bit address space, with 4KB pages and a 4-byte page-table size. There are roughly one million virtual pages in it, which will occupy 4MB memory. Simple Solution: Bigger Pages Side note: many archs now support multiple page sizes. Bigger pages may cause internal fragmentation. Hybrid Approach: Paging and Segments Most of the page table is unused, full of invalid entries. Instead of havin a single page table for the entris, why not one per logical segment. Base tells us the address of each segment. It holds the pysical address of the page table of that segment. Bound tells us the size of each segment. Thus the VA looks like: Seg | VPN | Offset The hardware use segment bit(SN) to determine which base and bound pair to use. IDEA: When you have two good and seemly opposite ideas, you should always see if you can combine them into hybrid that manage to achieve the best of both worlds. So if the code segment just have three entries, then the bound will be set to 3. This greatly reduces the waste of space for page tables. However, the problem is that if we have a large but sparsely-used heap. This will cause the external fragmentation again. Multi-level Page Tables It turns the linear page table like a tree. We use page directory to find the valid page tables. ![Multi-level Page Tables](img/Multi-level Page Tables.png) The page directory, in a simple two-level table, contains of a number of PDE(page directory entry). A PDE has a valid bit and a PFN, similar to PTE. However, the valid bit in PDE indcates whether at least one of the PTEs on the page pointed to by this PDE is valid. If the PDE is not valid, then the rest of the PDE is not defined. (The valid bit is the first bit in PDE) Advantage: Better supports sparse address spaces. If carefully constructed, each portion of the page table fits neatly within a page, so it’s easier to manage memory to avoid internal fragmentation. Disadvantage: If we have a TLB miss, two loads from memory will be required to get the right translation, one for the page directory and one for the PTE itself. So there is a time-space trade-off. Smaller cache is faster but not for free. The TLB miss suffers from higher cost with this smaller table. ![Page Directory and Page of PT](img/Page Directory and Page of PT.png) More Than Two Levels ![More than two levels](img/More than two levels.png) IMPORTANT: The PDT’s PA will be stored in a register, but it does not mean that the addr of PDT has to be in a fixed PA, it just has to be memorized. Inverted Page Tables Inverted page tables are even more space saving. We keep a single page table that has an entry for each physical page of the system. The entry tells use which prcess is using this page, and which virtual page of that process maps to this physical page. ","date":"2021-12-12","objectID":"/os_reading_note/:1:4","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#inverted-page-tables"},{"categories":null,"content":"Swapping: Mechanisms Swap space Firstly, we need to reserve some space on the disk for moving pages back and forth. Then the OS need to remember the disk address of a given page. The size of the swap space is important since it determines the maximum number of memory pages that can be in use by a system in a ginven time. ![Physical Memory and Swap Space](img/Physical Memory and Swap Space.png) The Present Bit The hardware first extract the VPN from the VA, the check the TLB for a match. If a miss, the hardware locates the PDT and corresponding page tables and look up the PTE, and the present bit on the PTE is 0, it means that the page is not in physical memory, which is commonly referred to as a page fault. Nowadays not only swapping will trigger page fault but also illegal memory accesses and so on. It becomes a more general term for “page missing”. The Page Fault Upon a page fault, the OS invokes page-fault handler to deal with the issue, no matter for a hardware or software managed TLBs. When a page fault happens, the OS needs to swap the page into memory in order to service the page fault. When the disk I/O completes, the OS will then update the page table to mark the page as present, update the PFN of the PTE, and retry the instruction. Notice that this next attampt still may generate a TLB miss, which would the be serviced and update the TLB with the translation.(The other approach is to update the TLB when handling the page fault.) Finally, a last attampt will get a TLB hit. Note that while the I/O is in flight, the process will be blocked, and other processes will execute. How will the OS know where to find the desired page? The OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address. When the OS receives a page fault for a page, it looks in the PTE to find the address, and issues the request to disk to fetch the page into memory. Why hardware doesn’t handle page faults? Page fault to disk is slow, even if the OS takes a long time to handle it, there is not to much of a difference. To deal with the page fault, the hardware need to know the swap space, the I/Os to disk, and a lot of other details. What if memory is full? The OS may lie to first page out one or more pages to make room for the new pages. The process of picking a page to kick out, or replace is known as the page-replacement policy. Page Fault Control Flow If the page was both present and valid, the TLB miss handler will just grab the PFN from the PTE into TLB and try again. If the page was not present but valid, then the page fault handler will run. If the page is not valid, in this case, no other bits matter. The hardware traps this invalid access, and the OS trap handler runs, likely terminating the offending process. When Replacement Really occur PLACEHOLDER ","date":"2021-12-12","objectID":"/os_reading_note/:1:5","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#swapping-mechanisms"},{"categories":null,"content":"Swapping: Mechanisms Swap space Firstly, we need to reserve some space on the disk for moving pages back and forth. Then the OS need to remember the disk address of a given page. The size of the swap space is important since it determines the maximum number of memory pages that can be in use by a system in a ginven time. ![Physical Memory and Swap Space](img/Physical Memory and Swap Space.png) The Present Bit The hardware first extract the VPN from the VA, the check the TLB for a match. If a miss, the hardware locates the PDT and corresponding page tables and look up the PTE, and the present bit on the PTE is 0, it means that the page is not in physical memory, which is commonly referred to as a page fault. Nowadays not only swapping will trigger page fault but also illegal memory accesses and so on. It becomes a more general term for “page missing”. The Page Fault Upon a page fault, the OS invokes page-fault handler to deal with the issue, no matter for a hardware or software managed TLBs. When a page fault happens, the OS needs to swap the page into memory in order to service the page fault. When the disk I/O completes, the OS will then update the page table to mark the page as present, update the PFN of the PTE, and retry the instruction. Notice that this next attampt still may generate a TLB miss, which would the be serviced and update the TLB with the translation.(The other approach is to update the TLB when handling the page fault.) Finally, a last attampt will get a TLB hit. Note that while the I/O is in flight, the process will be blocked, and other processes will execute. How will the OS know where to find the desired page? The OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address. When the OS receives a page fault for a page, it looks in the PTE to find the address, and issues the request to disk to fetch the page into memory. Why hardware doesn’t handle page faults? Page fault to disk is slow, even if the OS takes a long time to handle it, there is not to much of a difference. To deal with the page fault, the hardware need to know the swap space, the I/Os to disk, and a lot of other details. What if memory is full? The OS may lie to first page out one or more pages to make room for the new pages. The process of picking a page to kick out, or replace is known as the page-replacement policy. Page Fault Control Flow If the page was both present and valid, the TLB miss handler will just grab the PFN from the PTE into TLB and try again. If the page was not present but valid, then the page fault handler will run. If the page is not valid, in this case, no other bits matter. The hardware traps this invalid access, and the OS trap handler runs, likely terminating the offending process. When Replacement Really occur PLACEHOLDER ","date":"2021-12-12","objectID":"/os_reading_note/:1:5","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#swap-space"},{"categories":null,"content":"Swapping: Mechanisms Swap space Firstly, we need to reserve some space on the disk for moving pages back and forth. Then the OS need to remember the disk address of a given page. The size of the swap space is important since it determines the maximum number of memory pages that can be in use by a system in a ginven time. ![Physical Memory and Swap Space](img/Physical Memory and Swap Space.png) The Present Bit The hardware first extract the VPN from the VA, the check the TLB for a match. If a miss, the hardware locates the PDT and corresponding page tables and look up the PTE, and the present bit on the PTE is 0, it means that the page is not in physical memory, which is commonly referred to as a page fault. Nowadays not only swapping will trigger page fault but also illegal memory accesses and so on. It becomes a more general term for “page missing”. The Page Fault Upon a page fault, the OS invokes page-fault handler to deal with the issue, no matter for a hardware or software managed TLBs. When a page fault happens, the OS needs to swap the page into memory in order to service the page fault. When the disk I/O completes, the OS will then update the page table to mark the page as present, update the PFN of the PTE, and retry the instruction. Notice that this next attampt still may generate a TLB miss, which would the be serviced and update the TLB with the translation.(The other approach is to update the TLB when handling the page fault.) Finally, a last attampt will get a TLB hit. Note that while the I/O is in flight, the process will be blocked, and other processes will execute. How will the OS know where to find the desired page? The OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address. When the OS receives a page fault for a page, it looks in the PTE to find the address, and issues the request to disk to fetch the page into memory. Why hardware doesn’t handle page faults? Page fault to disk is slow, even if the OS takes a long time to handle it, there is not to much of a difference. To deal with the page fault, the hardware need to know the swap space, the I/Os to disk, and a lot of other details. What if memory is full? The OS may lie to first page out one or more pages to make room for the new pages. The process of picking a page to kick out, or replace is known as the page-replacement policy. Page Fault Control Flow If the page was both present and valid, the TLB miss handler will just grab the PFN from the PTE into TLB and try again. If the page was not present but valid, then the page fault handler will run. If the page is not valid, in this case, no other bits matter. The hardware traps this invalid access, and the OS trap handler runs, likely terminating the offending process. When Replacement Really occur PLACEHOLDER ","date":"2021-12-12","objectID":"/os_reading_note/:1:5","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#the-present-bit"},{"categories":null,"content":"Swapping: Mechanisms Swap space Firstly, we need to reserve some space on the disk for moving pages back and forth. Then the OS need to remember the disk address of a given page. The size of the swap space is important since it determines the maximum number of memory pages that can be in use by a system in a ginven time. ![Physical Memory and Swap Space](img/Physical Memory and Swap Space.png) The Present Bit The hardware first extract the VPN from the VA, the check the TLB for a match. If a miss, the hardware locates the PDT and corresponding page tables and look up the PTE, and the present bit on the PTE is 0, it means that the page is not in physical memory, which is commonly referred to as a page fault. Nowadays not only swapping will trigger page fault but also illegal memory accesses and so on. It becomes a more general term for “page missing”. The Page Fault Upon a page fault, the OS invokes page-fault handler to deal with the issue, no matter for a hardware or software managed TLBs. When a page fault happens, the OS needs to swap the page into memory in order to service the page fault. When the disk I/O completes, the OS will then update the page table to mark the page as present, update the PFN of the PTE, and retry the instruction. Notice that this next attampt still may generate a TLB miss, which would the be serviced and update the TLB with the translation.(The other approach is to update the TLB when handling the page fault.) Finally, a last attampt will get a TLB hit. Note that while the I/O is in flight, the process will be blocked, and other processes will execute. How will the OS know where to find the desired page? The OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address. When the OS receives a page fault for a page, it looks in the PTE to find the address, and issues the request to disk to fetch the page into memory. Why hardware doesn’t handle page faults? Page fault to disk is slow, even if the OS takes a long time to handle it, there is not to much of a difference. To deal with the page fault, the hardware need to know the swap space, the I/Os to disk, and a lot of other details. What if memory is full? The OS may lie to first page out one or more pages to make room for the new pages. The process of picking a page to kick out, or replace is known as the page-replacement policy. Page Fault Control Flow If the page was both present and valid, the TLB miss handler will just grab the PFN from the PTE into TLB and try again. If the page was not present but valid, then the page fault handler will run. If the page is not valid, in this case, no other bits matter. The hardware traps this invalid access, and the OS trap handler runs, likely terminating the offending process. When Replacement Really occur PLACEHOLDER ","date":"2021-12-12","objectID":"/os_reading_note/:1:5","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#the-page-fault"},{"categories":null,"content":"Swapping: Mechanisms Swap space Firstly, we need to reserve some space on the disk for moving pages back and forth. Then the OS need to remember the disk address of a given page. The size of the swap space is important since it determines the maximum number of memory pages that can be in use by a system in a ginven time. ![Physical Memory and Swap Space](img/Physical Memory and Swap Space.png) The Present Bit The hardware first extract the VPN from the VA, the check the TLB for a match. If a miss, the hardware locates the PDT and corresponding page tables and look up the PTE, and the present bit on the PTE is 0, it means that the page is not in physical memory, which is commonly referred to as a page fault. Nowadays not only swapping will trigger page fault but also illegal memory accesses and so on. It becomes a more general term for “page missing”. The Page Fault Upon a page fault, the OS invokes page-fault handler to deal with the issue, no matter for a hardware or software managed TLBs. When a page fault happens, the OS needs to swap the page into memory in order to service the page fault. When the disk I/O completes, the OS will then update the page table to mark the page as present, update the PFN of the PTE, and retry the instruction. Notice that this next attampt still may generate a TLB miss, which would the be serviced and update the TLB with the translation.(The other approach is to update the TLB when handling the page fault.) Finally, a last attampt will get a TLB hit. Note that while the I/O is in flight, the process will be blocked, and other processes will execute. How will the OS know where to find the desired page? The OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address. When the OS receives a page fault for a page, it looks in the PTE to find the address, and issues the request to disk to fetch the page into memory. Why hardware doesn’t handle page faults? Page fault to disk is slow, even if the OS takes a long time to handle it, there is not to much of a difference. To deal with the page fault, the hardware need to know the swap space, the I/Os to disk, and a lot of other details. What if memory is full? The OS may lie to first page out one or more pages to make room for the new pages. The process of picking a page to kick out, or replace is known as the page-replacement policy. Page Fault Control Flow If the page was both present and valid, the TLB miss handler will just grab the PFN from the PTE into TLB and try again. If the page was not present but valid, then the page fault handler will run. If the page is not valid, in this case, no other bits matter. The hardware traps this invalid access, and the OS trap handler runs, likely terminating the offending process. When Replacement Really occur PLACEHOLDER ","date":"2021-12-12","objectID":"/os_reading_note/:1:5","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#how-will-the-os-know-where-to-find-the-desired-page"},{"categories":null,"content":"Swapping: Mechanisms Swap space Firstly, we need to reserve some space on the disk for moving pages back and forth. Then the OS need to remember the disk address of a given page. The size of the swap space is important since it determines the maximum number of memory pages that can be in use by a system in a ginven time. ![Physical Memory and Swap Space](img/Physical Memory and Swap Space.png) The Present Bit The hardware first extract the VPN from the VA, the check the TLB for a match. If a miss, the hardware locates the PDT and corresponding page tables and look up the PTE, and the present bit on the PTE is 0, it means that the page is not in physical memory, which is commonly referred to as a page fault. Nowadays not only swapping will trigger page fault but also illegal memory accesses and so on. It becomes a more general term for “page missing”. The Page Fault Upon a page fault, the OS invokes page-fault handler to deal with the issue, no matter for a hardware or software managed TLBs. When a page fault happens, the OS needs to swap the page into memory in order to service the page fault. When the disk I/O completes, the OS will then update the page table to mark the page as present, update the PFN of the PTE, and retry the instruction. Notice that this next attampt still may generate a TLB miss, which would the be serviced and update the TLB with the translation.(The other approach is to update the TLB when handling the page fault.) Finally, a last attampt will get a TLB hit. Note that while the I/O is in flight, the process will be blocked, and other processes will execute. How will the OS know where to find the desired page? The OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address. When the OS receives a page fault for a page, it looks in the PTE to find the address, and issues the request to disk to fetch the page into memory. Why hardware doesn’t handle page faults? Page fault to disk is slow, even if the OS takes a long time to handle it, there is not to much of a difference. To deal with the page fault, the hardware need to know the swap space, the I/Os to disk, and a lot of other details. What if memory is full? The OS may lie to first page out one or more pages to make room for the new pages. The process of picking a page to kick out, or replace is known as the page-replacement policy. Page Fault Control Flow If the page was both present and valid, the TLB miss handler will just grab the PFN from the PTE into TLB and try again. If the page was not present but valid, then the page fault handler will run. If the page is not valid, in this case, no other bits matter. The hardware traps this invalid access, and the OS trap handler runs, likely terminating the offending process. When Replacement Really occur PLACEHOLDER ","date":"2021-12-12","objectID":"/os_reading_note/:1:5","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#why-hardware-doesnt-handle-page-faults"},{"categories":null,"content":"Swapping: Mechanisms Swap space Firstly, we need to reserve some space on the disk for moving pages back and forth. Then the OS need to remember the disk address of a given page. The size of the swap space is important since it determines the maximum number of memory pages that can be in use by a system in a ginven time. ![Physical Memory and Swap Space](img/Physical Memory and Swap Space.png) The Present Bit The hardware first extract the VPN from the VA, the check the TLB for a match. If a miss, the hardware locates the PDT and corresponding page tables and look up the PTE, and the present bit on the PTE is 0, it means that the page is not in physical memory, which is commonly referred to as a page fault. Nowadays not only swapping will trigger page fault but also illegal memory accesses and so on. It becomes a more general term for “page missing”. The Page Fault Upon a page fault, the OS invokes page-fault handler to deal with the issue, no matter for a hardware or software managed TLBs. When a page fault happens, the OS needs to swap the page into memory in order to service the page fault. When the disk I/O completes, the OS will then update the page table to mark the page as present, update the PFN of the PTE, and retry the instruction. Notice that this next attampt still may generate a TLB miss, which would the be serviced and update the TLB with the translation.(The other approach is to update the TLB when handling the page fault.) Finally, a last attampt will get a TLB hit. Note that while the I/O is in flight, the process will be blocked, and other processes will execute. How will the OS know where to find the desired page? The OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address. When the OS receives a page fault for a page, it looks in the PTE to find the address, and issues the request to disk to fetch the page into memory. Why hardware doesn’t handle page faults? Page fault to disk is slow, even if the OS takes a long time to handle it, there is not to much of a difference. To deal with the page fault, the hardware need to know the swap space, the I/Os to disk, and a lot of other details. What if memory is full? The OS may lie to first page out one or more pages to make room for the new pages. The process of picking a page to kick out, or replace is known as the page-replacement policy. Page Fault Control Flow If the page was both present and valid, the TLB miss handler will just grab the PFN from the PTE into TLB and try again. If the page was not present but valid, then the page fault handler will run. If the page is not valid, in this case, no other bits matter. The hardware traps this invalid access, and the OS trap handler runs, likely terminating the offending process. When Replacement Really occur PLACEHOLDER ","date":"2021-12-12","objectID":"/os_reading_note/:1:5","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#what-if-memory-is-full"},{"categories":null,"content":"Swapping: Mechanisms Swap space Firstly, we need to reserve some space on the disk for moving pages back and forth. Then the OS need to remember the disk address of a given page. The size of the swap space is important since it determines the maximum number of memory pages that can be in use by a system in a ginven time. ![Physical Memory and Swap Space](img/Physical Memory and Swap Space.png) The Present Bit The hardware first extract the VPN from the VA, the check the TLB for a match. If a miss, the hardware locates the PDT and corresponding page tables and look up the PTE, and the present bit on the PTE is 0, it means that the page is not in physical memory, which is commonly referred to as a page fault. Nowadays not only swapping will trigger page fault but also illegal memory accesses and so on. It becomes a more general term for “page missing”. The Page Fault Upon a page fault, the OS invokes page-fault handler to deal with the issue, no matter for a hardware or software managed TLBs. When a page fault happens, the OS needs to swap the page into memory in order to service the page fault. When the disk I/O completes, the OS will then update the page table to mark the page as present, update the PFN of the PTE, and retry the instruction. Notice that this next attampt still may generate a TLB miss, which would the be serviced and update the TLB with the translation.(The other approach is to update the TLB when handling the page fault.) Finally, a last attampt will get a TLB hit. Note that while the I/O is in flight, the process will be blocked, and other processes will execute. How will the OS know where to find the desired page? The OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address. When the OS receives a page fault for a page, it looks in the PTE to find the address, and issues the request to disk to fetch the page into memory. Why hardware doesn’t handle page faults? Page fault to disk is slow, even if the OS takes a long time to handle it, there is not to much of a difference. To deal with the page fault, the hardware need to know the swap space, the I/Os to disk, and a lot of other details. What if memory is full? The OS may lie to first page out one or more pages to make room for the new pages. The process of picking a page to kick out, or replace is known as the page-replacement policy. Page Fault Control Flow If the page was both present and valid, the TLB miss handler will just grab the PFN from the PTE into TLB and try again. If the page was not present but valid, then the page fault handler will run. If the page is not valid, in this case, no other bits matter. The hardware traps this invalid access, and the OS trap handler runs, likely terminating the offending process. When Replacement Really occur PLACEHOLDER ","date":"2021-12-12","objectID":"/os_reading_note/:1:5","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#page-fault-control-flow"},{"categories":null,"content":"Swapping: Mechanisms Swap space Firstly, we need to reserve some space on the disk for moving pages back and forth. Then the OS need to remember the disk address of a given page. The size of the swap space is important since it determines the maximum number of memory pages that can be in use by a system in a ginven time. ![Physical Memory and Swap Space](img/Physical Memory and Swap Space.png) The Present Bit The hardware first extract the VPN from the VA, the check the TLB for a match. If a miss, the hardware locates the PDT and corresponding page tables and look up the PTE, and the present bit on the PTE is 0, it means that the page is not in physical memory, which is commonly referred to as a page fault. Nowadays not only swapping will trigger page fault but also illegal memory accesses and so on. It becomes a more general term for “page missing”. The Page Fault Upon a page fault, the OS invokes page-fault handler to deal with the issue, no matter for a hardware or software managed TLBs. When a page fault happens, the OS needs to swap the page into memory in order to service the page fault. When the disk I/O completes, the OS will then update the page table to mark the page as present, update the PFN of the PTE, and retry the instruction. Notice that this next attampt still may generate a TLB miss, which would the be serviced and update the TLB with the translation.(The other approach is to update the TLB when handling the page fault.) Finally, a last attampt will get a TLB hit. Note that while the I/O is in flight, the process will be blocked, and other processes will execute. How will the OS know where to find the desired page? The OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address. When the OS receives a page fault for a page, it looks in the PTE to find the address, and issues the request to disk to fetch the page into memory. Why hardware doesn’t handle page faults? Page fault to disk is slow, even if the OS takes a long time to handle it, there is not to much of a difference. To deal with the page fault, the hardware need to know the swap space, the I/Os to disk, and a lot of other details. What if memory is full? The OS may lie to first page out one or more pages to make room for the new pages. The process of picking a page to kick out, or replace is known as the page-replacement policy. Page Fault Control Flow If the page was both present and valid, the TLB miss handler will just grab the PFN from the PTE into TLB and try again. If the page was not present but valid, then the page fault handler will run. If the page is not valid, in this case, no other bits matter. The hardware traps this invalid access, and the OS trap handler runs, likely terminating the offending process. When Replacement Really occur PLACEHOLDER ","date":"2021-12-12","objectID":"/os_reading_note/:1:5","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#when-replacement-really-occur"},{"categories":null,"content":"Swapping Policies Cache management The Optimal Replacement Policy FIFO Random LRU Approximating LRU The key is the use bit and clock algorithm. Thrashing ","date":"2021-12-12","objectID":"/os_reading_note/:1:6","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#swapping-policies"},{"categories":null,"content":"Swapping Policies Cache management The Optimal Replacement Policy FIFO Random LRU Approximating LRU The key is the use bit and clock algorithm. Thrashing ","date":"2021-12-12","objectID":"/os_reading_note/:1:6","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#cache-management"},{"categories":null,"content":"Swapping Policies Cache management The Optimal Replacement Policy FIFO Random LRU Approximating LRU The key is the use bit and clock algorithm. Thrashing ","date":"2021-12-12","objectID":"/os_reading_note/:1:6","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#the-optimal-replacement-policy"},{"categories":null,"content":"Swapping Policies Cache management The Optimal Replacement Policy FIFO Random LRU Approximating LRU The key is the use bit and clock algorithm. Thrashing ","date":"2021-12-12","objectID":"/os_reading_note/:1:6","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#fifo"},{"categories":null,"content":"Swapping Policies Cache management The Optimal Replacement Policy FIFO Random LRU Approximating LRU The key is the use bit and clock algorithm. Thrashing ","date":"2021-12-12","objectID":"/os_reading_note/:1:6","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#random"},{"categories":null,"content":"Swapping Policies Cache management The Optimal Replacement Policy FIFO Random LRU Approximating LRU The key is the use bit and clock algorithm. Thrashing ","date":"2021-12-12","objectID":"/os_reading_note/:1:6","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#lru"},{"categories":null,"content":"Swapping Policies Cache management The Optimal Replacement Policy FIFO Random LRU Approximating LRU The key is the use bit and clock algorithm. Thrashing ","date":"2021-12-12","objectID":"/os_reading_note/:1:6","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#approximating-lru"},{"categories":null,"content":"Swapping Policies Cache management The Optimal Replacement Policy FIFO Random LRU Approximating LRU The key is the use bit and clock algorithm. Thrashing ","date":"2021-12-12","objectID":"/os_reading_note/:1:6","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#thrashing"},{"categories":null,"content":"Concurrency ","date":"2021-12-12","objectID":"/os_reading_note/:2:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#concurrency"},{"categories":null,"content":"Intro Each thread is very much like a separate process, except for one difference: they share the same address space and thus can access the same data. Context switch between threads are similar to that between processes. We save the PC, registers, state and so on in TCB. The difference is that the address space remains the same. why thread? Parallelism To avoid blocking program progress due to slow I/O Thread Creation After the threads are created, the OS scheduler decides who runs the next. Why It Gets Worse: Shared Data [recall the concept if the Hazard] Data Hazards RAW i1. R2 \u003c- R5 + R3 i2. R4 \u003c- R2 + R3 WAR i1. R4 \u003c- R1 + R5 i2. R5 \u003c- R1 + R2 WAW i1. R2 \u003c- R4 + R7 i2. R2 \u003c- R1 + R3 Structural Hazards Control Hazards // compile with -g will include symbol info in the program. Then `prompt \u003e objdump -d main will give you the assembly code neatly labeled. // You may want to master debugger gdb, memory profilers valgrind or purify, and the compiler itself. Core Problem: Uncontrolled Scheduling We may have a race condition, or so to speak, a data race. Anindeterminateprogramconsistsofoneormoreraceconditions; the output of the program varies from run to run, depending on which threads ran when. The outcome is thus not deterministic, something we usually expect from computer systems. A critical section is a piece of code that accesses a shared variable (or more generally, a shared resource) and must not be concurrently executed by more than one thread. What we really want for this code is what we call mutual exclusion. This property guarantees that if one thread is executing within the critical section, the others will be prevented from doing so. One More Problem: Waiting For Another ","date":"2021-12-12","objectID":"/os_reading_note/:2:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#intro"},{"categories":null,"content":"Intro Each thread is very much like a separate process, except for one difference: they share the same address space and thus can access the same data. Context switch between threads are similar to that between processes. We save the PC, registers, state and so on in TCB. The difference is that the address space remains the same. why thread? Parallelism To avoid blocking program progress due to slow I/O Thread Creation After the threads are created, the OS scheduler decides who runs the next. Why It Gets Worse: Shared Data [recall the concept if the Hazard] Data Hazards RAW i1. R2 objdump -d main will give you the assembly code neatly labeled. // You may want to master debugger gdb, memory profilers valgrind or purify, and the compiler itself. Core Problem: Uncontrolled Scheduling We may have a race condition, or so to speak, a data race. Anindeterminateprogramconsistsofoneormoreraceconditions; the output of the program varies from run to run, depending on which threads ran when. The outcome is thus not deterministic, something we usually expect from computer systems. A critical section is a piece of code that accesses a shared variable (or more generally, a shared resource) and must not be concurrently executed by more than one thread. What we really want for this code is what we call mutual exclusion. This property guarantees that if one thread is executing within the critical section, the others will be prevented from doing so. One More Problem: Waiting For Another ","date":"2021-12-12","objectID":"/os_reading_note/:2:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#why-thread"},{"categories":null,"content":"Intro Each thread is very much like a separate process, except for one difference: they share the same address space and thus can access the same data. Context switch between threads are similar to that between processes. We save the PC, registers, state and so on in TCB. The difference is that the address space remains the same. why thread? Parallelism To avoid blocking program progress due to slow I/O Thread Creation After the threads are created, the OS scheduler decides who runs the next. Why It Gets Worse: Shared Data [recall the concept if the Hazard] Data Hazards RAW i1. R2 objdump -d main will give you the assembly code neatly labeled. // You may want to master debugger gdb, memory profilers valgrind or purify, and the compiler itself. Core Problem: Uncontrolled Scheduling We may have a race condition, or so to speak, a data race. Anindeterminateprogramconsistsofoneormoreraceconditions; the output of the program varies from run to run, depending on which threads ran when. The outcome is thus not deterministic, something we usually expect from computer systems. A critical section is a piece of code that accesses a shared variable (or more generally, a shared resource) and must not be concurrently executed by more than one thread. What we really want for this code is what we call mutual exclusion. This property guarantees that if one thread is executing within the critical section, the others will be prevented from doing so. One More Problem: Waiting For Another ","date":"2021-12-12","objectID":"/os_reading_note/:2:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#thread-creation"},{"categories":null,"content":"Intro Each thread is very much like a separate process, except for one difference: they share the same address space and thus can access the same data. Context switch between threads are similar to that between processes. We save the PC, registers, state and so on in TCB. The difference is that the address space remains the same. why thread? Parallelism To avoid blocking program progress due to slow I/O Thread Creation After the threads are created, the OS scheduler decides who runs the next. Why It Gets Worse: Shared Data [recall the concept if the Hazard] Data Hazards RAW i1. R2 objdump -d main will give you the assembly code neatly labeled. // You may want to master debugger gdb, memory profilers valgrind or purify, and the compiler itself. Core Problem: Uncontrolled Scheduling We may have a race condition, or so to speak, a data race. Anindeterminateprogramconsistsofoneormoreraceconditions; the output of the program varies from run to run, depending on which threads ran when. The outcome is thus not deterministic, something we usually expect from computer systems. A critical section is a piece of code that accesses a shared variable (or more generally, a shared resource) and must not be concurrently executed by more than one thread. What we really want for this code is what we call mutual exclusion. This property guarantees that if one thread is executing within the critical section, the others will be prevented from doing so. One More Problem: Waiting For Another ","date":"2021-12-12","objectID":"/os_reading_note/:2:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#why-it-gets-worse-shared-data"},{"categories":null,"content":"Intro Each thread is very much like a separate process, except for one difference: they share the same address space and thus can access the same data. Context switch between threads are similar to that between processes. We save the PC, registers, state and so on in TCB. The difference is that the address space remains the same. why thread? Parallelism To avoid blocking program progress due to slow I/O Thread Creation After the threads are created, the OS scheduler decides who runs the next. Why It Gets Worse: Shared Data [recall the concept if the Hazard] Data Hazards RAW i1. R2 objdump -d main will give you the assembly code neatly labeled. // You may want to master debugger gdb, memory profilers valgrind or purify, and the compiler itself. Core Problem: Uncontrolled Scheduling We may have a race condition, or so to speak, a data race. Anindeterminateprogramconsistsofoneormoreraceconditions; the output of the program varies from run to run, depending on which threads ran when. The outcome is thus not deterministic, something we usually expect from computer systems. A critical section is a piece of code that accesses a shared variable (or more generally, a shared resource) and must not be concurrently executed by more than one thread. What we really want for this code is what we call mutual exclusion. This property guarantees that if one thread is executing within the critical section, the others will be prevented from doing so. One More Problem: Waiting For Another ","date":"2021-12-12","objectID":"/os_reading_note/:2:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#recall-the-concept-if-the-hazard"},{"categories":null,"content":"Intro Each thread is very much like a separate process, except for one difference: they share the same address space and thus can access the same data. Context switch between threads are similar to that between processes. We save the PC, registers, state and so on in TCB. The difference is that the address space remains the same. why thread? Parallelism To avoid blocking program progress due to slow I/O Thread Creation After the threads are created, the OS scheduler decides who runs the next. Why It Gets Worse: Shared Data [recall the concept if the Hazard] Data Hazards RAW i1. R2 objdump -d main will give you the assembly code neatly labeled. // You may want to master debugger gdb, memory profilers valgrind or purify, and the compiler itself. Core Problem: Uncontrolled Scheduling We may have a race condition, or so to speak, a data race. Anindeterminateprogramconsistsofoneormoreraceconditions; the output of the program varies from run to run, depending on which threads ran when. The outcome is thus not deterministic, something we usually expect from computer systems. A critical section is a piece of code that accesses a shared variable (or more generally, a shared resource) and must not be concurrently executed by more than one thread. What we really want for this code is what we call mutual exclusion. This property guarantees that if one thread is executing within the critical section, the others will be prevented from doing so. One More Problem: Waiting For Another ","date":"2021-12-12","objectID":"/os_reading_note/:2:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#core-problem-uncontrolled-scheduling"},{"categories":null,"content":"Intro Each thread is very much like a separate process, except for one difference: they share the same address space and thus can access the same data. Context switch between threads are similar to that between processes. We save the PC, registers, state and so on in TCB. The difference is that the address space remains the same. why thread? Parallelism To avoid blocking program progress due to slow I/O Thread Creation After the threads are created, the OS scheduler decides who runs the next. Why It Gets Worse: Shared Data [recall the concept if the Hazard] Data Hazards RAW i1. R2 objdump -d main will give you the assembly code neatly labeled. // You may want to master debugger gdb, memory profilers valgrind or purify, and the compiler itself. Core Problem: Uncontrolled Scheduling We may have a race condition, or so to speak, a data race. Anindeterminateprogramconsistsofoneormoreraceconditions; the output of the program varies from run to run, depending on which threads ran when. The outcome is thus not deterministic, something we usually expect from computer systems. A critical section is a piece of code that accesses a shared variable (or more generally, a shared resource) and must not be concurrently executed by more than one thread. What we really want for this code is what we call mutual exclusion. This property guarantees that if one thread is executing within the critical section, the others will be prevented from doing so. One More Problem: Waiting For Another ","date":"2021-12-12","objectID":"/os_reading_note/:2:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#one-more-problem-waiting-for-another"},{"categories":null,"content":"Interlude: Thread API Thread Creation #include \u003cpthread.h\u003eint pthread_create(pthread_t* thread, const pthread_attr_t * attr, void* (*start_routine) (void*), void *arg); /* The second argument, attr, is used to specify any attributes this thread might have. Some examples include setting the stack size or perhaps in- formation about the scheduling priority of the thread. An attribute is initialized with a separate call to pthread attr init(); see the man- ual page for details. However, in most cases, the defaults will be fine; in this case, we will simply pass the value NULL in. The third argument is the most complex, but is really just asking: which function should this thread start running in? In C, we call this a function pointer, and this one tells us the following is expected: a function name (start routine), which is passed a single argument of type void * (as indicated in the parentheses after start routine), and which returns a value of type void * (i.e., a void pointer). Finally, the fourth argument, arg, is exactly the argument to be passed to the function where the thread begins execution. */ /* having a void pointer as an argument to the function start routine allows us to pass in any type of argument; having it as a return value allows the thread to return any type of result. */ #include \u003cstdio.h\u003e#include \u003cpthread.h\u003e typedef struct { int a; int b; } myarg_t; void *mythread(void *arg) { myarg_t *args = (myarg_t*) arg; printf(\"%d %d\\n\", args-\u003ea, args-\u003eb); return NULL; } int main(int argc, char* argv[]){ pthread_t p; myarg_t args = { 10, 20 }; int rc = pthread_create(\u0026p, NULL, mythread, \u0026args); } Thread Completion // What if you want to wait for a thread to complete? int pthread_join(pthread_t thread, void **value_ptr); /* This routine takes two arguments. The first is of type pthread t, and is used to specify which thread to wait for. This variable is initialized by the thread creation routine (when you pass a pointer to it as an argument to pthread create()); if you keep it around, you can use it to wait for that thread to terminate. The second argument is a pointer to the return value you expect to get back. Because the routine can return anything, it is defined to return a pointer to void; because the pthread join() routine changes the value of the passed in argument, you need to pass in a pointer to that value, not just the value itself. */ typedef struct { int a; int b; } myarg_t; typedef struct { int x; int y; } myret_t; void *mythread(void *arg) { myret_t *rvals = Malloc(sizeof(myret_t)); rvals-\u003ex = 1; rvals-\u003ey = 2; return (void *) rvals; } int main(int argc, char *argv[]) { pthread_t p; myret_t *rvals; myarg_t args = { 10, 20 }; Pthread_create(\u0026p, NULL, mythread, \u0026args); Pthread_join(p, (void **) \u0026rvals); printf(\"returned %d %d\\n\", rvals-\u003ex, rvals-\u003ey); free(rvals); return 0; } If we just create a thread with no args, we can pass NULL in as an arg. Similarly, we can pass NULL into pthread_join() if we care nothing about the return value. If we are just passing in a single value, then we don’t need to package it up as a arg. We should be extremely careful with how values are returned from a thread. Never return a pointer which refers to something allocated on the thread’s call stack. We have a pthread_create() followed bypthread_join(). We can use procedure call instead. Locks The routines should be easy to understand and use. When you have a region of code that is a critical section, and thus needs to be protected to ensure correct operation, locks are quite useful. int pthread_mutex_lock(pthread_mutex_t *mutex) int pthread_mutex_unlock(pthread_mutex_t *mutex) pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; // we must do the init for locks // or we can do as int rc = pthread_mutex_init(\u0026lock, NULL); assert(rc == 0); // always check success! pthread_mutex_lock(\u0026lock); x = x + 1; // or whatever your critical section is pthread_mutex_unlock(\u0026lock); Also, pthread mutex destroy() should be called afterwards. There","date":"2021-12-12","objectID":"/os_reading_note/:2:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#interlude-thread-api"},{"categories":null,"content":"Interlude: Thread API Thread Creation #include int pthread_create(pthread_t* thread, const pthread_attr_t * attr, void* (*start_routine) (void*), void *arg); /* The second argument, attr, is used to specify any attributes this thread might have. Some examples include setting the stack size or perhaps in- formation about the scheduling priority of the thread. An attribute is initialized with a separate call to pthread attr init(); see the man- ual page for details. However, in most cases, the defaults will be fine; in this case, we will simply pass the value NULL in. The third argument is the most complex, but is really just asking: which function should this thread start running in? In C, we call this a function pointer, and this one tells us the following is expected: a function name (start routine), which is passed a single argument of type void * (as indicated in the parentheses after start routine), and which returns a value of type void * (i.e., a void pointer). Finally, the fourth argument, arg, is exactly the argument to be passed to the function where the thread begins execution. */ /* having a void pointer as an argument to the function start routine allows us to pass in any type of argument; having it as a return value allows the thread to return any type of result. */ #include #include typedef struct { int a; int b; } myarg_t; void *mythread(void *arg) { myarg_t *args = (myarg_t*) arg; printf(\"%d %d\\n\", args-a, args-b); return NULL; } int main(int argc, char* argv[]){ pthread_t p; myarg_t args = { 10, 20 }; int rc = pthread_create(\u0026p, NULL, mythread, \u0026args); } Thread Completion // What if you want to wait for a thread to complete? int pthread_join(pthread_t thread, void **value_ptr); /* This routine takes two arguments. The first is of type pthread t, and is used to specify which thread to wait for. This variable is initialized by the thread creation routine (when you pass a pointer to it as an argument to pthread create()); if you keep it around, you can use it to wait for that thread to terminate. The second argument is a pointer to the return value you expect to get back. Because the routine can return anything, it is defined to return a pointer to void; because the pthread join() routine changes the value of the passed in argument, you need to pass in a pointer to that value, not just the value itself. */ typedef struct { int a; int b; } myarg_t; typedef struct { int x; int y; } myret_t; void *mythread(void *arg) { myret_t *rvals = Malloc(sizeof(myret_t)); rvals-x = 1; rvals-y = 2; return (void *) rvals; } int main(int argc, char *argv[]) { pthread_t p; myret_t *rvals; myarg_t args = { 10, 20 }; Pthread_create(\u0026p, NULL, mythread, \u0026args); Pthread_join(p, (void **) \u0026rvals); printf(\"returned %d %d\\n\", rvals-x, rvals-y); free(rvals); return 0; } If we just create a thread with no args, we can pass NULL in as an arg. Similarly, we can pass NULL into pthread_join() if we care nothing about the return value. If we are just passing in a single value, then we don’t need to package it up as a arg. We should be extremely careful with how values are returned from a thread. Never return a pointer which refers to something allocated on the thread’s call stack. We have a pthread_create() followed bypthread_join(). We can use procedure call instead. Locks The routines should be easy to understand and use. When you have a region of code that is a critical section, and thus needs to be protected to ensure correct operation, locks are quite useful. int pthread_mutex_lock(pthread_mutex_t *mutex) int pthread_mutex_unlock(pthread_mutex_t *mutex) pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; // we must do the init for locks // or we can do as int rc = pthread_mutex_init(\u0026lock, NULL); assert(rc == 0); // always check success! pthread_mutex_lock(\u0026lock); x = x + 1; // or whatever your critical section is pthread_mutex_unlock(\u0026lock); Also, pthread mutex destroy() should be called afterwards. There","date":"2021-12-12","objectID":"/os_reading_note/:2:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#thread-creation-1"},{"categories":null,"content":"Interlude: Thread API Thread Creation #include int pthread_create(pthread_t* thread, const pthread_attr_t * attr, void* (*start_routine) (void*), void *arg); /* The second argument, attr, is used to specify any attributes this thread might have. Some examples include setting the stack size or perhaps in- formation about the scheduling priority of the thread. An attribute is initialized with a separate call to pthread attr init(); see the man- ual page for details. However, in most cases, the defaults will be fine; in this case, we will simply pass the value NULL in. The third argument is the most complex, but is really just asking: which function should this thread start running in? In C, we call this a function pointer, and this one tells us the following is expected: a function name (start routine), which is passed a single argument of type void * (as indicated in the parentheses after start routine), and which returns a value of type void * (i.e., a void pointer). Finally, the fourth argument, arg, is exactly the argument to be passed to the function where the thread begins execution. */ /* having a void pointer as an argument to the function start routine allows us to pass in any type of argument; having it as a return value allows the thread to return any type of result. */ #include #include typedef struct { int a; int b; } myarg_t; void *mythread(void *arg) { myarg_t *args = (myarg_t*) arg; printf(\"%d %d\\n\", args-a, args-b); return NULL; } int main(int argc, char* argv[]){ pthread_t p; myarg_t args = { 10, 20 }; int rc = pthread_create(\u0026p, NULL, mythread, \u0026args); } Thread Completion // What if you want to wait for a thread to complete? int pthread_join(pthread_t thread, void **value_ptr); /* This routine takes two arguments. The first is of type pthread t, and is used to specify which thread to wait for. This variable is initialized by the thread creation routine (when you pass a pointer to it as an argument to pthread create()); if you keep it around, you can use it to wait for that thread to terminate. The second argument is a pointer to the return value you expect to get back. Because the routine can return anything, it is defined to return a pointer to void; because the pthread join() routine changes the value of the passed in argument, you need to pass in a pointer to that value, not just the value itself. */ typedef struct { int a; int b; } myarg_t; typedef struct { int x; int y; } myret_t; void *mythread(void *arg) { myret_t *rvals = Malloc(sizeof(myret_t)); rvals-x = 1; rvals-y = 2; return (void *) rvals; } int main(int argc, char *argv[]) { pthread_t p; myret_t *rvals; myarg_t args = { 10, 20 }; Pthread_create(\u0026p, NULL, mythread, \u0026args); Pthread_join(p, (void **) \u0026rvals); printf(\"returned %d %d\\n\", rvals-x, rvals-y); free(rvals); return 0; } If we just create a thread with no args, we can pass NULL in as an arg. Similarly, we can pass NULL into pthread_join() if we care nothing about the return value. If we are just passing in a single value, then we don’t need to package it up as a arg. We should be extremely careful with how values are returned from a thread. Never return a pointer which refers to something allocated on the thread’s call stack. We have a pthread_create() followed bypthread_join(). We can use procedure call instead. Locks The routines should be easy to understand and use. When you have a region of code that is a critical section, and thus needs to be protected to ensure correct operation, locks are quite useful. int pthread_mutex_lock(pthread_mutex_t *mutex) int pthread_mutex_unlock(pthread_mutex_t *mutex) pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; // we must do the init for locks // or we can do as int rc = pthread_mutex_init(\u0026lock, NULL); assert(rc == 0); // always check success! pthread_mutex_lock(\u0026lock); x = x + 1; // or whatever your critical section is pthread_mutex_unlock(\u0026lock); Also, pthread mutex destroy() should be called afterwards. There","date":"2021-12-12","objectID":"/os_reading_note/:2:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#thread-completion"},{"categories":null,"content":"Interlude: Thread API Thread Creation #include int pthread_create(pthread_t* thread, const pthread_attr_t * attr, void* (*start_routine) (void*), void *arg); /* The second argument, attr, is used to specify any attributes this thread might have. Some examples include setting the stack size or perhaps in- formation about the scheduling priority of the thread. An attribute is initialized with a separate call to pthread attr init(); see the man- ual page for details. However, in most cases, the defaults will be fine; in this case, we will simply pass the value NULL in. The third argument is the most complex, but is really just asking: which function should this thread start running in? In C, we call this a function pointer, and this one tells us the following is expected: a function name (start routine), which is passed a single argument of type void * (as indicated in the parentheses after start routine), and which returns a value of type void * (i.e., a void pointer). Finally, the fourth argument, arg, is exactly the argument to be passed to the function where the thread begins execution. */ /* having a void pointer as an argument to the function start routine allows us to pass in any type of argument; having it as a return value allows the thread to return any type of result. */ #include #include typedef struct { int a; int b; } myarg_t; void *mythread(void *arg) { myarg_t *args = (myarg_t*) arg; printf(\"%d %d\\n\", args-a, args-b); return NULL; } int main(int argc, char* argv[]){ pthread_t p; myarg_t args = { 10, 20 }; int rc = pthread_create(\u0026p, NULL, mythread, \u0026args); } Thread Completion // What if you want to wait for a thread to complete? int pthread_join(pthread_t thread, void **value_ptr); /* This routine takes two arguments. The first is of type pthread t, and is used to specify which thread to wait for. This variable is initialized by the thread creation routine (when you pass a pointer to it as an argument to pthread create()); if you keep it around, you can use it to wait for that thread to terminate. The second argument is a pointer to the return value you expect to get back. Because the routine can return anything, it is defined to return a pointer to void; because the pthread join() routine changes the value of the passed in argument, you need to pass in a pointer to that value, not just the value itself. */ typedef struct { int a; int b; } myarg_t; typedef struct { int x; int y; } myret_t; void *mythread(void *arg) { myret_t *rvals = Malloc(sizeof(myret_t)); rvals-x = 1; rvals-y = 2; return (void *) rvals; } int main(int argc, char *argv[]) { pthread_t p; myret_t *rvals; myarg_t args = { 10, 20 }; Pthread_create(\u0026p, NULL, mythread, \u0026args); Pthread_join(p, (void **) \u0026rvals); printf(\"returned %d %d\\n\", rvals-x, rvals-y); free(rvals); return 0; } If we just create a thread with no args, we can pass NULL in as an arg. Similarly, we can pass NULL into pthread_join() if we care nothing about the return value. If we are just passing in a single value, then we don’t need to package it up as a arg. We should be extremely careful with how values are returned from a thread. Never return a pointer which refers to something allocated on the thread’s call stack. We have a pthread_create() followed bypthread_join(). We can use procedure call instead. Locks The routines should be easy to understand and use. When you have a region of code that is a critical section, and thus needs to be protected to ensure correct operation, locks are quite useful. int pthread_mutex_lock(pthread_mutex_t *mutex) int pthread_mutex_unlock(pthread_mutex_t *mutex) pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; // we must do the init for locks // or we can do as int rc = pthread_mutex_init(\u0026lock, NULL); assert(rc == 0); // always check success! pthread_mutex_lock(\u0026lock); x = x + 1; // or whatever your critical section is pthread_mutex_unlock(\u0026lock); Also, pthread mutex destroy() should be called afterwards. There","date":"2021-12-12","objectID":"/os_reading_note/:2:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#locks"},{"categories":null,"content":"Interlude: Thread API Thread Creation #include int pthread_create(pthread_t* thread, const pthread_attr_t * attr, void* (*start_routine) (void*), void *arg); /* The second argument, attr, is used to specify any attributes this thread might have. Some examples include setting the stack size or perhaps in- formation about the scheduling priority of the thread. An attribute is initialized with a separate call to pthread attr init(); see the man- ual page for details. However, in most cases, the defaults will be fine; in this case, we will simply pass the value NULL in. The third argument is the most complex, but is really just asking: which function should this thread start running in? In C, we call this a function pointer, and this one tells us the following is expected: a function name (start routine), which is passed a single argument of type void * (as indicated in the parentheses after start routine), and which returns a value of type void * (i.e., a void pointer). Finally, the fourth argument, arg, is exactly the argument to be passed to the function where the thread begins execution. */ /* having a void pointer as an argument to the function start routine allows us to pass in any type of argument; having it as a return value allows the thread to return any type of result. */ #include #include typedef struct { int a; int b; } myarg_t; void *mythread(void *arg) { myarg_t *args = (myarg_t*) arg; printf(\"%d %d\\n\", args-a, args-b); return NULL; } int main(int argc, char* argv[]){ pthread_t p; myarg_t args = { 10, 20 }; int rc = pthread_create(\u0026p, NULL, mythread, \u0026args); } Thread Completion // What if you want to wait for a thread to complete? int pthread_join(pthread_t thread, void **value_ptr); /* This routine takes two arguments. The first is of type pthread t, and is used to specify which thread to wait for. This variable is initialized by the thread creation routine (when you pass a pointer to it as an argument to pthread create()); if you keep it around, you can use it to wait for that thread to terminate. The second argument is a pointer to the return value you expect to get back. Because the routine can return anything, it is defined to return a pointer to void; because the pthread join() routine changes the value of the passed in argument, you need to pass in a pointer to that value, not just the value itself. */ typedef struct { int a; int b; } myarg_t; typedef struct { int x; int y; } myret_t; void *mythread(void *arg) { myret_t *rvals = Malloc(sizeof(myret_t)); rvals-x = 1; rvals-y = 2; return (void *) rvals; } int main(int argc, char *argv[]) { pthread_t p; myret_t *rvals; myarg_t args = { 10, 20 }; Pthread_create(\u0026p, NULL, mythread, \u0026args); Pthread_join(p, (void **) \u0026rvals); printf(\"returned %d %d\\n\", rvals-x, rvals-y); free(rvals); return 0; } If we just create a thread with no args, we can pass NULL in as an arg. Similarly, we can pass NULL into pthread_join() if we care nothing about the return value. If we are just passing in a single value, then we don’t need to package it up as a arg. We should be extremely careful with how values are returned from a thread. Never return a pointer which refers to something allocated on the thread’s call stack. We have a pthread_create() followed bypthread_join(). We can use procedure call instead. Locks The routines should be easy to understand and use. When you have a region of code that is a critical section, and thus needs to be protected to ensure correct operation, locks are quite useful. int pthread_mutex_lock(pthread_mutex_t *mutex) int pthread_mutex_unlock(pthread_mutex_t *mutex) pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; // we must do the init for locks // or we can do as int rc = pthread_mutex_init(\u0026lock, NULL); assert(rc == 0); // always check success! pthread_mutex_lock(\u0026lock); x = x + 1; // or whatever your critical section is pthread_mutex_unlock(\u0026lock); Also, pthread mutex destroy() should be called afterwards. There","date":"2021-12-12","objectID":"/os_reading_note/:2:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#condition-variables"},{"categories":null,"content":"Interlude: Thread API Thread Creation #include int pthread_create(pthread_t* thread, const pthread_attr_t * attr, void* (*start_routine) (void*), void *arg); /* The second argument, attr, is used to specify any attributes this thread might have. Some examples include setting the stack size or perhaps in- formation about the scheduling priority of the thread. An attribute is initialized with a separate call to pthread attr init(); see the man- ual page for details. However, in most cases, the defaults will be fine; in this case, we will simply pass the value NULL in. The third argument is the most complex, but is really just asking: which function should this thread start running in? In C, we call this a function pointer, and this one tells us the following is expected: a function name (start routine), which is passed a single argument of type void * (as indicated in the parentheses after start routine), and which returns a value of type void * (i.e., a void pointer). Finally, the fourth argument, arg, is exactly the argument to be passed to the function where the thread begins execution. */ /* having a void pointer as an argument to the function start routine allows us to pass in any type of argument; having it as a return value allows the thread to return any type of result. */ #include #include typedef struct { int a; int b; } myarg_t; void *mythread(void *arg) { myarg_t *args = (myarg_t*) arg; printf(\"%d %d\\n\", args-a, args-b); return NULL; } int main(int argc, char* argv[]){ pthread_t p; myarg_t args = { 10, 20 }; int rc = pthread_create(\u0026p, NULL, mythread, \u0026args); } Thread Completion // What if you want to wait for a thread to complete? int pthread_join(pthread_t thread, void **value_ptr); /* This routine takes two arguments. The first is of type pthread t, and is used to specify which thread to wait for. This variable is initialized by the thread creation routine (when you pass a pointer to it as an argument to pthread create()); if you keep it around, you can use it to wait for that thread to terminate. The second argument is a pointer to the return value you expect to get back. Because the routine can return anything, it is defined to return a pointer to void; because the pthread join() routine changes the value of the passed in argument, you need to pass in a pointer to that value, not just the value itself. */ typedef struct { int a; int b; } myarg_t; typedef struct { int x; int y; } myret_t; void *mythread(void *arg) { myret_t *rvals = Malloc(sizeof(myret_t)); rvals-x = 1; rvals-y = 2; return (void *) rvals; } int main(int argc, char *argv[]) { pthread_t p; myret_t *rvals; myarg_t args = { 10, 20 }; Pthread_create(\u0026p, NULL, mythread, \u0026args); Pthread_join(p, (void **) \u0026rvals); printf(\"returned %d %d\\n\", rvals-x, rvals-y); free(rvals); return 0; } If we just create a thread with no args, we can pass NULL in as an arg. Similarly, we can pass NULL into pthread_join() if we care nothing about the return value. If we are just passing in a single value, then we don’t need to package it up as a arg. We should be extremely careful with how values are returned from a thread. Never return a pointer which refers to something allocated on the thread’s call stack. We have a pthread_create() followed bypthread_join(). We can use procedure call instead. Locks The routines should be easy to understand and use. When you have a region of code that is a critical section, and thus needs to be protected to ensure correct operation, locks are quite useful. int pthread_mutex_lock(pthread_mutex_t *mutex) int pthread_mutex_unlock(pthread_mutex_t *mutex) pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; // we must do the init for locks // or we can do as int rc = pthread_mutex_init(\u0026lock, NULL); assert(rc == 0); // always check success! pthread_mutex_lock(\u0026lock); x = x + 1; // or whatever your critical section is pthread_mutex_unlock(\u0026lock); Also, pthread mutex destroy() should be called afterwards. There","date":"2021-12-12","objectID":"/os_reading_note/:2:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#compile-and-run"},{"categories":null,"content":"Interlude: Thread API Thread Creation #include int pthread_create(pthread_t* thread, const pthread_attr_t * attr, void* (*start_routine) (void*), void *arg); /* The second argument, attr, is used to specify any attributes this thread might have. Some examples include setting the stack size or perhaps in- formation about the scheduling priority of the thread. An attribute is initialized with a separate call to pthread attr init(); see the man- ual page for details. However, in most cases, the defaults will be fine; in this case, we will simply pass the value NULL in. The third argument is the most complex, but is really just asking: which function should this thread start running in? In C, we call this a function pointer, and this one tells us the following is expected: a function name (start routine), which is passed a single argument of type void * (as indicated in the parentheses after start routine), and which returns a value of type void * (i.e., a void pointer). Finally, the fourth argument, arg, is exactly the argument to be passed to the function where the thread begins execution. */ /* having a void pointer as an argument to the function start routine allows us to pass in any type of argument; having it as a return value allows the thread to return any type of result. */ #include #include typedef struct { int a; int b; } myarg_t; void *mythread(void *arg) { myarg_t *args = (myarg_t*) arg; printf(\"%d %d\\n\", args-a, args-b); return NULL; } int main(int argc, char* argv[]){ pthread_t p; myarg_t args = { 10, 20 }; int rc = pthread_create(\u0026p, NULL, mythread, \u0026args); } Thread Completion // What if you want to wait for a thread to complete? int pthread_join(pthread_t thread, void **value_ptr); /* This routine takes two arguments. The first is of type pthread t, and is used to specify which thread to wait for. This variable is initialized by the thread creation routine (when you pass a pointer to it as an argument to pthread create()); if you keep it around, you can use it to wait for that thread to terminate. The second argument is a pointer to the return value you expect to get back. Because the routine can return anything, it is defined to return a pointer to void; because the pthread join() routine changes the value of the passed in argument, you need to pass in a pointer to that value, not just the value itself. */ typedef struct { int a; int b; } myarg_t; typedef struct { int x; int y; } myret_t; void *mythread(void *arg) { myret_t *rvals = Malloc(sizeof(myret_t)); rvals-x = 1; rvals-y = 2; return (void *) rvals; } int main(int argc, char *argv[]) { pthread_t p; myret_t *rvals; myarg_t args = { 10, 20 }; Pthread_create(\u0026p, NULL, mythread, \u0026args); Pthread_join(p, (void **) \u0026rvals); printf(\"returned %d %d\\n\", rvals-x, rvals-y); free(rvals); return 0; } If we just create a thread with no args, we can pass NULL in as an arg. Similarly, we can pass NULL into pthread_join() if we care nothing about the return value. If we are just passing in a single value, then we don’t need to package it up as a arg. We should be extremely careful with how values are returned from a thread. Never return a pointer which refers to something allocated on the thread’s call stack. We have a pthread_create() followed bypthread_join(). We can use procedure call instead. Locks The routines should be easy to understand and use. When you have a region of code that is a critical section, and thus needs to be protected to ensure correct operation, locks are quite useful. int pthread_mutex_lock(pthread_mutex_t *mutex) int pthread_mutex_unlock(pthread_mutex_t *mutex) pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; // we must do the init for locks // or we can do as int rc = pthread_mutex_init(\u0026lock, NULL); assert(rc == 0); // always check success! pthread_mutex_lock(\u0026lock); x = x + 1; // or whatever your critical section is pthread_mutex_unlock(\u0026lock); Also, pthread mutex destroy() should be called afterwards. There","date":"2021-12-12","objectID":"/os_reading_note/:2:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#summary"},{"categories":null,"content":"Locks The Basic Idea ","date":"2021-12-12","objectID":"/os_reading_note/:2:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#locks-1"},{"categories":null,"content":"Locks The Basic Idea ","date":"2021-12-12","objectID":"/os_reading_note/:2:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note","uri":"/os_reading_note/#the-basic-idea"},{"categories":null,"content":"Memory Sharing can we share memory between processors? some page table entries need to be mapped to the same physical frames One can read, other can write?: yes, control bits can be diff Does the shared mem must be the same VA in the two processes?: no How can we know a shared mem can be reclaimed?: have a counter like a ref bit Copy on Write UNIX fork with copy on write copy the parent page table entries to the child, and turn on a special bit called copy-on-write bit on both parent and child’s PTEs Notice this is a OS bit (unused bit modified by the OS), hardware don’t know what it is! read is fine, but if anyone want to write… We need the help from the hardware which means we need to turn on the read-only bit on hardware. When a write happens on either process, exception happens due to read-only bit. Kernel handler will find out the exception is due to copy on write. Kernel then allocate a new physical frame, and copy the content from the current shared frame to the new frame. The offending PTE(whichever need to write) will point to this new frame. So now the shared mem is not shared. Clear copy-on-write bit on the offending PTE, decrease the ref bit for that shared frame, and if the is no body else sharing that shared mem(ref bit is 0), then also clear the other PTE’s copy-on-write bit. We here also need a reverse pointer pointing to the PTE who points to the shared mem. if ref bit is now 0, then follow reverse pointer to clear copy-on-write and read-only bit on the last process. copy-on-write can be implement in the software level since hardware don’t care. Notice that both hardware and software know PTE, but only kernel knows the Copy-on-write Fill On Demand Can I start running a program before its code is in physical memory? Design a page fault handler OS must book-keeping whereabouts of all the physical frames on persistent storage What info shall the kernel maintain? For every swapped out VPN of a process, the disk location of the page content when does such book-keeping info need to be create? the first time a virtual page is swapped out from a process Subsequent swap out of the page amy result in change of the mapping, depending on OS policy What needs to happen during the page fault handler? allocate a free physical frame to copy the page data to how to locate the page data on the persistent storage? use the book-keeping info stored. what kernel data needs to be updated? PTE entry for the virtual page can also update the TLB what other actions need to take place? load the page content from persistent storage into mem while waiting for disk I/O to complete, schedule a diff process to run, mark the offending process as “blocked”. How does the OS know which physical frame this offending PTE used to point? This question is wrong! You should not go to find that physical frame, it might have been used by another process. We should copy the disk data to a new physical frame. Or you might overwrite the data being used by other processor. Transfer the data stored on the persistent storage into mem. ","date":"2021-12-12","objectID":"/os_slides_note/:0:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#"},{"categories":null,"content":"Memory Sharing can we share memory between processors? some page table entries need to be mapped to the same physical frames One can read, other can write?: yes, control bits can be diff Does the shared mem must be the same VA in the two processes?: no How can we know a shared mem can be reclaimed?: have a counter like a ref bit Copy on Write UNIX fork with copy on write copy the parent page table entries to the child, and turn on a special bit called copy-on-write bit on both parent and child’s PTEs Notice this is a OS bit (unused bit modified by the OS), hardware don’t know what it is! read is fine, but if anyone want to write… We need the help from the hardware which means we need to turn on the read-only bit on hardware. When a write happens on either process, exception happens due to read-only bit. Kernel handler will find out the exception is due to copy on write. Kernel then allocate a new physical frame, and copy the content from the current shared frame to the new frame. The offending PTE(whichever need to write) will point to this new frame. So now the shared mem is not shared. Clear copy-on-write bit on the offending PTE, decrease the ref bit for that shared frame, and if the is no body else sharing that shared mem(ref bit is 0), then also clear the other PTE’s copy-on-write bit. We here also need a reverse pointer pointing to the PTE who points to the shared mem. if ref bit is now 0, then follow reverse pointer to clear copy-on-write and read-only bit on the last process. copy-on-write can be implement in the software level since hardware don’t care. Notice that both hardware and software know PTE, but only kernel knows the Copy-on-write Fill On Demand Can I start running a program before its code is in physical memory? Design a page fault handler OS must book-keeping whereabouts of all the physical frames on persistent storage What info shall the kernel maintain? For every swapped out VPN of a process, the disk location of the page content when does such book-keeping info need to be create? the first time a virtual page is swapped out from a process Subsequent swap out of the page amy result in change of the mapping, depending on OS policy What needs to happen during the page fault handler? allocate a free physical frame to copy the page data to how to locate the page data on the persistent storage? use the book-keeping info stored. what kernel data needs to be updated? PTE entry for the virtual page can also update the TLB what other actions need to take place? load the page content from persistent storage into mem while waiting for disk I/O to complete, schedule a diff process to run, mark the offending process as “blocked”. How does the OS know which physical frame this offending PTE used to point? This question is wrong! You should not go to find that physical frame, it might have been used by another process. We should copy the disk data to a new physical frame. Or you might overwrite the data being used by other processor. Transfer the data stored on the persistent storage into mem. ","date":"2021-12-12","objectID":"/os_slides_note/:0:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#memory-sharing"},{"categories":null,"content":"Memory Sharing can we share memory between processors? some page table entries need to be mapped to the same physical frames One can read, other can write?: yes, control bits can be diff Does the shared mem must be the same VA in the two processes?: no How can we know a shared mem can be reclaimed?: have a counter like a ref bit Copy on Write UNIX fork with copy on write copy the parent page table entries to the child, and turn on a special bit called copy-on-write bit on both parent and child’s PTEs Notice this is a OS bit (unused bit modified by the OS), hardware don’t know what it is! read is fine, but if anyone want to write… We need the help from the hardware which means we need to turn on the read-only bit on hardware. When a write happens on either process, exception happens due to read-only bit. Kernel handler will find out the exception is due to copy on write. Kernel then allocate a new physical frame, and copy the content from the current shared frame to the new frame. The offending PTE(whichever need to write) will point to this new frame. So now the shared mem is not shared. Clear copy-on-write bit on the offending PTE, decrease the ref bit for that shared frame, and if the is no body else sharing that shared mem(ref bit is 0), then also clear the other PTE’s copy-on-write bit. We here also need a reverse pointer pointing to the PTE who points to the shared mem. if ref bit is now 0, then follow reverse pointer to clear copy-on-write and read-only bit on the last process. copy-on-write can be implement in the software level since hardware don’t care. Notice that both hardware and software know PTE, but only kernel knows the Copy-on-write Fill On Demand Can I start running a program before its code is in physical memory? Design a page fault handler OS must book-keeping whereabouts of all the physical frames on persistent storage What info shall the kernel maintain? For every swapped out VPN of a process, the disk location of the page content when does such book-keeping info need to be create? the first time a virtual page is swapped out from a process Subsequent swap out of the page amy result in change of the mapping, depending on OS policy What needs to happen during the page fault handler? allocate a free physical frame to copy the page data to how to locate the page data on the persistent storage? use the book-keeping info stored. what kernel data needs to be updated? PTE entry for the virtual page can also update the TLB what other actions need to take place? load the page content from persistent storage into mem while waiting for disk I/O to complete, schedule a diff process to run, mark the offending process as “blocked”. How does the OS know which physical frame this offending PTE used to point? This question is wrong! You should not go to find that physical frame, it might have been used by another process. We should copy the disk data to a new physical frame. Or you might overwrite the data being used by other processor. Transfer the data stored on the persistent storage into mem. ","date":"2021-12-12","objectID":"/os_slides_note/:0:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#copy-on-write"},{"categories":null,"content":"Memory Sharing can we share memory between processors? some page table entries need to be mapped to the same physical frames One can read, other can write?: yes, control bits can be diff Does the shared mem must be the same VA in the two processes?: no How can we know a shared mem can be reclaimed?: have a counter like a ref bit Copy on Write UNIX fork with copy on write copy the parent page table entries to the child, and turn on a special bit called copy-on-write bit on both parent and child’s PTEs Notice this is a OS bit (unused bit modified by the OS), hardware don’t know what it is! read is fine, but if anyone want to write… We need the help from the hardware which means we need to turn on the read-only bit on hardware. When a write happens on either process, exception happens due to read-only bit. Kernel handler will find out the exception is due to copy on write. Kernel then allocate a new physical frame, and copy the content from the current shared frame to the new frame. The offending PTE(whichever need to write) will point to this new frame. So now the shared mem is not shared. Clear copy-on-write bit on the offending PTE, decrease the ref bit for that shared frame, and if the is no body else sharing that shared mem(ref bit is 0), then also clear the other PTE’s copy-on-write bit. We here also need a reverse pointer pointing to the PTE who points to the shared mem. if ref bit is now 0, then follow reverse pointer to clear copy-on-write and read-only bit on the last process. copy-on-write can be implement in the software level since hardware don’t care. Notice that both hardware and software know PTE, but only kernel knows the Copy-on-write Fill On Demand Can I start running a program before its code is in physical memory? Design a page fault handler OS must book-keeping whereabouts of all the physical frames on persistent storage What info shall the kernel maintain? For every swapped out VPN of a process, the disk location of the page content when does such book-keeping info need to be create? the first time a virtual page is swapped out from a process Subsequent swap out of the page amy result in change of the mapping, depending on OS policy What needs to happen during the page fault handler? allocate a free physical frame to copy the page data to how to locate the page data on the persistent storage? use the book-keeping info stored. what kernel data needs to be updated? PTE entry for the virtual page can also update the TLB what other actions need to take place? load the page content from persistent storage into mem while waiting for disk I/O to complete, schedule a diff process to run, mark the offending process as “blocked”. How does the OS know which physical frame this offending PTE used to point? This question is wrong! You should not go to find that physical frame, it might have been used by another process. We should copy the disk data to a new physical frame. Or you might overwrite the data being used by other processor. Transfer the data stored on the persistent storage into mem. ","date":"2021-12-12","objectID":"/os_slides_note/:0:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#notice-that-both-hardware-and-software-know-pte-but-only-kernel-knows-the-copy-on-write"},{"categories":null,"content":"Memory Sharing can we share memory between processors? some page table entries need to be mapped to the same physical frames One can read, other can write?: yes, control bits can be diff Does the shared mem must be the same VA in the two processes?: no How can we know a shared mem can be reclaimed?: have a counter like a ref bit Copy on Write UNIX fork with copy on write copy the parent page table entries to the child, and turn on a special bit called copy-on-write bit on both parent and child’s PTEs Notice this is a OS bit (unused bit modified by the OS), hardware don’t know what it is! read is fine, but if anyone want to write… We need the help from the hardware which means we need to turn on the read-only bit on hardware. When a write happens on either process, exception happens due to read-only bit. Kernel handler will find out the exception is due to copy on write. Kernel then allocate a new physical frame, and copy the content from the current shared frame to the new frame. The offending PTE(whichever need to write) will point to this new frame. So now the shared mem is not shared. Clear copy-on-write bit on the offending PTE, decrease the ref bit for that shared frame, and if the is no body else sharing that shared mem(ref bit is 0), then also clear the other PTE’s copy-on-write bit. We here also need a reverse pointer pointing to the PTE who points to the shared mem. if ref bit is now 0, then follow reverse pointer to clear copy-on-write and read-only bit on the last process. copy-on-write can be implement in the software level since hardware don’t care. Notice that both hardware and software know PTE, but only kernel knows the Copy-on-write Fill On Demand Can I start running a program before its code is in physical memory? Design a page fault handler OS must book-keeping whereabouts of all the physical frames on persistent storage What info shall the kernel maintain? For every swapped out VPN of a process, the disk location of the page content when does such book-keeping info need to be create? the first time a virtual page is swapped out from a process Subsequent swap out of the page amy result in change of the mapping, depending on OS policy What needs to happen during the page fault handler? allocate a free physical frame to copy the page data to how to locate the page data on the persistent storage? use the book-keeping info stored. what kernel data needs to be updated? PTE entry for the virtual page can also update the TLB what other actions need to take place? load the page content from persistent storage into mem while waiting for disk I/O to complete, schedule a diff process to run, mark the offending process as “blocked”. How does the OS know which physical frame this offending PTE used to point? This question is wrong! You should not go to find that physical frame, it might have been used by another process. We should copy the disk data to a new physical frame. Or you might overwrite the data being used by other processor. Transfer the data stored on the persistent storage into mem. ","date":"2021-12-12","objectID":"/os_slides_note/:0:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#fill-on-demand"},{"categories":null,"content":"Memory Sharing can we share memory between processors? some page table entries need to be mapped to the same physical frames One can read, other can write?: yes, control bits can be diff Does the shared mem must be the same VA in the two processes?: no How can we know a shared mem can be reclaimed?: have a counter like a ref bit Copy on Write UNIX fork with copy on write copy the parent page table entries to the child, and turn on a special bit called copy-on-write bit on both parent and child’s PTEs Notice this is a OS bit (unused bit modified by the OS), hardware don’t know what it is! read is fine, but if anyone want to write… We need the help from the hardware which means we need to turn on the read-only bit on hardware. When a write happens on either process, exception happens due to read-only bit. Kernel handler will find out the exception is due to copy on write. Kernel then allocate a new physical frame, and copy the content from the current shared frame to the new frame. The offending PTE(whichever need to write) will point to this new frame. So now the shared mem is not shared. Clear copy-on-write bit on the offending PTE, decrease the ref bit for that shared frame, and if the is no body else sharing that shared mem(ref bit is 0), then also clear the other PTE’s copy-on-write bit. We here also need a reverse pointer pointing to the PTE who points to the shared mem. if ref bit is now 0, then follow reverse pointer to clear copy-on-write and read-only bit on the last process. copy-on-write can be implement in the software level since hardware don’t care. Notice that both hardware and software know PTE, but only kernel knows the Copy-on-write Fill On Demand Can I start running a program before its code is in physical memory? Design a page fault handler OS must book-keeping whereabouts of all the physical frames on persistent storage What info shall the kernel maintain? For every swapped out VPN of a process, the disk location of the page content when does such book-keeping info need to be create? the first time a virtual page is swapped out from a process Subsequent swap out of the page amy result in change of the mapping, depending on OS policy What needs to happen during the page fault handler? allocate a free physical frame to copy the page data to how to locate the page data on the persistent storage? use the book-keeping info stored. what kernel data needs to be updated? PTE entry for the virtual page can also update the TLB what other actions need to take place? load the page content from persistent storage into mem while waiting for disk I/O to complete, schedule a diff process to run, mark the offending process as “blocked”. How does the OS know which physical frame this offending PTE used to point? This question is wrong! You should not go to find that physical frame, it might have been used by another process. We should copy the disk data to a new physical frame. Or you might overwrite the data being used by other processor. Transfer the data stored on the persistent storage into mem. ","date":"2021-12-12","objectID":"/os_slides_note/:0:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#design-a-page-fault-handler"},{"categories":null,"content":"Lecture 16: Concurrency II Synchronization: Mutual Exclusive + Ordering By the definition of thread, its access to shared memory space is unfettered. And its scheduling is completely unpredictable. So, from this angle, all threads race to access data! ==\u003e very bad design Defs: Race condition Mutual exclusion Lock Critical section ","date":"2021-12-12","objectID":"/os_slides_note/:1:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#lecture-16-concurrency-ii"},{"categories":null,"content":"Locks Lock::acquire Lock::release Milk problem: Liveness: Someone buys if needed Safety: At most one person buys Fairness: Everyone has fair chances to get the lock 1 \u0026 2 are the most important, 3 is good to have. If you only have a single processor, we should disable the interrupt, or we may have a dead lock. ","date":"2021-12-12","objectID":"/os_slides_note/:1:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#locks"},{"categories":null,"content":"ONLY IN LECTURE Once you acquire the lock, you can assume that it’s in persistant state. // Basic programming pattern for Critical section Lock::acquire Manipulation of shared data Lock::release One lock per shared data To be effective, all threads must apply locking on the data The difference between Concurrency access in OS v.s. program: OS does not trust others. The program can behave like designed, there is no enforcement. So Synchronization is kinda weird for the OS since it’s more of a cooperation rather than enforcement. The lock does not really lock it, it just signal others that the data is occupied by me, do not come. When you have some threads, you’ll never know when they will be executed. Implementing Threads: kernel Threads: to the kernel, a kernel thread and a single threaded user processor looks quite similar. multithreads processes using kernel threads User-level threads We are going to focus on the first possibility. Thread data structure: TCB and stack TCB –\u003e stack info, saved registers, thread meta data shared state are shared among all threads: code global vars, heap Thread Context Switch Voluntary yield and join Involuntary Interrupt or exception Some other thread is higher priority How does it happen? return to a new place The key trick: void thread_switch(oldThreadTCB, newTheadTCB){ pushad; oldThreadTCB-\u003esp = %esp; %esp = newThreadTCB-\u003esp; popad; return; } How to implement thread_yield()? in the thread_yield(), there is thread_switch() in the yield() after the swith(), the it’s resumed to the old TCB NOT THE chosen TCB. And the eableInterrupt() is not re-eable the one at the beginning of the yield(), but that in another thread. the yield() is very fast so that do not worry about the length of the time of interrupt.(for single core) void thread_yield(){ TCB *previous_runningTCB, *chosenTCB, *finishedTCB; //Prevent an interrupt from stopping us in the middle of a switch disableInterrupts(); // choose another TCB from the ready list chosenTCB = readyList.getNextThread(); if (chosenTCB == NULL){ //Nothing, go back to the original thread } else{ runningThread-\u003estate = ready; readyList.add(runningThread); previous_runningTCB = runningThread; runningThread = chosenTCB; thread_switch(*previous_runningTCB, chosenTCB); runningThread-\u003estate = running; } while((finishedTCB = finishedList-\u003egetNextThread()) != NULL){ delete finishedTCB-\u003estack; delete finishedTCB; } enableInterrupts(); } why disable the interrupt? to protect the shared data: ready list! If this is not kernel code, we have to use other tricks like locks. Never acquire a lock in interrupt handler!!! because if the interrupt handler tries to acquire a lock that being held by others, DEAD LOK!! In this case, one CPU core own their own ready list. Or there still may be a race condition on the ready list since the disableInterrupt() only works on the current CPU. The downside is that if a thread is ready but other cores are not doing anything. ","date":"2021-12-12","objectID":"/os_slides_note/:1:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#only-in-lecture"},{"categories":null,"content":"ONLY IN LECTURE Once you acquire the lock, you can assume that it’s in persistant state. // Basic programming pattern for Critical section Lock::acquire Manipulation of shared data Lock::release One lock per shared data To be effective, all threads must apply locking on the data The difference between Concurrency access in OS v.s. program: OS does not trust others. The program can behave like designed, there is no enforcement. So Synchronization is kinda weird for the OS since it’s more of a cooperation rather than enforcement. The lock does not really lock it, it just signal others that the data is occupied by me, do not come. When you have some threads, you’ll never know when they will be executed. Implementing Threads: kernel Threads: to the kernel, a kernel thread and a single threaded user processor looks quite similar. multithreads processes using kernel threads User-level threads We are going to focus on the first possibility. Thread data structure: TCB and stack TCB – stack info, saved registers, thread meta data shared state are shared among all threads: code global vars, heap Thread Context Switch Voluntary yield and join Involuntary Interrupt or exception Some other thread is higher priority How does it happen? return to a new place The key trick: void thread_switch(oldThreadTCB, newTheadTCB){ pushad; oldThreadTCB-sp = %esp; %esp = newThreadTCB-sp; popad; return; } How to implement thread_yield()? in the thread_yield(), there is thread_switch() in the yield() after the swith(), the it’s resumed to the old TCB NOT THE chosen TCB. And the eableInterrupt() is not re-eable the one at the beginning of the yield(), but that in another thread. the yield() is very fast so that do not worry about the length of the time of interrupt.(for single core) void thread_yield(){ TCB *previous_runningTCB, *chosenTCB, *finishedTCB; //Prevent an interrupt from stopping us in the middle of a switch disableInterrupts(); // choose another TCB from the ready list chosenTCB = readyList.getNextThread(); if (chosenTCB == NULL){ //Nothing, go back to the original thread } else{ runningThread-state = ready; readyList.add(runningThread); previous_runningTCB = runningThread; runningThread = chosenTCB; thread_switch(*previous_runningTCB, chosenTCB); runningThread-state = running; } while((finishedTCB = finishedList-getNextThread()) != NULL){ delete finishedTCB-stack; delete finishedTCB; } enableInterrupts(); } why disable the interrupt? to protect the shared data: ready list! If this is not kernel code, we have to use other tricks like locks. Never acquire a lock in interrupt handler!!! because if the interrupt handler tries to acquire a lock that being held by others, DEAD LOK!! In this case, one CPU core own their own ready list. Or there still may be a race condition on the ready list since the disableInterrupt() only works on the current CPU. The downside is that if a thread is ready but other cores are not doing anything. ","date":"2021-12-12","objectID":"/os_slides_note/:1:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#how-to-implement-thread_yield"},{"categories":null,"content":"ONLY IN LECTURE Once you acquire the lock, you can assume that it’s in persistant state. // Basic programming pattern for Critical section Lock::acquire Manipulation of shared data Lock::release One lock per shared data To be effective, all threads must apply locking on the data The difference between Concurrency access in OS v.s. program: OS does not trust others. The program can behave like designed, there is no enforcement. So Synchronization is kinda weird for the OS since it’s more of a cooperation rather than enforcement. The lock does not really lock it, it just signal others that the data is occupied by me, do not come. When you have some threads, you’ll never know when they will be executed. Implementing Threads: kernel Threads: to the kernel, a kernel thread and a single threaded user processor looks quite similar. multithreads processes using kernel threads User-level threads We are going to focus on the first possibility. Thread data structure: TCB and stack TCB – stack info, saved registers, thread meta data shared state are shared among all threads: code global vars, heap Thread Context Switch Voluntary yield and join Involuntary Interrupt or exception Some other thread is higher priority How does it happen? return to a new place The key trick: void thread_switch(oldThreadTCB, newTheadTCB){ pushad; oldThreadTCB-sp = %esp; %esp = newThreadTCB-sp; popad; return; } How to implement thread_yield()? in the thread_yield(), there is thread_switch() in the yield() after the swith(), the it’s resumed to the old TCB NOT THE chosen TCB. And the eableInterrupt() is not re-eable the one at the beginning of the yield(), but that in another thread. the yield() is very fast so that do not worry about the length of the time of interrupt.(for single core) void thread_yield(){ TCB *previous_runningTCB, *chosenTCB, *finishedTCB; //Prevent an interrupt from stopping us in the middle of a switch disableInterrupts(); // choose another TCB from the ready list chosenTCB = readyList.getNextThread(); if (chosenTCB == NULL){ //Nothing, go back to the original thread } else{ runningThread-state = ready; readyList.add(runningThread); previous_runningTCB = runningThread; runningThread = chosenTCB; thread_switch(*previous_runningTCB, chosenTCB); runningThread-state = running; } while((finishedTCB = finishedList-getNextThread()) != NULL){ delete finishedTCB-stack; delete finishedTCB; } enableInterrupts(); } why disable the interrupt? to protect the shared data: ready list! If this is not kernel code, we have to use other tricks like locks. Never acquire a lock in interrupt handler!!! because if the interrupt handler tries to acquire a lock that being held by others, DEAD LOK!! In this case, one CPU core own their own ready list. Or there still may be a race condition on the ready list since the disableInterrupt() only works on the current CPU. The downside is that if a thread is ready but other cores are not doing anything. ","date":"2021-12-12","objectID":"/os_slides_note/:1:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#why-disable-the-interrupt"},{"categories":null,"content":"ONLY IN LECTURE Once you acquire the lock, you can assume that it’s in persistant state. // Basic programming pattern for Critical section Lock::acquire Manipulation of shared data Lock::release One lock per shared data To be effective, all threads must apply locking on the data The difference between Concurrency access in OS v.s. program: OS does not trust others. The program can behave like designed, there is no enforcement. So Synchronization is kinda weird for the OS since it’s more of a cooperation rather than enforcement. The lock does not really lock it, it just signal others that the data is occupied by me, do not come. When you have some threads, you’ll never know when they will be executed. Implementing Threads: kernel Threads: to the kernel, a kernel thread and a single threaded user processor looks quite similar. multithreads processes using kernel threads User-level threads We are going to focus on the first possibility. Thread data structure: TCB and stack TCB – stack info, saved registers, thread meta data shared state are shared among all threads: code global vars, heap Thread Context Switch Voluntary yield and join Involuntary Interrupt or exception Some other thread is higher priority How does it happen? return to a new place The key trick: void thread_switch(oldThreadTCB, newTheadTCB){ pushad; oldThreadTCB-sp = %esp; %esp = newThreadTCB-sp; popad; return; } How to implement thread_yield()? in the thread_yield(), there is thread_switch() in the yield() after the swith(), the it’s resumed to the old TCB NOT THE chosen TCB. And the eableInterrupt() is not re-eable the one at the beginning of the yield(), but that in another thread. the yield() is very fast so that do not worry about the length of the time of interrupt.(for single core) void thread_yield(){ TCB *previous_runningTCB, *chosenTCB, *finishedTCB; //Prevent an interrupt from stopping us in the middle of a switch disableInterrupts(); // choose another TCB from the ready list chosenTCB = readyList.getNextThread(); if (chosenTCB == NULL){ //Nothing, go back to the original thread } else{ runningThread-state = ready; readyList.add(runningThread); previous_runningTCB = runningThread; runningThread = chosenTCB; thread_switch(*previous_runningTCB, chosenTCB); runningThread-state = running; } while((finishedTCB = finishedList-getNextThread()) != NULL){ delete finishedTCB-stack; delete finishedTCB; } enableInterrupts(); } why disable the interrupt? to protect the shared data: ready list! If this is not kernel code, we have to use other tricks like locks. Never acquire a lock in interrupt handler!!! because if the interrupt handler tries to acquire a lock that being held by others, DEAD LOK!! In this case, one CPU core own their own ready list. Or there still may be a race condition on the ready list since the disableInterrupt() only works on the current CPU. The downside is that if a thread is ready but other cores are not doing anything. ","date":"2021-12-12","objectID":"/os_slides_note/:1:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#never-acquire-a-lock-in-interrupt-handler"},{"categories":null,"content":"Lecture 18 EXAM: what does this line do? Is there anything wrong? yield() switch() join() … we should be able to implement those. All these funtions have nothing special that make them can be only done in kernel, you can implement similar things in user mode with proper coding. Thread_create() need a wrapper, to call the create() and exit once created, since we might not use it immediately after creating it. allocate TCB allocate stack build stack frame for base of stack (stub) put thread on ready list will run sometime later (maybe right away) thread_create(thread_t thread, void (*func)(int), int arg){ TCB* tcb = new TCB(); thread-\u003etcb = tcb; tcb-\u003estack_size = INIT_STACK_SIZE; tcb-\u003estack = new Stack(INIT_STACK_SIZE); // now the TCB and stack are allocated tcb-\u003esp = stack + INIT_STACK_SIZE; tcb-\u003epc = stub; // init the register to let the program run at the stub *(tcb-\u003esp--) = stub; *(tcb-\u003esp) = func; tcb-\u003esp -= SizeOfPopad; tcb-\u003estate = READY; readyList.add(tcb); // put tcb on ready readyList } stub(func, args){ (*func)(args); thread_exit(); } Notice here the tcb’s member vars: tcb, stack_size, stack, sp, pc, state… the create() function pretend it’s resuming from another thread, so that every function is in a unified format, which means they can all use thread_swith(). When it gets resumed, it needs certain words int the stack to fufill the popad instructions in thread_switch(). Thus, we need a fake funcion like stub() to create that stack space. The popad will add those random vals into the stack but it doesn’t matter since a new thread should not trust any un-init vals. Subtlety thread_create() puts new thread on ready list When it first runs, some thread needs to call thread_switch() save old thread state to stack pop the new thread state from stack into the registers Set up new thread’s stack as if it has saved its state in thread_switch() return to the sub at the base address of the stack to run the func ","date":"2021-12-12","objectID":"/os_slides_note/:2:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#lecture-18"},{"categories":null,"content":"Lecture 18 EXAM: what does this line do? Is there anything wrong? yield() switch() join() … we should be able to implement those. All these funtions have nothing special that make them can be only done in kernel, you can implement similar things in user mode with proper coding. Thread_create() need a wrapper, to call the create() and exit once created, since we might not use it immediately after creating it. allocate TCB allocate stack build stack frame for base of stack (stub) put thread on ready list will run sometime later (maybe right away) thread_create(thread_t thread, void (*func)(int), int arg){ TCB* tcb = new TCB(); thread-tcb = tcb; tcb-stack_size = INIT_STACK_SIZE; tcb-stack = new Stack(INIT_STACK_SIZE); // now the TCB and stack are allocated tcb-sp = stack + INIT_STACK_SIZE; tcb-pc = stub; // init the register to let the program run at the stub *(tcb-sp--) = stub; *(tcb-sp) = func; tcb-sp -= SizeOfPopad; tcb-state = READY; readyList.add(tcb); // put tcb on ready readyList } stub(func, args){ (*func)(args); thread_exit(); } Notice here the tcb’s member vars: tcb, stack_size, stack, sp, pc, state… the create() function pretend it’s resuming from another thread, so that every function is in a unified format, which means they can all use thread_swith(). When it gets resumed, it needs certain words int the stack to fufill the popad instructions in thread_switch(). Thus, we need a fake funcion like stub() to create that stack space. The popad will add those random vals into the stack but it doesn’t matter since a new thread should not trust any un-init vals. Subtlety thread_create() puts new thread on ready list When it first runs, some thread needs to call thread_switch() save old thread state to stack pop the new thread state from stack into the registers Set up new thread’s stack as if it has saved its state in thread_switch() return to the sub at the base address of the stack to run the func ","date":"2021-12-12","objectID":"/os_slides_note/:2:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#thread_create"},{"categories":null,"content":"Lecture 18 EXAM: what does this line do? Is there anything wrong? yield() switch() join() … we should be able to implement those. All these funtions have nothing special that make them can be only done in kernel, you can implement similar things in user mode with proper coding. Thread_create() need a wrapper, to call the create() and exit once created, since we might not use it immediately after creating it. allocate TCB allocate stack build stack frame for base of stack (stub) put thread on ready list will run sometime later (maybe right away) thread_create(thread_t thread, void (*func)(int), int arg){ TCB* tcb = new TCB(); thread-tcb = tcb; tcb-stack_size = INIT_STACK_SIZE; tcb-stack = new Stack(INIT_STACK_SIZE); // now the TCB and stack are allocated tcb-sp = stack + INIT_STACK_SIZE; tcb-pc = stub; // init the register to let the program run at the stub *(tcb-sp--) = stub; *(tcb-sp) = func; tcb-sp -= SizeOfPopad; tcb-state = READY; readyList.add(tcb); // put tcb on ready readyList } stub(func, args){ (*func)(args); thread_exit(); } Notice here the tcb’s member vars: tcb, stack_size, stack, sp, pc, state… the create() function pretend it’s resuming from another thread, so that every function is in a unified format, which means they can all use thread_swith(). When it gets resumed, it needs certain words int the stack to fufill the popad instructions in thread_switch(). Thus, we need a fake funcion like stub() to create that stack space. The popad will add those random vals into the stack but it doesn’t matter since a new thread should not trust any un-init vals. Subtlety thread_create() puts new thread on ready list When it first runs, some thread needs to call thread_switch() save old thread state to stack pop the new thread state from stack into the registers Set up new thread’s stack as if it has saved its state in thread_switch() return to the sub at the base address of the stack to run the func ","date":"2021-12-12","objectID":"/os_slides_note/:2:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#subtlety"},{"categories":null,"content":"Involuntary timer or I/O interrput lernel can decide that some other thread should run once you are in the kernel, the process are seen as threads. notice that user process has the kernel stack and the user stack, but the kernel thread only has the kernel stack we can use switch() as context switch simple version End of interrupt handler calls thread_switch() when resumed, return from handler resumes kernel thread or uer process. Thus, processor context is saved/restored twice.(once by thread switch, and once by interrupt handler) ![After an interrupt at x86](img/After an interrupt at x86.png) It seems that we need to do sth like in the yield(), call the scheduler to run to give us a chosen pcb and call swith(), in the switch() we use iret after popad. However, think about this. Since you are already in an interrupt, we have states saved in the trap frame, on top of that we also do pushad in the swith(), which is redundant. Resuming a thread from the switch() which is also interrupted let the resumed thread start at the interrupt handler, which will next do the popad again followed by an iret. Faster Thread/Process Switch Essentially tail call elimination \u003c== if the context switch is cause by interrupt interrupt handler will saved all the states decide to run a new thread throw away current state of interrupt handler you DO NOT really call thread_switch() and create a new frame for it, which would save the registers and return address. Instead it become a jump. reuse the thread’s saved state when entering the interrupt handler Set saved stack pointer to trap frame (on x86, including the saved registers and six vals) On resume, pops trap frame to restore interrupt thread This requires thread_switch() use the same format stack frame as trap frame. On x86, this also means thread_switch() will use iret, instead of ret instruction. Now both voluntary and involuntary context switch are all looks like resume from an interrupt so that they are uniform. Can we support user level threads without the help of kernel? reason: kernel only has one way of scheduling, which is less flexible for the need from app to app. In this case, the kernel may not know the existence of the user threads since the thread context swith is not down throught kernel. What if there is an I/O request and the kernel block the whole processor? How do we preempt other threads when kernel does not kick in (one way to do is to use yield() everywhere to let threads balance the workload but this is unpractical)? ","date":"2021-12-12","objectID":"/os_slides_note/:2:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#involuntary"},{"categories":null,"content":"Involuntary timer or I/O interrput lernel can decide that some other thread should run once you are in the kernel, the process are seen as threads. notice that user process has the kernel stack and the user stack, but the kernel thread only has the kernel stack we can use switch() as context switch simple version End of interrupt handler calls thread_switch() when resumed, return from handler resumes kernel thread or uer process. Thus, processor context is saved/restored twice.(once by thread switch, and once by interrupt handler) ![After an interrupt at x86](img/After an interrupt at x86.png) It seems that we need to do sth like in the yield(), call the scheduler to run to give us a chosen pcb and call swith(), in the switch() we use iret after popad. However, think about this. Since you are already in an interrupt, we have states saved in the trap frame, on top of that we also do pushad in the swith(), which is redundant. Resuming a thread from the switch() which is also interrupted let the resumed thread start at the interrupt handler, which will next do the popad again followed by an iret. Faster Thread/Process Switch Essentially tail call elimination ","date":"2021-12-12","objectID":"/os_slides_note/:2:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#faster-threadprocess-switch"},{"categories":null,"content":"Involuntary timer or I/O interrput lernel can decide that some other thread should run once you are in the kernel, the process are seen as threads. notice that user process has the kernel stack and the user stack, but the kernel thread only has the kernel stack we can use switch() as context switch simple version End of interrupt handler calls thread_switch() when resumed, return from handler resumes kernel thread or uer process. Thus, processor context is saved/restored twice.(once by thread switch, and once by interrupt handler) ![After an interrupt at x86](img/After an interrupt at x86.png) It seems that we need to do sth like in the yield(), call the scheduler to run to give us a chosen pcb and call swith(), in the switch() we use iret after popad. However, think about this. Since you are already in an interrupt, we have states saved in the trap frame, on top of that we also do pushad in the swith(), which is redundant. Resuming a thread from the switch() which is also interrupted let the resumed thread start at the interrupt handler, which will next do the popad again followed by an iret. Faster Thread/Process Switch Essentially tail call elimination ","date":"2021-12-12","objectID":"/os_slides_note/:2:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#can-we-support-user-level-threads-without-the-help-of-kernel"},{"categories":null,"content":"Involuntary timer or I/O interrput lernel can decide that some other thread should run once you are in the kernel, the process are seen as threads. notice that user process has the kernel stack and the user stack, but the kernel thread only has the kernel stack we can use switch() as context switch simple version End of interrupt handler calls thread_switch() when resumed, return from handler resumes kernel thread or uer process. Thus, processor context is saved/restored twice.(once by thread switch, and once by interrupt handler) ![After an interrupt at x86](img/After an interrupt at x86.png) It seems that we need to do sth like in the yield(), call the scheduler to run to give us a chosen pcb and call swith(), in the switch() we use iret after popad. However, think about this. Since you are already in an interrupt, we have states saved in the trap frame, on top of that we also do pushad in the swith(), which is redundant. Resuming a thread from the switch() which is also interrupted let the resumed thread start at the interrupt handler, which will next do the popad again followed by an iret. Faster Thread/Process Switch Essentially tail call elimination ","date":"2021-12-12","objectID":"/os_slides_note/:2:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#reason-kernel-only-has-one-way-of-scheduling-which-is-less-flexible-for-the-need-from-app-to-app"},{"categories":null,"content":"Lecture 19 We can use signal. Like in the terminal \u003cctr + c\u003e is s signal to do a up call, and kernel will ask the user processor to do something. And then we can write our own signal handler. For example, kernel has timer interrupt, so kernel can decide to send a signal to a process who need it periodically. /* An upcall is a mechanism that allows the kernel to execute a function in userspace, and potentially be returned information as a result. An upcall is like a signal, except that the kernel may use it at any time, for any purpose, including in an interrupt handler. This means that an upcall can potentially destroy the behaviour of the kernel. If an interrupt handler decides to ask a user space function for some information, and the function page faults or does blocking IO, your kernel is quite likely trashed. So, upcalls aren't there for everyday software development. But for specific purposes, they can be extremely useful. */ Green thread(early JAVA) user level lib, within a process lub does thread context swtich preemption via upcall/UNIX signal on timer interrupt use multiple processes for multi-core parallelism shared mem region mapped into each process Since the process can change the user level stack for the threads Say if we have a multi-core CPU, this does not work. Because when you want to let a core to take care of a thread, it needs kernel to kick in. However, the green thread does everthing at user level. The solution is to use shared mem region mapped into each process. More modern way Scheduler activations kernel allocates processors to user-level thread lib (after the specific core take over, the scheduling will be done in that core, which are decided by the user level processors in that core, and we have the following 2 bullet points) thread lib implements context switch thread lib decides what thread to run next upcall whenever kernel needs a user level scheduling decision (if there is I/O interrupt, kernel will ask the process to do sth or it will be blocked) process assigned a new processor processor removed from process sys call blocks in kernel ","date":"2021-12-12","objectID":"/os_slides_note/:3:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#lecture-19"},{"categories":null,"content":"Lecture 19 We can use signal. Like in the terminal is s signal to do a up call, and kernel will ask the user processor to do something. And then we can write our own signal handler. For example, kernel has timer interrupt, so kernel can decide to send a signal to a process who need it periodically. /* An upcall is a mechanism that allows the kernel to execute a function in userspace, and potentially be returned information as a result. An upcall is like a signal, except that the kernel may use it at any time, for any purpose, including in an interrupt handler. This means that an upcall can potentially destroy the behaviour of the kernel. If an interrupt handler decides to ask a user space function for some information, and the function page faults or does blocking IO, your kernel is quite likely trashed. So, upcalls aren't there for everyday software development. But for specific purposes, they can be extremely useful. */ Green thread(early JAVA) user level lib, within a process lub does thread context swtich preemption via upcall/UNIX signal on timer interrupt use multiple processes for multi-core parallelism shared mem region mapped into each process Since the process can change the user level stack for the threads Say if we have a multi-core CPU, this does not work. Because when you want to let a core to take care of a thread, it needs kernel to kick in. However, the green thread does everthing at user level. The solution is to use shared mem region mapped into each process. More modern way Scheduler activations kernel allocates processors to user-level thread lib (after the specific core take over, the scheduling will be done in that core, which are decided by the user level processors in that core, and we have the following 2 bullet points) thread lib implements context switch thread lib decides what thread to run next upcall whenever kernel needs a user level scheduling decision (if there is I/O interrupt, kernel will ask the process to do sth or it will be blocked) process assigned a new processor processor removed from process sys call blocks in kernel ","date":"2021-12-12","objectID":"/os_slides_note/:3:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#green-threadearly-java"},{"categories":null,"content":"Lecture 19 We can use signal. Like in the terminal is s signal to do a up call, and kernel will ask the user processor to do something. And then we can write our own signal handler. For example, kernel has timer interrupt, so kernel can decide to send a signal to a process who need it periodically. /* An upcall is a mechanism that allows the kernel to execute a function in userspace, and potentially be returned information as a result. An upcall is like a signal, except that the kernel may use it at any time, for any purpose, including in an interrupt handler. This means that an upcall can potentially destroy the behaviour of the kernel. If an interrupt handler decides to ask a user space function for some information, and the function page faults or does blocking IO, your kernel is quite likely trashed. So, upcalls aren't there for everyday software development. But for specific purposes, they can be extremely useful. */ Green thread(early JAVA) user level lib, within a process lub does thread context swtich preemption via upcall/UNIX signal on timer interrupt use multiple processes for multi-core parallelism shared mem region mapped into each process Since the process can change the user level stack for the threads Say if we have a multi-core CPU, this does not work. Because when you want to let a core to take care of a thread, it needs kernel to kick in. However, the green thread does everthing at user level. The solution is to use shared mem region mapped into each process. More modern way Scheduler activations kernel allocates processors to user-level thread lib (after the specific core take over, the scheduling will be done in that core, which are decided by the user level processors in that core, and we have the following 2 bullet points) thread lib implements context switch thread lib decides what thread to run next upcall whenever kernel needs a user level scheduling decision (if there is I/O interrupt, kernel will ask the process to do sth or it will be blocked) process assigned a new processor processor removed from process sys call blocks in kernel ","date":"2021-12-12","objectID":"/os_slides_note/:3:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#more-modern-way"},{"categories":null,"content":"Spinlock issue Spinlock::acquire() { while (testAndSet(\u0026lockValue) == BUSY){ } } Spinlock::release() { lockValue = FREE } // downside: a waste of cpu resources // fairness issues: generally cannot assure fairness How to solve it? we may try to make the spinlock less aggressive. for perfomance and we should have a wait queue? for fairness // There might be multiple threads trying to acquire the lock and put themselves into the wait list. So there will be race conditions(since you will have the pointers to move and change). // Thus, they have to be synchronized ==\u003e they have to be protected by another lock -- a spinlock class Lock { private: SpinLock spinLock; // ==\u003e to protext the lock data structure int value = FREE; Queue waiting; public: void acquire(); void release(); } Lock::acquire() { spinLock.acquire(); if (value == BUSY) { waiting.add(myTCB); scheduler-\u003esuspend(\u0026spinLock); }else { value = BUSY; spinLock.release(); } } Lock::release() { spinLock.aquire(); if (!waiting.empty()) { next = waiting.remove(); scheduler-\u003emakeReady(next); }else { value = FREE; } spinLock.release(); } Sched::suspend(SpinLock *spinLock) { TCB* next; disableInterrupts(); // we need to disable interrupt to ensure the data consistancy? schedSpinLock.acquire(); // we need a seperate spin lock to protect the ready list spinLock-\u003erelease(); runningThread-\u003estate = WAITING; next = readyList.remove(); runningThread = next; thread_switch(myTCB, next); runningThread-\u003e RUNNING; schedSpinLock-\u003erelease(); enableInterrupts(); } Notice that the suspend() can resume the thread stopped by yield(). why we want interrupt if the interrupt handler need to modify the ready list, then it will acquire the spin lock that may be held by others. DEAD LOCK! Sched::makeReady(TCB* thread) { disableInterrupt(); schedSpinLock.acquire(); readyList.add(thread); thread-\u003estate = READY; schedSpinLock.release(); enableInterrupts(); } IMPORTANT: in Lock::acquire(), can we release the spinLock right after the waiting.add()? Because other threads may do sth before we change the readyList. maybe makeReady the thread that we are about to suspend. You won’t have a dead lock. But I’m about to put myself to waitList having a state of WAITING after someone put me in the readyList. The problem here is that every thread in ready list should have a state of ready, it’s about the data consistency. It’s not a bug though We really need to acqurie the lock in a order so that we don’t generate a gap between and may produce flaws. ","date":"2021-12-12","objectID":"/os_slides_note/:3:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#spinlock-issue"},{"categories":null,"content":"Spinlock issue Spinlock::acquire() { while (testAndSet(\u0026lockValue) == BUSY){ } } Spinlock::release() { lockValue = FREE } // downside: a waste of cpu resources // fairness issues: generally cannot assure fairness How to solve it? we may try to make the spinlock less aggressive. for perfomance and we should have a wait queue? for fairness // There might be multiple threads trying to acquire the lock and put themselves into the wait list. So there will be race conditions(since you will have the pointers to move and change). // Thus, they have to be synchronized == they have to be protected by another lock -- a spinlock class Lock { private: SpinLock spinLock; // == to protext the lock data structure int value = FREE; Queue waiting; public: void acquire(); void release(); } Lock::acquire() { spinLock.acquire(); if (value == BUSY) { waiting.add(myTCB); scheduler-suspend(\u0026spinLock); }else { value = BUSY; spinLock.release(); } } Lock::release() { spinLock.aquire(); if (!waiting.empty()) { next = waiting.remove(); scheduler-makeReady(next); }else { value = FREE; } spinLock.release(); } Sched::suspend(SpinLock *spinLock) { TCB* next; disableInterrupts(); // we need to disable interrupt to ensure the data consistancy? schedSpinLock.acquire(); // we need a seperate spin lock to protect the ready list spinLock-release(); runningThread-state = WAITING; next = readyList.remove(); runningThread = next; thread_switch(myTCB, next); runningThread- RUNNING; schedSpinLock-release(); enableInterrupts(); } Notice that the suspend() can resume the thread stopped by yield(). why we want interrupt if the interrupt handler need to modify the ready list, then it will acquire the spin lock that may be held by others. DEAD LOCK! Sched::makeReady(TCB* thread) { disableInterrupt(); schedSpinLock.acquire(); readyList.add(thread); thread-state = READY; schedSpinLock.release(); enableInterrupts(); } IMPORTANT: in Lock::acquire(), can we release the spinLock right after the waiting.add()? Because other threads may do sth before we change the readyList. maybe makeReady the thread that we are about to suspend. You won’t have a dead lock. But I’m about to put myself to waitList having a state of WAITING after someone put me in the readyList. The problem here is that every thread in ready list should have a state of ready, it’s about the data consistency. It’s not a bug though We really need to acqurie the lock in a order so that we don’t generate a gap between and may produce flaws. ","date":"2021-12-12","objectID":"/os_slides_note/:3:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#how-to-solve-it"},{"categories":null,"content":"Spinlock issue Spinlock::acquire() { while (testAndSet(\u0026lockValue) == BUSY){ } } Spinlock::release() { lockValue = FREE } // downside: a waste of cpu resources // fairness issues: generally cannot assure fairness How to solve it? we may try to make the spinlock less aggressive. for perfomance and we should have a wait queue? for fairness // There might be multiple threads trying to acquire the lock and put themselves into the wait list. So there will be race conditions(since you will have the pointers to move and change). // Thus, they have to be synchronized == they have to be protected by another lock -- a spinlock class Lock { private: SpinLock spinLock; // == to protext the lock data structure int value = FREE; Queue waiting; public: void acquire(); void release(); } Lock::acquire() { spinLock.acquire(); if (value == BUSY) { waiting.add(myTCB); scheduler-suspend(\u0026spinLock); }else { value = BUSY; spinLock.release(); } } Lock::release() { spinLock.aquire(); if (!waiting.empty()) { next = waiting.remove(); scheduler-makeReady(next); }else { value = FREE; } spinLock.release(); } Sched::suspend(SpinLock *spinLock) { TCB* next; disableInterrupts(); // we need to disable interrupt to ensure the data consistancy? schedSpinLock.acquire(); // we need a seperate spin lock to protect the ready list spinLock-release(); runningThread-state = WAITING; next = readyList.remove(); runningThread = next; thread_switch(myTCB, next); runningThread- RUNNING; schedSpinLock-release(); enableInterrupts(); } Notice that the suspend() can resume the thread stopped by yield(). why we want interrupt if the interrupt handler need to modify the ready list, then it will acquire the spin lock that may be held by others. DEAD LOCK! Sched::makeReady(TCB* thread) { disableInterrupt(); schedSpinLock.acquire(); readyList.add(thread); thread-state = READY; schedSpinLock.release(); enableInterrupts(); } IMPORTANT: in Lock::acquire(), can we release the spinLock right after the waiting.add()? Because other threads may do sth before we change the readyList. maybe makeReady the thread that we are about to suspend. You won’t have a dead lock. But I’m about to put myself to waitList having a state of WAITING after someone put me in the readyList. The problem here is that every thread in ready list should have a state of ready, it’s about the data consistency. It’s not a bug though We really need to acqurie the lock in a order so that we don’t generate a gap between and may produce flaws. ","date":"2021-12-12","objectID":"/os_slides_note/:3:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#why-we-want-interrupt"},{"categories":null,"content":"Lecture 20 Ordering, Condition Variable IMPORTANT, EXAM: why should we release the lock in suspend() rather than in the acquire() after waiting.add() // If we release the spinLock right after the waiting.add(), we will fail to keep the consistency. Other thread may get in and put that thread we just added in to the waiting list in the ready list. // And since the state update and switch() does not get to run, the vals of the TCB get put in the ready list is stale. // So when get resumed, it kind of walking back the time. Since the pointer to the code section may still point to somewhere it has executed in the past. Exam may ask where the lastest timing we can put the release() ","date":"2021-12-12","objectID":"/os_slides_note/:4:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#lecture-20-ordering-condition-variable"},{"categories":null,"content":"Ordering we can do thread_join() for every thread thread_join() is like a waiting list to synchronize all threads Shared Bounded Buffer A bounded buffer is shared among multiple threads for message exchange producer threads put items into the buffer (sending the message) consumerj thread get items from the buffer (receiving a message) Requirement: each message is received by only one thread. implementation // Consumer while(!(item=tryget())) { } use(item); // producer while(!tryput(item) { } tryget() { item = NULL; lock.acquire(); if (front \u003c tail) { item = buf(front % MAX); front++; notify threads waiting on condition (tail - front) \u003c MAX } else { go to sleep on condition front \u003c tail put thread on waiting list and release the lock lock.acquire } lock.release(); return item; } tryput(item) { success = 0; lock.acquire(); if (tail - front != 0) { buf(tail % MAX) = item; tail++; success = 1; notify threads waiting on condition front \u003c tail } else { go to sleep on condition (tail - front) \u003c MAX put thread on waiting list and release the lock lock.acquire } lock.release(); return success; } // need to have separate waiting queues for differnent conditions of interest Should we assume the condition is right after being waken up. NO!! Since other threads may take over in between right after the thread was woken up. How to fix this? tryget() { item = NULL; lock.acquire(); while (!(front \u003c tail) { go to sleep on condition front \u003c tail put thread on waiting list and release the lock // context switch (suspend or yield) lock.acquire } item = buf(front % MAX); front++; notify threads waiting on condition (tail - front) \u003c MAX lock.release(); return item; } tryput(item) { success = 0; lock.acquire(); while (!((tail - front) \u003c MAX)) { go to sleep on condition (tail - front) \u003c MAX put thread on waiting list and release the lock // context switch (suspend or yield) lock.acquire } buf(tail % MAX) = item; tail++; success = 1; notify threads waiting on condition front \u003c tail lock.release(); return success; } ","date":"2021-12-12","objectID":"/os_slides_note/:4:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#ordering"},{"categories":null,"content":"Ordering we can do thread_join() for every thread thread_join() is like a waiting list to synchronize all threads Shared Bounded Buffer A bounded buffer is shared among multiple threads for message exchange producer threads put items into the buffer (sending the message) consumerj thread get items from the buffer (receiving a message) Requirement: each message is received by only one thread. implementation // Consumer while(!(item=tryget())) { } use(item); // producer while(!tryput(item) { } tryget() { item = NULL; lock.acquire(); if (front ","date":"2021-12-12","objectID":"/os_slides_note/:4:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#shared-bounded-buffer"},{"categories":null,"content":"Ordering we can do thread_join() for every thread thread_join() is like a waiting list to synchronize all threads Shared Bounded Buffer A bounded buffer is shared among multiple threads for message exchange producer threads put items into the buffer (sending the message) consumerj thread get items from the buffer (receiving a message) Requirement: each message is received by only one thread. implementation // Consumer while(!(item=tryget())) { } use(item); // producer while(!tryput(item) { } tryget() { item = NULL; lock.acquire(); if (front ","date":"2021-12-12","objectID":"/os_slides_note/:4:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#implementation"},{"categories":null,"content":"Condition Variables Indicates a condition that needs to be satisfied for a thread to proceed. if condition is not satisfied, a thread can be put on a wait queue associated with the condition variable. Must call wait() inside a critical section, while holding the lock wait() atomically releases the lock and put the thread on the wait queue. when woken, reacquires the lock before returning from wait() A thread can wake thread(s) waiting on a condition variable signal broadcast ","date":"2021-12-12","objectID":"/os_slides_note/:4:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#condition-variables"},{"categories":null,"content":"CV Design Pattern methodThatWaits() { lock.acquire(); // R/W shared state while (!testSharedState()) { cv.wait(\u0026lock); } // R/W shared state lock.release(); } methodThatSignals() { lock.acquire(); // R/W shared state // if testSharedState is no true cv.signal(); // R/W shared state lock.release(); } ","date":"2021-12-12","objectID":"/os_slides_note/:4:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#cv-design-pattern"},{"categories":null,"content":"Shared Bounded Buffer get() { lock.acquire(); while(front == tail) { empty.wait(\u0026lock); } item = buf[front % MAX]; front++; full.signal(); lock.release(); return item; } put(item) { lock.acquire(); while ((tail - front) == MAX) { full.wait(\u0026lock); } buf[tail % MAX] = item; tail++; empty.signal(); lock.release*(); } // Initially: front = tail = 0; MAX is buffer capacity // empty/full are condition variables pre/post CV ","date":"2021-12-12","objectID":"/os_slides_note/:4:4","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#shared-bounded-buffer-1"},{"categories":null,"content":"Key Pionts for CV ALWAYS hold lock when calling wait, signal, broadcast Condition variable is sync FOR shared state ALWAYS hold lock when accessing shared state CV is memoryless if signal when no one is waiting, no op if wait before signal, waiter wakes up Wait atomically releases lock and put the thread into waiting list why? we might lost the signal, since before putting the thread into the waiting list, the signal might be emitted. When a thread is woken up from wait, it may not run immediately (this is why we need to put it in a loop) signal/broadcast put thread on ready list Wait MUST be in a loop while(needToWait()) { condition.Wait(\u0026lock); } simplified implementation of condition vars and locks of code that uses condition vars and locks ","date":"2021-12-12","objectID":"/os_slides_note/:4:5","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#key-pionts-for-cv"},{"categories":null,"content":"CV implementation class CV { private: Queue waiting; public: void wait(lock.isHeld()); void signal(); void broadcast(); } void CV::wait(Lock *lock) { assert(lock.isHeld()); waiting.add(myTCB); scheduler.suspend(\u0026lock); lock-\u003eacquire(); } void CV::signal() { if (waiting.notEmpty()) { thread = waiting.remove(); scheduler.makeReady(thread); } } void CV::broadcast() { while (waiting.notEmpty()) { thread = waiting.remove(); scheduler.makeReady(thread); } } ","date":"2021-12-12","objectID":"/os_slides_note/:4:6","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#cv-implementation"},{"categories":null,"content":"Lecture 21 Semaphore (Dijkstra again) One execution sequence waits for another purpose 1: mutual exclusion purpose 2: ordering (we use CV) Block oneself Unblock others Mutual Exclusion Lock.acquire() Lock.release() Ordering Cond.wait() Cond.signal() ","date":"2021-12-12","objectID":"/os_slides_note/:5:0","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#lecture-21-semaphore"},{"categories":null,"content":"Semaphore Concept Semaphore has a non-negative integer value P() atomically waits for value to become \u003e 0,. then decrements V() atomically increments value (waking up waiter if needed) Operations are atomic e.g., if value is 1, two P’s will result in value 0 and one waiter Block oneself Unblock others Mutual Exclusion Sem.P() Sem.V() Ordering Sem.P() Sem.V() Prob: it may hard to understand the code since this is too elegant Init val! Sem.init(N) sets an initial value of the semaphore The init val determines the behavior of the semaphore in subsequent use Using Sem for Mutual Exclusion Sem.init(1) ![Sem for Mutual Exclusion](img/Sem for Mutual Exclusion.png) EXAM: how to implement sth in Semaphore! The atomic means the whole things have to be atomic. From sem operations to wake up funciont and increment/decrement are all atomic! When we use with a init of 1, we call it binary sem, since it works like a lock. Using Sem for Ordering ![Sem for Ordering](img/Sem for Ordering.png) why you can’t do sem_mutex.P() first? // Becase if you do a sem_full.P(), you can potentially get blocked. // You can only be woken up by another thread using V(). // However, you are holding the sem_mutex.P() so that nobody can get in // DEAD LOCK! empty is init with 0 and full is init with MAX. Since the put() will work first and get() will wait first. Notice: For singals, they can emit singals and nobody is waiting. But for V(), if no one is waiting, then we will change the val! Which is not acceptable. V() will increment the val by 1. Signal is memoryless, but V() is not. Thus,semaphore is harder to implement. Also, for signal, when you wake up, you have to check the condition since it can be false by then. However, semaphore is differnent. Sem for CV? ","date":"2021-12-12","objectID":"/os_slides_note/:5:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#semaphore-concept"},{"categories":null,"content":"Semaphore Concept Semaphore has a non-negative integer value P() atomically waits for value to become 0,. then decrements V() atomically increments value (waking up waiter if needed) Operations are atomic e.g., if value is 1, two P’s will result in value 0 and one waiter Block oneself Unblock others Mutual Exclusion Sem.P() Sem.V() Ordering Sem.P() Sem.V() Prob: it may hard to understand the code since this is too elegant Init val! Sem.init(N) sets an initial value of the semaphore The init val determines the behavior of the semaphore in subsequent use Using Sem for Mutual Exclusion Sem.init(1) ![Sem for Mutual Exclusion](img/Sem for Mutual Exclusion.png) EXAM: how to implement sth in Semaphore! The atomic means the whole things have to be atomic. From sem operations to wake up funciont and increment/decrement are all atomic! When we use with a init of 1, we call it binary sem, since it works like a lock. Using Sem for Ordering ![Sem for Ordering](img/Sem for Ordering.png) why you can’t do sem_mutex.P() first? // Becase if you do a sem_full.P(), you can potentially get blocked. // You can only be woken up by another thread using V(). // However, you are holding the sem_mutex.P() so that nobody can get in // DEAD LOCK! empty is init with 0 and full is init with MAX. Since the put() will work first and get() will wait first. Notice: For singals, they can emit singals and nobody is waiting. But for V(), if no one is waiting, then we will change the val! Which is not acceptable. V() will increment the val by 1. Signal is memoryless, but V() is not. Thus,semaphore is harder to implement. Also, for signal, when you wake up, you have to check the condition since it can be false by then. However, semaphore is differnent. Sem for CV? ","date":"2021-12-12","objectID":"/os_slides_note/:5:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#init-val"},{"categories":null,"content":"Semaphore Concept Semaphore has a non-negative integer value P() atomically waits for value to become 0,. then decrements V() atomically increments value (waking up waiter if needed) Operations are atomic e.g., if value is 1, two P’s will result in value 0 and one waiter Block oneself Unblock others Mutual Exclusion Sem.P() Sem.V() Ordering Sem.P() Sem.V() Prob: it may hard to understand the code since this is too elegant Init val! Sem.init(N) sets an initial value of the semaphore The init val determines the behavior of the semaphore in subsequent use Using Sem for Mutual Exclusion Sem.init(1) ![Sem for Mutual Exclusion](img/Sem for Mutual Exclusion.png) EXAM: how to implement sth in Semaphore! The atomic means the whole things have to be atomic. From sem operations to wake up funciont and increment/decrement are all atomic! When we use with a init of 1, we call it binary sem, since it works like a lock. Using Sem for Ordering ![Sem for Ordering](img/Sem for Ordering.png) why you can’t do sem_mutex.P() first? // Becase if you do a sem_full.P(), you can potentially get blocked. // You can only be woken up by another thread using V(). // However, you are holding the sem_mutex.P() so that nobody can get in // DEAD LOCK! empty is init with 0 and full is init with MAX. Since the put() will work first and get() will wait first. Notice: For singals, they can emit singals and nobody is waiting. But for V(), if no one is waiting, then we will change the val! Which is not acceptable. V() will increment the val by 1. Signal is memoryless, but V() is not. Thus,semaphore is harder to implement. Also, for signal, when you wake up, you have to check the condition since it can be false by then. However, semaphore is differnent. Sem for CV? ","date":"2021-12-12","objectID":"/os_slides_note/:5:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#using-sem-for-mutual-exclusion"},{"categories":null,"content":"Semaphore Concept Semaphore has a non-negative integer value P() atomically waits for value to become 0,. then decrements V() atomically increments value (waking up waiter if needed) Operations are atomic e.g., if value is 1, two P’s will result in value 0 and one waiter Block oneself Unblock others Mutual Exclusion Sem.P() Sem.V() Ordering Sem.P() Sem.V() Prob: it may hard to understand the code since this is too elegant Init val! Sem.init(N) sets an initial value of the semaphore The init val determines the behavior of the semaphore in subsequent use Using Sem for Mutual Exclusion Sem.init(1) ![Sem for Mutual Exclusion](img/Sem for Mutual Exclusion.png) EXAM: how to implement sth in Semaphore! The atomic means the whole things have to be atomic. From sem operations to wake up funciont and increment/decrement are all atomic! When we use with a init of 1, we call it binary sem, since it works like a lock. Using Sem for Ordering ![Sem for Ordering](img/Sem for Ordering.png) why you can’t do sem_mutex.P() first? // Becase if you do a sem_full.P(), you can potentially get blocked. // You can only be woken up by another thread using V(). // However, you are holding the sem_mutex.P() so that nobody can get in // DEAD LOCK! empty is init with 0 and full is init with MAX. Since the put() will work first and get() will wait first. Notice: For singals, they can emit singals and nobody is waiting. But for V(), if no one is waiting, then we will change the val! Which is not acceptable. V() will increment the val by 1. Signal is memoryless, but V() is not. Thus,semaphore is harder to implement. Also, for signal, when you wake up, you have to check the condition since it can be false by then. However, semaphore is differnent. Sem for CV? ","date":"2021-12-12","objectID":"/os_slides_note/:5:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#using-sem-for-ordering"},{"categories":null,"content":"Semaphore Concept Semaphore has a non-negative integer value P() atomically waits for value to become 0,. then decrements V() atomically increments value (waking up waiter if needed) Operations are atomic e.g., if value is 1, two P’s will result in value 0 and one waiter Block oneself Unblock others Mutual Exclusion Sem.P() Sem.V() Ordering Sem.P() Sem.V() Prob: it may hard to understand the code since this is too elegant Init val! Sem.init(N) sets an initial value of the semaphore The init val determines the behavior of the semaphore in subsequent use Using Sem for Mutual Exclusion Sem.init(1) ![Sem for Mutual Exclusion](img/Sem for Mutual Exclusion.png) EXAM: how to implement sth in Semaphore! The atomic means the whole things have to be atomic. From sem operations to wake up funciont and increment/decrement are all atomic! When we use with a init of 1, we call it binary sem, since it works like a lock. Using Sem for Ordering ![Sem for Ordering](img/Sem for Ordering.png) why you can’t do sem_mutex.P() first? // Becase if you do a sem_full.P(), you can potentially get blocked. // You can only be woken up by another thread using V(). // However, you are holding the sem_mutex.P() so that nobody can get in // DEAD LOCK! empty is init with 0 and full is init with MAX. Since the put() will work first and get() will wait first. Notice: For singals, they can emit singals and nobody is waiting. But for V(), if no one is waiting, then we will change the val! Which is not acceptable. V() will increment the val by 1. Signal is memoryless, but V() is not. Thus,semaphore is harder to implement. Also, for signal, when you wake up, you have to check the condition since it can be false by then. However, semaphore is differnent. Sem for CV? ","date":"2021-12-12","objectID":"/os_slides_note/:5:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#why-you-cant-do-sem_mutexp-first"},{"categories":null,"content":"Semaphore Concept Semaphore has a non-negative integer value P() atomically waits for value to become 0,. then decrements V() atomically increments value (waking up waiter if needed) Operations are atomic e.g., if value is 1, two P’s will result in value 0 and one waiter Block oneself Unblock others Mutual Exclusion Sem.P() Sem.V() Ordering Sem.P() Sem.V() Prob: it may hard to understand the code since this is too elegant Init val! Sem.init(N) sets an initial value of the semaphore The init val determines the behavior of the semaphore in subsequent use Using Sem for Mutual Exclusion Sem.init(1) ![Sem for Mutual Exclusion](img/Sem for Mutual Exclusion.png) EXAM: how to implement sth in Semaphore! The atomic means the whole things have to be atomic. From sem operations to wake up funciont and increment/decrement are all atomic! When we use with a init of 1, we call it binary sem, since it works like a lock. Using Sem for Ordering ![Sem for Ordering](img/Sem for Ordering.png) why you can’t do sem_mutex.P() first? // Becase if you do a sem_full.P(), you can potentially get blocked. // You can only be woken up by another thread using V(). // However, you are holding the sem_mutex.P() so that nobody can get in // DEAD LOCK! empty is init with 0 and full is init with MAX. Since the put() will work first and get() will wait first. Notice: For singals, they can emit singals and nobody is waiting. But for V(), if no one is waiting, then we will change the val! Which is not acceptable. V() will increment the val by 1. Signal is memoryless, but V() is not. Thus,semaphore is harder to implement. Also, for signal, when you wake up, you have to check the condition since it can be false by then. However, semaphore is differnent. Sem for CV? ","date":"2021-12-12","objectID":"/os_slides_note/:5:1","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#sem-for-cv"},{"categories":null,"content":"Bottom Like regarding Semaphore an elegant and uniform solution for Synchronization Require careful design, in particular mapping the semaphore’s initial value to problem domain can be confusing, especially since it can be used for two very differnent purposes Recommendation for general programming: stick with lock and CV ","date":"2021-12-12","objectID":"/os_slides_note/:5:2","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#bottom-like-regarding-semaphore"},{"categories":null,"content":"Concurrency Bugs (safty: race condition progress: dead lock or starvation) did not apply Synchronization deadlock or starvation The Dining Philosopher Problem Principled Synchronization identfy objs or data structures that can be accessed by multiple threads Concurrency in OS kernel, everything Add locks to objs/module grab lock on start to every method/procedure Release lock on finish If need to wait while (needToWait()){condition.Wait(lock);} do not assume when you wake up the condition has become true if do sth that might wake someone up signal or broadcast always leave shared state vars in a consistent state when lock is released, or when entering waiting if need to acquire multiple locks, use the same order across all threads ","date":"2021-12-12","objectID":"/os_slides_note/:5:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#concurrency-bugs"},{"categories":null,"content":"Concurrency Bugs (safty: race condition progress: dead lock or starvation) did not apply Synchronization deadlock or starvation The Dining Philosopher Problem Principled Synchronization identfy objs or data structures that can be accessed by multiple threads Concurrency in OS kernel, everything Add locks to objs/module grab lock on start to every method/procedure Release lock on finish If need to wait while (needToWait()){condition.Wait(lock);} do not assume when you wake up the condition has become true if do sth that might wake someone up signal or broadcast always leave shared state vars in a consistent state when lock is released, or when entering waiting if need to acquire multiple locks, use the same order across all threads ","date":"2021-12-12","objectID":"/os_slides_note/:5:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#the-dining-philosopher-problem"},{"categories":null,"content":"Concurrency Bugs (safty: race condition progress: dead lock or starvation) did not apply Synchronization deadlock or starvation The Dining Philosopher Problem Principled Synchronization identfy objs or data structures that can be accessed by multiple threads Concurrency in OS kernel, everything Add locks to objs/module grab lock on start to every method/procedure Release lock on finish If need to wait while (needToWait()){condition.Wait(lock);} do not assume when you wake up the condition has become true if do sth that might wake someone up signal or broadcast always leave shared state vars in a consistent state when lock is released, or when entering waiting if need to acquire multiple locks, use the same order across all threads ","date":"2021-12-12","objectID":"/os_slides_note/:5:3","series":null,"tags":["OS","Note"],"title":"Virtual Memory and Concurrency Note 2","uri":"/os_slides_note/#principled-synchronization"},{"categories":null,"content":" first-post Share the knowledge and love. Read more... ","date":"2021-11-01","objectID":"/showcase/:0:0","series":null,"tags":null,"title":"Showcase","uri":"/showcase/#"},{"categories":null,"content":" You are not connected to the Internet, only cached pages will be available. ","date":"0001-01-01","objectID":"/offline/:0:0","series":null,"tags":null,"title":"Offline","uri":"/offline/#"}]