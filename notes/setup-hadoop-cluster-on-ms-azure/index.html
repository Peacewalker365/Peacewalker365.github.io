<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <meta name="generator" content="Hugo 0.90.1" />
  <link rel="canonical" href="https://Peacewalker365.github.io/notes/setup-hadoop-cluster-on-ms-azure/">

  
    
    <meta name="description" content="Report: Setup Hadoop Cluster on MS Azure https://klasserom.azurewebsites.net/Lessons/Binder/2410
Notice: In this report, the ssh .pem is named SSH_keypair.pem, and the user name in Linux by default is set to PceWlkr.
set up virtual machine Basics use the B1s as the bare bone std
The first vm set in the group will need to generate a new key pair of SSH public key, and give it a name.
Others can just use the SSH we had already.">
  

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#000000">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="theme-color" content="#ffffff">

  <link rel="stylesheet" type="text/css" href="/css/paper.css">

  
  
  <link rel="stylesheet" type="text/css" href="/css/custom.css">

  
  
  <title>Hadoop Cluster Setup | myStack</title>
</head>
  <body>
    <div class="container paper">
      <nav class="border split-nav">
  <div class="nav-brand">
    <h3><a href="/">myStack</a></h3>
  </div>
  <div class="collapsible">
    <input id="collapsible1" type="checkbox" name="collapsible1">
    <button>
    <label for="collapsible1">
        <div class="bar1"></div>
        <div class="bar2"></div>
        <div class="bar3"></div>
      </label>
    </button>
    <div class="collapsible-body">
      <ul class="inline">
      
        <li><a href="/blogs/">Blog</a></li>
      
        <li><a href="/notes/">Note</a></li>
      
        <li><a href="/tags/">Tags</a></li>
      
        <li><a href="/about/">About</a></li>
      
        <li><a href="https://github.com/Peacewalker365">Github</a></li>
      
      </ul>
    </div>
  </div>
</nav>
      <main>
        

<h1>Hadoop Cluster Setup</h1>
<h1 id="report-setup-hadoop-cluster-on-ms-azure">Report: Setup Hadoop Cluster on MS Azure</h1>
<p><a href="https://klasserom.azurewebsites.net/Lessons/Binder/2410">https://klasserom.azurewebsites.net/Lessons/Binder/2410</a></p>
<p>Notice: In this report, the ssh .pem is named SSH_keypair.pem, and the user name in Linux by default is set to PceWlkr.</p>
<h2 id="set-up-virtual-machine">set up virtual machine</h2>
<h3 id="basics">Basics</h3>
<p>use the B1s as the bare bone std</p>
<p>The first vm set in the group will need to generate a new key pair of SSH public key, and give it a name.</p>
<p>Others can just use the SSH we had already.</p>
<p>Set the public inbound ports.</p>
<p>(we set this up for testing purpose only, so the fact that all IP addrs will be allowed to access my vms doesn&rsquo;t bother.)</p>
<h3 id="disks">Disks</h3>
<p>go with standard SSD</p>
<p>encryption type: default</p>
<h3 id="networking">Networking</h3>
<p>virtual network: default</p>
<p>subnet: default</p>
<p>public IP: use the namenode ip</p>
<p>NIC network security group: Basic</p>
<p>Public inbound ports: allow selected ports</p>
<p>inbound ports</p>
<h3 id="management">Management</h3>
<p>auto-shutdown: do this since we may forgot shut then down and lose money for that</p>
<h3 id="advanced">Advanced</h3>
<h3 id="tags">Tags</h3>
<h3 id="review-and-create">Review and create</h3>
<p>review the price with extra cautions</p>
<h3 id="additional-info">Additional info</h3>
<p>it takes some time to create/</p>
<p>Download the private SSH key pair and store it carefully</p>
<h4 id="better-to-have-winscp-available">Better to have WinSCP available</h4>
<p>SFTP</p>
<p>host name is the public ip</p>
<p>port num is what we set before</p>
<p>input the .pem file, which is the OpenSSH private key</p>
<ul>
<li>WinSCP will convert it to .ppk</li>
</ul>
<p>call this node namenode</p>
<p>when connected, you can see the /home/PceWlkr/</p>
<h4 id="putty">Putty</h4>
<p>use putty to login the server</p>
<p><code>sudo apt-get update </code> //update the linux sys and everything installed</p>
<h2 id="add-vmi-to-your-virtual-network">Add VMI to your Virtual Network</h2>
<p>Now we should have a public IP addr 
Everytime you restart the VM, it will give you a new public IP.
Remember to update the IP you used for WinSCP.</p>
<p>We are going to create more VMs as the DataNodes.</p>
<p>Resource group: VirtualMachines
name: DataNode001
image: Ubuntu Server 20.04 LTS Gen 1 
Size: Std B1s</p>
<p>Username: PceWlkr 
SSH: SSH_keypair
inbound ports: SSH(22)</p>
<p>OS disk type: std
subnet: make it on the same network as last time
security group: Basic (will adjust later)</p>
<p>enable the auto-shutdown like before</p>
<p>Go to Virtual Machines on the dash board, check if everything is there. 
Go to the namenode and copy the IP addr and then go the WinSCP to change the host.
Clone it to a new site. 
Edit the newsite to register the datanode001 on WinSCP like before.</p>
<p>Go the terminal to check if things are there.</p>
<p>![VM Network](img/VM Network.png)</p>
<h2 id="add-a-dns-to-your-vmi">Add a DNS to your VMI</h2>
<p>Go to the namenode and click the DNS name.
Enter a DNS name label which must be unique that nobody else has reserved.
Now we can use the DNS in the WinSCP instead of IP.</p>
<h2 id="network-security-ports">Network Security Ports</h2>
<p>Go to the VM and then go to the Networking setting.
Add inbound port rule.
Service shoule be changed to SSH.
Priority: 100
Name: SSH_22</p>
<p>Then add another inbound port for the remote desktop.
Service:RDP
Priority: 110 (lower then SSH)
Name: RDP_3389</p>
<p><img src="/img/Common_Setting_for_Ports.png" alt="Common Setting for Ports"></p>
<p>Also set up a PPK SSH for the namenode.
Go to Advanced and Authentication, and go for the .pub in the private key file.
Convert it and then save and login.</p>
<h2 id="updateupgrade-packages">Update/Upgrade Packages</h2>
<h3 id="disable-unattended-upgrades">Disable Unattended Upgrades</h3>
<p><code>sudo apt -y remove unattended-upgrades</code>
or if you do not want to remove Unattended-Upgrades, use
<code>sudo dpkg-reconfigure unattended-upgrades</code> to turn on or off the features.</p>
<h3 id="update-and-upgrade-packages">Update and Upgrade Packages</h3>
<p><code>sudo apt-get update -y &amp;&amp; sudo apt-get upgrade -y</code></p>
<h2 id="hadoop-installation-and-configuration">Hadoop Installation and Configuration</h2>
<p>You will learn the following:</p>
<p>Administrate users in Ubuntu
Manage environment variables in Ubuntu
Use Secure Shell (SSH) to communicate with Virtual Machines
Hadoop Prerequistes
Java Developers Kit (JDK)
pdsh
rsync
Install and Configure Hadoop
Administrate Hadoop configuration files
Test HDFS commands
Test MapReduce using sample applications</p>
<h3 id="cluster-environment-setup">Cluster Environment Setup</h3>
<p>We have done this:
<code>sudo apt -y remove unattended-upgrades &amp;&amp; sudo apt-get update -y &amp;&amp; sudo apt-get upgrade -y</code></p>
<h4 id="secure-admin-account">Secure Admin Account</h4>
<p>In this case, my admin account&rsquo;s name is PceWlkr.
<code>sudo passwd PceWlkr</code></p>
<h4 id="common-environment-variables-for-hadoop-clusters">Common Environment Variables for Hadoop Clusters</h4>
<p>In general, these include:</p>
<p>Java - JAVA_HOME
Hadoop - HADOOP_HOME
Hive - HIVE_HOME
Pig - PIG_HOME</p>
<p>The <code>etc/profile.d</code> dir holding shell script will be run at start-up and install fuatures for all users.
So we&rsquo;ll create a file <code>bigdata.sh</code> for later installations.</p>
<p><code>sudo touch /etc/profile.d/bigdata.sh</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export BigDataSH<span style="color:#f92672">=</span>/etc/profile.d/bigdata.sh
export IdentityFile<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/.ssh/SSH_keypair.pem&#34;</span>
export SSHConfigFile<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;/.ssh/config&#34;</span>

sudo rm $BigDataSH
echo -e <span style="color:#e6db74">&#39;#!/bin/bash \n&#39;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;# Environment Variables for Big Data tools\n&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export BigDataSH=</span><span style="color:#e6db74">${</span>BigDataSH<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export IdentityFile=~</span><span style="color:#e6db74">${</span>IdentityFile<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export SSHConfigFile=~</span><span style="color:#e6db74">${</span>SSHConfigFile<span style="color:#e6db74">}</span><span style="color:#e6db74">\n&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null

sudo chmod <span style="color:#ae81ff">644</span> $BigDataSH
</code></pre></div><p><code>cat $BigDataSH</code></p>
<p>The result should be:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bash
</span><span style="color:#75715e"></span>
<span style="color:#75715e"># Environment Variables for Big Data tools</span>

export BigDataSH<span style="color:#f92672">=</span>/etc/profile.d/bigdata.sh
export IdentityFile<span style="color:#f92672">=</span>~/.ssh/SSH_keypair.pem
export SSHConfigFile<span style="color:#f92672">=</span>~/.ssh/config
</code></pre></div><p><code>sudo reboot</code></p>
<p>After rebooting, confirm the vars exist.
<code>echo $BigDataSH $IdentityFile $SSHConfigFile</code></p>
<p>The result should be:
/etc/profile.d/bigdata.sh /home/PceWlkr/.ssh/SSH_keypair.pem /home/PceWlkr/.ssh/config</p>
<h4 id="map-virtual-machine-cluster-environment-variables">Map Virtual Machine Cluster Environment Variables</h4>
<ol>
<li>Collect the Public DNS and &ldquo;Internal&rdquo; IP addresses from your Virtual Machine Instances</li>
<li>Copy and paste the following commands into a text editor:</li>
</ol>
<pre tabindex="0"><code>export NameNodeDNS=&quot;Namenode&quot;
export DataNode001DNS=&quot;Datanode001&quot;
export DataNode002DNS=&quot;Datanode002&quot;
export DataNode003DNS=&quot;Datanode003&quot;
export DataNode004DNS=&quot;Datanode004&quot;

export NameNodeIP=&quot;10.0.0.4&quot;
export DataNode001IP=&quot;10.0.0.5&quot;
export DataNode002IP=&quot;10.0.0.6&quot;
export DataNode003IP=&quot;10.0.0.7&quot;
</code></pre><p>Notice:
you can use <code>hostname</code> and <code>hostname -I</code> to find the hostname and private IP</p>
<p>Copy all of the commands above from the text editor
Paste the commands into the SSH session at the command line for your Virtual Machine Instance
When you pasted the commands, they were executed automatically.
Try to use one of the variables we set:
<code>echo $DataNode001DNS</code></p>
<p>The result should look something like this:</p>
<p><code>Datanode001</code></p>
<p>Execute the following to add the schema to the bigdata.sh file:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">echo -e <span style="color:#e6db74">&#34;# Cluster Variables START&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export NameNodeDNS=\&#34;</span><span style="color:#e6db74">${</span>NameNodeDNS<span style="color:#e6db74">}</span><span style="color:#e6db74">\&#34;&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export DataNode001DNS=\&#34;</span><span style="color:#e6db74">${</span>DataNode001DNS<span style="color:#e6db74">}</span><span style="color:#e6db74">\&#34;&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export DataNode002DNS=\&#34;</span><span style="color:#e6db74">${</span>DataNode002DNS<span style="color:#e6db74">}</span><span style="color:#e6db74">\&#34;&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export DataNode003DNS=\&#34;</span><span style="color:#e6db74">${</span>DataNode003DNS<span style="color:#e6db74">}</span><span style="color:#e6db74">\&#34;&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export DataNode004DNS=\&#34;</span><span style="color:#e6db74">${</span>DataNode004DNS<span style="color:#e6db74">}</span><span style="color:#e6db74">\&#34;&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export NameNodeIP=\&#34;</span><span style="color:#e6db74">${</span>NameNodeIP<span style="color:#e6db74">}</span><span style="color:#e6db74">\&#34;&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export DataNode001IP=\&#34;</span><span style="color:#e6db74">${</span>DataNode001IP<span style="color:#e6db74">}</span><span style="color:#e6db74">\&#34;&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export DataNode002IP=\&#34;</span><span style="color:#e6db74">${</span>DataNode002IP<span style="color:#e6db74">}</span><span style="color:#e6db74">\&#34;&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export DataNode003IP=\&#34;</span><span style="color:#e6db74">${</span>DataNode003IP<span style="color:#e6db74">}</span><span style="color:#e6db74">\&#34;&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export DataNode004IP=\&#34;</span><span style="color:#e6db74">${</span>DataNode004IP<span style="color:#e6db74">}</span><span style="color:#e6db74">\&#34;&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;# Cluster Variables END&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
</code></pre></div><p>Then Display the contents of the $BigDataSh file:
<code>cat $BigDataSH</code></p>
<p>result should be like:</p>
<pre tabindex="0"><code>#!/bin/bash

# Environment Variables for Big Data tools

export BigDataSH=/etc/profile.d/bigdata.sh
export IdentityFile=~/.ssh/SSH_keypair.pem
export SSHConfigFile=~/.ssh/config

# Cluster Variables START
export NameNodeDNS=&quot;Namenode&quot;
export DataNode001DNS=&quot;Datanode001&quot;
export DataNode002DNS=&quot;Datanode002&quot;
export DataNode003DNS=&quot;Datanode003&quot;
export DataNode004DNS=&quot;Datanode004&quot;

export NameNodeIP=&quot;10.0.0.4&quot;
export DataNode001IP=&quot;10.0.0.5&quot;
export DataNode002IP=&quot;10.0.0.6&quot;
export DataNode003IP=&quot;10.0.0.7&quot;
export DataNode004IP=&quot;10.0.0.8&quot;
# Cluster Variables END
</code></pre><p><code>sudo reboot</code>
test if the var is still accessible:
<code>echo $Datanode001DNS</code>
result should be like:
Datanode001</p>
<h4 id="passwordless-ssh-for-cluster-using-config-and-pem-files">Passwordless SSH for Cluster: Using .config and .pem files</h4>
<p>Edit a file named config:
The file maintains information about SSH connections for our cluster:</p>
<p>HostName - entry is the Public DNS or IPV4 value for the Virtual Machine instance.
User - the default user name for your Virtual Machine instance is ubuntu
IdentityFile - path to the Private Key (.pem) file you created while setting up your instances
The default key files that are created by SSH are: 
id_rsa - private key file
The .pem file is simply the id_rsa file with the extension of .pem 
id_rsa.pub - public key file generated during ssh-keygen or the creation of the first Virtual Machine Instance
id_rsa.ppk - private key file created by PuTTY when you connect through WinSCP</p>
<pre tabindex="0"><code>
Host *
  User PceWlkr
  IdentityFile ~/.ssh/SSH_keypair.pem

Host 0.0.0.0
  HostName 127.0.0.1

Host Namenode
  Hostname 10.0.0.4

Host Datanode001
  Hostname 10.0.0.5

Host Datanode002
  Hostname 10.0.0.6

#Host Datanode00n
#  Hostname xxx.xxx.xxx.xxx
</code></pre><p>Then put <code>config</code> and <code>.pem file</code> into the ~/.ssh/</p>
<p>The following code will build the config file from the command line:</p>
<p>Previously, we set up the $SSHConfigFile variable in bigdata.sh. It should point to ~/.ssh/config.</p>
<p>The following commands will build a config file based on environment variables that were set up earlier:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">sudo rm -rf $SSHConfigFile

echo -e <span style="color:#e6db74">&#34;Host *&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;  User PceWlkr&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;  IdentityFile </span><span style="color:#e6db74">${</span>IdentityFile<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;Host </span><span style="color:#e6db74">${</span>NameNodeDNS<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;  Hostname </span><span style="color:#e6db74">${</span>NameNodeIP<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;Host </span><span style="color:#e6db74">${</span>DataNode001DNS<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;  Hostname </span><span style="color:#e6db74">${</span>DataNode001IP<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;Host </span><span style="color:#e6db74">${</span>DataNode002DNS<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;  Hostname </span><span style="color:#e6db74">${</span>DataNode002IP<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;Host </span><span style="color:#e6db74">${</span>DataNode003DNS<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;  Hostname </span><span style="color:#e6db74">${</span>DataNode003IP<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;Host </span><span style="color:#e6db74">${</span>DataNode004DNS<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;  Hostname </span><span style="color:#e6db74">${</span>DataNode004IP<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;&#34;</span> | sudo tee --append $SSHConfigFile &gt; /dev/null
</code></pre></div><p>Set the permissions levels of the $SSHConfigFile file:
<code>sudo chmod 644 $SSHConfigFile</code></p>
<p>Then copy the config file and .pem from namenode to other nodes using:
<code>scp ~/.ssh/config ~/.ssh/SSH_keypair.pem PceWlkr@Datanode001:~/.ssh/</code>
or <code>scp -p $SSHConfigFile $IdentityFile PceWlkr@Datanode001:~/.ssh/</code></p>
<p>Then use <code>ssh Datanode001</code> to access a node.
<code>exit</code> to quit.</p>
<p><strong>Repeat till all other nodes realize the same functions</strong></p>
<h3 id="java-developers-kit-jdk">Java Developers Kit (JDK)</h3>
<p><code>sudo apt-get -y update</code>
<code>sudo apt-get -y install default-jdk</code></p>
<p>confirm you have it:
<code>cd /usr/lib/jvm/ &amp;&amp; ls</code>
result should be like:
default-java  java-1.11.0-openjdk-amd64  java-11-openjdk-amd64  openjdk-11</p>
<h4 id="add-environment-variables-to-etcprofiledbigdatash">Add Environment Variables to /etc/profile.d/bigdata.sh</h4>
<p>Open the file /etc/profile.d/bigdata.sh
<code>sudo nano $BigDataSH</code></p>
<p>Add these lines to the end of the bigdata.sh file:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">export JAVA_HOME<span style="color:#f92672">=</span>/usr/lib/jvm/default-java
PATH<span style="color:#f92672">=</span>$PATH:$JAVA_HOME/bin
</code></pre></div><p>Or, copy the following code and paste it at the command line. This will echo the entries into the bigdata.sh file</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">echo <span style="color:#e6db74">&#34;# JAVA Variables START&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo <span style="color:#e6db74">&#34;export JAVA_HOME=/usr/lib/jvm/default-java&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo <span style="color:#e6db74">&#34;PATH=\$PATH:\$JAVA_HOME/bin&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo <span style="color:#e6db74">&#34;# JAVA Variables END&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
</code></pre></div><p>Confirm that your Java variables were added, open the /etc/profile.d/bigdata.sh file:
<code>cat $BigDataSH</code></p>
<p>Your bigdata.sh file output should look like this now:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e">#!/bin/bash
</span><span style="color:#75715e"></span><span style="color:#75715e"># Environment Variables for Big Data tools</span>
export BigDataSH<span style="color:#f92672">=</span>/etc/profile.d/bigdata.sh
export IdentityFile<span style="color:#f92672">=</span>/etc/ssh/ssh_config.d/SSH_keypair.pem
export SSHConfigFile<span style="color:#f92672">=</span>/etc/ssh/ssh_config.d/config.conf

<span style="color:#75715e"># JAVA Variables START</span>
export JAVA_HOME<span style="color:#f92672">=</span>/usr/lib/jvm/default-java
PATH<span style="color:#f92672">=</span>$PATH:$JAVA_HOME/bin
<span style="color:#75715e"># JAVA Variables END</span>
</code></pre></div><p><code>sudo reboot</code></p>
<h4 id="confirm-the-java-version-and-environment-variables">Confirm the Java Version and Environment Variables</h4>
<p><code>java -version</code></p>
<p>result should be like:</p>
<pre tabindex="0"><code>openjdk version &quot;11.0.11&quot; 2021-04-20
OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04)
OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing)
</code></pre><p>confirm you have it:
<code>echo $JAVA_HOME</code>
result should be like:
/usr/lib/jvm/default-java</p>
<h4 id="java-developers-kit-jdk-for-datanodes-in-a-cluster">Java Developers Kit (JDK) for DataNodes in a Cluster</h4>
<p>To send a command to a DateNode using SSH, we wrap the command with quotes &quot;&quot; and ssh into the Node:
<code>ssh DataNode001 &quot;{command}&quot;</code></p>
<h5 id="update-packages-on-the-server">Update packages on the server</h5>
<p><code>ssh Datanode001 &quot;sudo apt-get -y update&quot;</code>
<code>ssh Datanode001 &quot;sudo apt-get -y install default-jdk&quot;</code></p>
<p>Notice:
You will synchronize your DataNodes with the NameNode later, so you do not need to set up environment variables yet.</p>
<h4 id="pdsh">pdsh</h4>
<pre tabindex="0"><code>pdsh is a variant of the rsh(1) command. Unlike rsh(1), which runs commands on a single remote host, pdsh can run multiple remote commands in parallel. pdsh uses a &quot;sliding window&quot; (or fanout) of threads to conserve resources on the initiating host while allowing some connections to time out.
</code></pre><p><code>sudo apt-get -y install pdsh</code>
Add the following variable to your bigdata.sh file:
<code>export PDSH_RCMD_TYPE=ssh</code></p>
<p>or run this commands:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">echo -e <span style="color:#e6db74">&#34;# PDSH Variables START&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export PDSH_RCMD_TYPE=ssh&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;# PDSH Variables END&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
</code></pre></div><p><code>sudo reboot</code></p>
<h4 id="rsync">rsync</h4>
<pre tabindex="0"><code>rsync is a utility for efficiently transferring and synchronizing files across computer systems by checking the timestamp and size of files. It is commonly found on Unix-like systems and functions as both a file synchronization and file transfer program. The rsync algorithm is a type of delta encoding and is used for minimizing network usage. Zlib may be used for additional compression, and SSH or stunnel can be used for data security. rsync(1) - Linux man page
</code></pre><p><code>sudo apt-get -y install rsync</code></p>
<p><code>sudo reboot</code></p>
<h2 id="install-and-configure-hadoop-as-a-single-node-cluster">Install and Configure Hadoop as a Single Node Cluster</h2>
<h3 id="download-hadoop-from-apache">Download Hadoop from Apache</h3>
<p><code>wget http://apache.forsale.plus/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz -P ~/Downloads/Hadoop</code></p>
<h3 id="uncompress-the-hadoop-tar-file-into-the-usrlocal-folder">Uncompress the Hadoop tar file into the /usr/local folder</h3>
<p><code>sudo tar -zxvf ~/Downloads/Hadoop/hadoop-*.tar.gz -C /usr/local</code></p>
<h3 id="rename-the-hadoop--directory-to-usrlocalhadoop">Rename the hadoop-* directory to /usr/local/hadoop</h3>
<p><code>sudo mv /usr/local/hadoop-* /usr/local/hadoop/</code></p>
<h3 id="modify-permissions-on-usrlocalhadoop">Modify permissions on /usr/local/hadoop/</h3>
<p>Allow read+write on the /usr/local/hadoop/ directory for anyone in the hadoop user group.</p>
<h4 id="double-check">Double Check!</h4>
<p>Before you run this command, confirm that you have created a hadoop user group. See Hadoop User and Group</p>
<h5 id="hadoop-user-named-hduser-and-user-group-named-hadoop">Hadoop User named (hduser) and User Group named (hadoop)</h5>
<h6 id="add-the-hadoop-user-group">Add the hadoop User Group</h6>
<p><code>sudo addgroup hadoop</code>
<code>sudo adduser hduser</code></p>
<h6 id="add-hduser-to-user-groups">Add hduser to User Groups</h6>
<p>Run this command to add hduser to the hadoop user group:
<code>sudo usermod -a -G hadoop hduser</code></p>
<p>Run this command to add hduser to the sudo (superuser) user group:
<code>sudo usermod -a -G sudo hduser</code></p>
<p>We will also add the  user to the hadoop user group.
<code>sudo usermod -a -G hadoop PceWlkr</code></p>
<p>Now you can switch to hduser when you type this command:
<code>su - hduser</code></p>
<p>Confirm which groups hduser is a member of:
<code>groups hduser</code></p>
<p>The result should look something like this:
hduser : hduser sudo hadoop</p>
<p><code>sudo reboot</code>
<code>su - hduser</code></p>
<h4 id="back-to-topic">Back to Topic</h4>
<p>This command will set ownership of all files and directories in the /usr/local/hadoop/ directory:
<code>sudo chown PceWlkr:PceWlkr -R /usr/local/hadoop/</code></p>
<p>Set the permissions on the /usr/local/hadoop/ directory to rwxrwxr&ndash;:</p>
<p>rwxrwxr&ndash;
User and Group: Read + Write + Execute
Other users: read</p>
<p><code>sudo chmod -R 774 /usr/local/hadoop/</code></p>
<h4 id="set-environment-variables">Set Environment Variables</h4>
<p>Use this command to edit the /etc/profile.d/bigdata.sh file:</p>
<p><code>sudo nano $BigDataSH</code></p>
<pre tabindex="0"><code>export HADOOP_HOME=&quot;/usr/local/hadoop&quot;
export HADOOP_CONF_DIR=&quot;${HADOOP_HOME}/etc/hadoop&quot;
export YARN_EXAMPLES=&quot;${HADOOP_HOME}/share/hadoop/mapreduce&quot;
PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
</code></pre><p>or run these:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">echo -e <span style="color:#e6db74">&#34;# HADOOP Variables START&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export HADOOP_HOME=\&#34;/usr/local/hadoop\&#34;&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export HADOOP_CONF_DIR=\&#34;\${HADOOP_HOME}/etc/hadoop\&#34;&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;export YARN_EXAMPLES=\&#34;\${HADOOP_HOME}/share/hadoop/mapreduce\&#34;&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;PATH=\$PATH:\$HADOOP_HOME/bin:\$HADOOP_HOME/sbin&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
echo -e <span style="color:#e6db74">&#34;# HADOOP Variables END&#34;</span> | sudo tee --append $BigDataSH &gt; /dev/null
</code></pre></div><h4 id="instantiate-environment-variables">Instantiate Environment Variables</h4>
<p>The following command will instantiate the new variables available immediately.  You can use this method to instantiate variables in any of the modified shell scripts .sh files:</p>
<p><code>source $BigDataSH</code></p>
<p>test if it&rsquo;s there:
<code>echo $HADOOP_HOME</code></p>
<p>result should be:
/usr/local/hadoop</p>
<p><code>sudo reboot</code></p>
<h4 id="test-environment-variables">Test Environment Variables</h4>
<p><code>echo $HADOOP_HOME</code></p>
<h4 id="test-hadoop-version">Test Hadoop Version</h4>
<p><code>hadoop version</code></p>
<h3 id="hadoop-configuration-files">Hadoop Configuration Files</h3>
<h4 id="add-java_home-variable-to-hadoop_conf_dirhadoop-envsh">Add JAVA_HOME Variable to $HADOOP_CONF_DIR/hadoop-env.sh</h4>
<p><code>echo $JAVA_HOME</code></p>
<p><code>sudo nano $HADOOP_CONF_DIR/hadoop-env.sh</code></p>
<p>Locate the area in the hadoop-env.sh file that indicates the JAVA_HOME variable. For Hadoop 3.3.1, this is on line 54.</p>
<p>The line should look something like this:</p>
<p><code># export JAVA_HOME=</code></p>
<p>Change the line entry to look like this statement; this value should be the same as your JAVA_HOME environment variable:</p>
<p><code>export JAVA_HOME=/usr/lib/jvm/default-java</code>
<code>sed -i 's/# export JAVA_HOME=/export JAVA_HOME=\/usr\/lib\/jvm\/default-java # export JAVA_HOME=/g' $HADOOP_CONF_DIR/hadoop-env.sh</code></p>
<p><code>cat $BigDataSH</code>
result should be:</p>
<pre tabindex="0"><code># The java implementation to use. By default, this environment
# variable is REQUIRED on ALL platforms except OS X!
export JAVA_HOME=/usr/lib/jvm/default-java
</code></pre><h4 id="modify-hadoop_conf_dircore-sitexml">Modify $HADOOP_CONF_DIR/core-site.xml</h4>
<p><code>sudo nano $HADOOP_CONF_DIR/core-site.xml</code>
Replace the contents of the core-site.xml file <configuration></configuration> section with the following lines:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml"><span style="color:#f92672">&lt;configuration&gt;</span>

	<span style="color:#75715e">&lt;!--Custom Properties--&gt;</span>
	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>thisnamenode<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#f92672">&lt;value&gt;</span>localhost<span style="color:#f92672">&lt;/value&gt;</span>

		<span style="color:#f92672">&lt;description&gt;</span>This used as a variable throughout the configuration files.
		localhost may be replaced with a DNS that points to the NameNode.
		<span style="color:#f92672">&lt;/description&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>

	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>homefolder<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#f92672">&lt;value&gt;</span>/home/${user.name}<span style="color:#f92672">&lt;/value&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>

	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>fs.defaultFS<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#f92672">&lt;value&gt;</span>hdfs://${thisnamenode}:9000<span style="color:#f92672">&lt;/value&gt;</span>

		<span style="color:#f92672">&lt;description&gt;</span>localhost may be replaced with a DNS that points to the NameNode.<span style="color:#f92672">&lt;/description&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>

	<span style="color:#75715e">&lt;!-- Do not enable permission check --&gt;</span>
	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>dfs.permissions.enabled<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#f92672">&lt;value&gt;</span>false<span style="color:#f92672">&lt;/value&gt;</span>

		<span style="color:#f92672">&lt;description&gt;</span>If &#34;true&#34;, enable permission checking in HDFS. If &#34;false&#34;, permission checking is turned off, but all other behavior is unchanged.
		Switching from one parameter value to the other does not change the mode, owner or group of files or directories.
		<span style="color:#f92672">&lt;/description&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>


	<span style="color:#75715e">&lt;!-- The current user is all set to root --&gt;</span>
	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>hadoop.http.staticuser.user<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#75715e">&lt;!-- For Virtual Machine Instances in the Cloud, use ubuntu --&gt;</span>
		<span style="color:#75715e">&lt;!-- &lt;value&gt;Pcewlkr&lt;/value&gt; --&gt;</span>
		<span style="color:#f92672">&lt;value&gt;</span>Pcewlkr<span style="color:#f92672">&lt;/value&gt;</span>

		<span style="color:#f92672">&lt;description&gt;</span>Pcewlkr is the default user for our NameNode. This property sets the WebUI user for file browsing.
For Virtual Machine Instances in the Cloud, use Pcewlkr
		<span style="color:#f92672">&lt;/description&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>

<span style="color:#f92672">&lt;/configuration&gt;</span>
</code></pre></div><h4 id="modify-hadoop_conf_diryarn-sitexml">Modify $HADOOP_CONF_DIR/yarn-site.xml</h4>
<p><code>sudo nano $HADOOP_CONF_DIR/yarn-site.xml</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml"><span style="color:#f92672">&lt;configuration&gt;</span>

	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>yarn.nodemanager.aux-services<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#f92672">&lt;value&gt;</span>mapreduce_shuffle<span style="color:#f92672">&lt;/value&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>

	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>mapred.job.tracker<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#f92672">&lt;value&gt;</span>${thisnamenode}:9001<span style="color:#f92672">&lt;/value&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>

	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>yarn.nodemanager.env-whitelist<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#f92672">&lt;value&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span style="color:#f92672">&lt;/value&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>

<span style="color:#f92672">&lt;/configuration&gt;</span>
</code></pre></div><h4 id="modify-hadoop_conf_dirmapred-sitexml">Modify $HADOOP_CONF_DIR/mapred-site.xml</h4>
<p><code>sudo nano $HADOOP_CONF_DIR/mapred-site.xml</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml"><span style="color:#f92672">&lt;configuration&gt;</span>

	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>mapreduce.jobtracker.address<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#f92672">&lt;value&gt;</span>local<span style="color:#f92672">&lt;/value&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>

	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>mapreduce.framework.name<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#f92672">&lt;value&gt;</span>yarn<span style="color:#f92672">&lt;/value&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>

<span style="color:#f92672">&lt;/configuration&gt;</span>
</code></pre></div><h4 id="modify-hadoop_conf_dirhdfs-sitexml">Modify $HADOOP_CONF_DIR/hdfs-site.xml</h4>
<p><code>sudo nano $HADOOP_CONF_DIR/hdfs-site.xml</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml"><span style="color:#f92672">&lt;configuration&gt;</span>

	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>dfs.replication<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#75715e">&lt;!--&lt;value&gt;3&lt;/value&gt;--&gt;</span>
		<span style="color:#f92672">&lt;value&gt;</span>1<span style="color:#f92672">&lt;/value&gt;</span>

		<span style="color:#f92672">&lt;description&gt;</span>Default block replication.

The actual number of replications can be specified when the file is created.

The default is used if replication is not specified in create time.

When migrating your cluster to a Fully Distributed Cluster, change this to 3.

		<span style="color:#f92672">&lt;/description&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>

	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>dfs.permissions.enabled<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#f92672">&lt;value&gt;</span>false<span style="color:#f92672">&lt;/value&gt;</span>

		<span style="color:#f92672">&lt;description&gt;</span>If &#34;true&#34;, enable permission checking in HDFS. If &#34;false&#34;, permission checking is turned off, but all other behavior is unchanged.
		Switching from one parameter value to the other does not change the mode, owner or group of files or directories.
		<span style="color:#f92672">&lt;/description&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>

	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>dfs.namenode.name.dir<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#f92672">&lt;value&gt;</span>file:///home/${user.name}/hadoop/dfs/name<span style="color:#f92672">&lt;/value&gt;</span>

		<span style="color:#f92672">&lt;description&gt;</span>Determines where on the local filesystem the DFS name node should store the name table(fsimage).
		If this is a comma-delimited list of directories then the name table is replicated in all of the directories, for redundancy.
		<span style="color:#f92672">&lt;/description&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>

	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>dfs.datanode.data.dir<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#f92672">&lt;value&gt;</span>file:///home/${user.name}/hadoop/dfs/data<span style="color:#f92672">&lt;/value&gt;</span>

		<span style="color:#f92672">&lt;description&gt;</span>Determines where on the local filesystem an DFS data node should store its blocks. If this is a comma-delimited list of directories,
		then data will be stored in all named directories, typically on different devices. Directories that do not exist are ignored.
		<span style="color:#f92672">&lt;/description&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>

	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>dfs.namenode.checkpoint.dir<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#f92672">&lt;value&gt;</span>file:///home/${user.name}/hadoop/dfs/namesecondary<span style="color:#f92672">&lt;/value&gt;</span>

		<span style="color:#f92672">&lt;description&gt;</span>Determines where on the local filesystem the DFS secondary name node should store the temporary images to merge.
		If this is a comma-delimited list of directories then the image is replicated in all of the directories for redundancy.
		<span style="color:#f92672">&lt;/description&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>

	<span style="color:#f92672">&lt;property&gt;</span>

		<span style="color:#f92672">&lt;name&gt;</span>dfs.journalnode.edits.dir<span style="color:#f92672">&lt;/name&gt;</span>

		<span style="color:#f92672">&lt;value&gt;</span>file:///home/${user.name}/hadoop/dfs/journalnode<span style="color:#f92672">&lt;/value&gt;</span>

		<span style="color:#f92672">&lt;description&gt;</span>
		The directory where the journal edit files are stored.
		<span style="color:#f92672">&lt;/description&gt;</span>

	<span style="color:#f92672">&lt;/property&gt;</span>

<span style="color:#f92672">&lt;/configuration&gt;</span>
</code></pre></div><h3 id="format-the-hdfs">Format the HDFS</h3>
<p><code>hdfs namenode -format</code></p>
<h3 id="start-up-your-hadoop-cluster">Start up your Hadoop Cluster</h3>
<p><code>start-dfs.sh</code></p>
<p>check the namenode  <code>http://DNSofNameNode:9870</code></p>
<p><code>start-yarn.sh</code></p>
<p><code>mapred --daemon start historyserver</code></p>
<p>check the history website  <code>http://DNSofNameNode:19888</code></p>
<p>check the resource manager website  <code>http://DNSofNameNode:8088</code></p>
<p><code>jps</code> will show you the running java processes</p>
<p>On windows, edit <code>C:\Windows\System32\drivers\etc\hosts</code>, add</p>
<pre tabindex="0"><code>xxx.xxx.xxx.xxx   namenode.internal.cloudapp.net  namenode
xxx.xxx.xxx.xxx   datanode001.internal.cloudapp.net  datanode001
xxx.xxx.xxx.xxx   datanode002.internal.cloudapp.net  datanode002
</code></pre><p>So that Windows can find the interal website http://namenode:9870 or <a href="http://namenode.internal.cloudapp.net:9870">http://namenode.internal.cloudapp.net:9870</a></p>
<h3 id="test-the-hdfs">Test the HDFS</h3>
<p><code>hdfs dfsadmin -report</code></p>
<p><code>hdfs dfs -mkdir -p /user/PceWlkr/hdfs/tests</code></p>
<p><code>mkdir -p ~/Documents/HDFS/Tests</code></p>
<p><code>touch ~/Documents/HDFS/Tests/test.txt</code></p>
<p><code>hdfs dfs -put ~/Documents/HDFS/Tests/test.txt /user/PceWlkr/hdfs/tests</code></p>
<p><code>hdfs dfs -ls /user/PceWlkr/hdfs/tests</code></p>
<h2 id="fully-distributed-hadoop-cluster-on-a-cloud-provider">Fully Distributed Hadoop Cluster on a Cloud Provider</h2>
<p><img src="/img/Fully_Distributed_Mode_Config_Files.png" alt="Fully Distributed Mode Config Files"></p>
<p>These configuration files include:</p>
<ul>
<li>SSH
<ul>
<li>config</li>
<li>SSH_keypair.pem</li>
</ul>
</li>
<li>Environment Variables
<ul>
<li>bigdata.sh</li>
</ul>
</li>
<li>Environment Setup
<ul>
<li>Java JDK</li>
<li>pdsh</li>
<li>rsync</li>
</ul>
</li>
<li>Hadoop Configuration - the Hadoop entire directory is usually copied directly to the new DataNodes
<ul>
<li>hadoop-env.sh</li>
<li>hdfs.site.xml</li>
<li>core-site.xml</li>
<li>yarn-site.xml</li>
<li>mapred-site.xml</li>
<li>masters</li>
<li>workers</li>
</ul>
</li>
</ul>
<h3 id="configure-masters-file">Configure .masters File</h3>
<p><code>sudo touch $HADOOP_CONF_DIR/masters</code>
Add the DNS of your Master Node:</p>
<pre tabindex="0"><code>Namenode
Datanode001 
Datanode002 
</code></pre><p><code>sudo chown ubuntu $HADOOP_CONF_DIR/masters</code>
<code>sudo chmod 0644 $HADOOP_CONF_DIR/masters</code></p>
<h3 id="configure-workers-file">Configure .workers File</h3>
<p>&lsquo;sudo nano $HADOOP_CONF_DIR/workers&rsquo;
Add the entries for the Data Nodes:</p>
<pre tabindex="0"><code>Datanode001 
Datanode002 
</code></pre><p><code>sudo chown ubuntu $HADOOP_CONF_DIR/workers</code>
<code>sudo chmod 0644 $HADOOP_CONF_DIR/workers</code></p>
<h3 id="modify-configuration-files-for-fully-distributed-cluster">Modify configuration files for Fully Distributed Cluster</h3>
<p><code>sudo nano $HADOOP_CONF_DIR/core-site.xml</code></p>
<p>Change the thisnamenode property value to NameNode in the core-site.xml file.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml"><span style="color:#f92672">&lt;property&gt;</span>

    <span style="color:#f92672">&lt;name&gt;</span>thisnamenode<span style="color:#f92672">&lt;/name&gt;</span>

    <span style="color:#f92672">&lt;value&gt;</span>NameNode<span style="color:#f92672">&lt;/value&gt;</span>

    <span style="color:#f92672">&lt;description&gt;</span>NameNode is the hostname specified in the config file and etc/hosts file.
It may be replaced with a DNS that points to your NameNode.<span style="color:#f92672">&lt;/description&gt;</span>

  <span style="color:#f92672">&lt;/property&gt;</span>
</code></pre></div><p><code>sudo nano $HADOOP_CONF_DIR/hdfs-site.xml</code></p>
<p>Edit the dfs.replication property in hdfs-site.xml.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-xml" data-lang="xml"><span style="color:#f92672">&lt;property&gt;</span>

    <span style="color:#f92672">&lt;name&gt;</span>dfs.replication<span style="color:#f92672">&lt;/name&gt;</span>

    <span style="color:#f92672">&lt;value&gt;</span>3<span style="color:#f92672">&lt;/value&gt;</span>

    <span style="color:#f92672">&lt;description&gt;</span>Default block replication.

The actual number of replications can be specified when the file is created.

The default is used if replication is not specified in create time.

    <span style="color:#f92672">&lt;/description&gt;</span>

  <span style="color:#f92672">&lt;/property&gt;</span>
</code></pre></div><h3 id="add-a-datanode-to-your-hadoop-cluster">Add a DataNode to your Hadoop Cluster</h3>
<p><code>stop-dfs.sh</code></p>
<p><code>scp -p $SSHConfigFile $IdentityFile PceWlkr@Datanode001:~/.ssh/</code></p>
<p><code>scp -p $SSHConfigFile $IdentityFile PceWlkr@Datanode002:~/.ssh/</code></p>
<p><code>ssh Datanode001</code></p>
<p><code>sudo apt -y remove unattended-upgrades &amp;&amp; sudo apt-get -y update &amp;&amp; sudo apt-get -y upgrade &amp;&amp; sudo apt-get -y install default-jdk pdsh rsync</code></p>
<p><code>exit</code></p>
<p><code>ssh Datanode002</code></p>
<p><code>sudo apt -y remove unattended-upgrades &amp;&amp; sudo apt-get -y update &amp;&amp; sudo apt-get -y upgrade &amp;&amp; sudo apt-get -y install default-jdk pdsh rsync</code></p>
<p><code>exit</code></p>
<p>If the <code>hadoop</code> directory does not exist on the DataNode, you must first create it. Do this for each DataNode in your Cluster.
Create the <code>hadoop</code> directory:</p>
<p><code>ssh PceWlkr@Datanode001 &quot;sudo mkdir -p ${HADOOP_HOME}/&quot;</code></p>
<p><code>ssh PceWlkr@Datanode001 &quot;sudo chown -R PceWlkr:PceWlkr ${HADOOP_HOME}/&quot;</code></p>
<p><code>ssh PceWlkr@Datanode001 &quot;sudo chmod -R 774 ${HADOOP_HOME}/&quot;</code></p>
<p><code>ssh PceWlkr@Datanode002 &quot;sudo mkdir -p ${HADOOP_HOME}/&quot;</code></p>
<p><code>ssh PceWlkr@Datanode002 &quot;sudo chown -R PceWlkr:PceWlkr ${HADOOP_HOME}/&quot;</code></p>
<p><code>ssh PceWlkr@Datanode002 &quot;sudo chmod -R 774 ${HADOOP_HOME}/&quot;</code></p>
<p><code>cat $BigDataSH | ssh PceWlkr@Datanode001 &quot;sudo tee ${BigDataSH}&quot;</code></p>
<p><code>cat $BigDataSH | ssh PceWlkr@Datanode002 &quot;sudo tee ${BigDataSH}&quot;</code></p>
<p><code>sudo rm $HADOOP_HOME/logs/*.*</code></p>
<p><code>rsync -ravl $HADOOP_HOME PceWlkr@Datanode001:/usr/local/</code></p>
<p><code>rsync -ravl $HADOOP_HOME PceWlkr@Datanode002:/usr/local/</code></p>
<p>Then reboot the Datanode001 and Datanode002</p>
<h3 id="test-the-hdfs-again">Test the HDFS Again</h3>
<h3 id="test-mapreduce">Test MapReduce</h3>
<p><code>hadoop jar $YARN_EXAMPLES/hadoop-mapreduce-examples-3.3.1.jar</code></p>
<p><code>hdfs dfs -mkdir -p /user/PceWlkr/wordcount/input</code></p>
<p><code>hdfs dfs -chmod -R 777 /user</code></p>
<p><code>hdfs dfs -put $HADOOP_HOME/*.txt    /user/PceWlkr/wordcount/input</code></p>
<p><code>hadoop jar $YARN_EXAMPLES/hadoop-mapreduce-examples-3.3.1.jar  wordcount /user/ubPceWlkrntu/wordcount/input /user/PceWlkr/wordcount/output</code></p>
<p><code>hdfs dfs -cat /user/PceWlkr/wordcount/output/*</code></p>
<h1 id="env-vars">Env Vars</h1>
<h4 id="the-hadoop-config-directory">The hadoop config directory</h4>
<p>HADOOP_CONF_DIR</p>
<h4 id="the-directory-of-the-progtram">The directory of the progtram</h4>
<p>JAVA_HOME</p>
<p>HADOOP_HOME</p>
<p>HIVE_HOME</p>
<p>PIG_HOME</p>
<p>HADOOP_YARN_HOME</p>
<p>HADOOP_MAPRED_HOME</p>
<h4 id="the-path-of-the-files-under-ssh">The path of the files under ~/.ssh/</h4>
<p>BigDataSH=/etc/profile.d/bigdata.sh</p>
<p>IdentityFile=~/.ssh/SSH_keypair.pem</p>
<p>SSHConfigFile=~/.ssh/config</p>
<h4 id="the-following-are-dns-and-ip-for-all-nodes">The following are DNS and IP for all nodes</h4>
<p>NameNodeDNS=&ldquo;Namenode&rdquo;</p>
<p>DataNode001DNS=&ldquo;Datanode001&rdquo;</p>
<p>DataNode002DNS=&ldquo;Datanode002&rdquo;</p>
<p>NameNodeIP=&ldquo;10.0.0.4&rdquo;</p>
<p>DataNode001IP=&ldquo;10.0.0.5&rdquo;</p>
<p>DataNode002IP=&ldquo;10.0.0.6&rdquo;</p>
<h1 id="configs">Configs</h1>
<ul>
<li>SSH
<ul>
<li>config &ndash;&gt; give a identity to anyone enter through ssh, and give all host a IP and DNS so that ssh can recognize them</li>
<li>SSH_keypair.pem &ndash;&gt; The ssh key</li>
</ul>
</li>
<li>Environment Variables
<ul>
<li>bigdata.sh &ndash;&gt; storing env vars for ssh config and files, java vars, pdsh vars, and other vars</li>
</ul>
</li>
<li>Hadoop Configuration - the Hadoop entire directory is usually copied directly to the new DataNodes
<ul>
<li>hadoop-env.sh &ndash;&gt; some env vars</li>
<li>hdfs.site.xml &ndash;&gt; config for HDFS</li>
<li>core-site.xml &ndash;&gt; config for Hadoop and Namenode</li>
<li>yarn-site.xml &ndash;&gt; config for yarn</li>
<li>mapred-site.xml &ndash;&gt; config for MapReduce</li>
<li>masters &ndash;&gt; specify the master nodes</li>
<li>workers &ndash;&gt; specify the slave nodes</li>
</ul>
</li>
</ul>
<h1 id="daemons">Daemons</h1>
<p>HDFS Daemons: <strong>They are configured by masters and workers and hdfs.site.xml and core-site.xml</strong></p>
<p>Name Node</p>
<p>Secondary Name Node</p>
<p>Data Node</p>
<p>YARN Daemons: <strong>They are configured by yarn-site.xml</strong></p>
<p>Resource Manager</p>
<p>Node Manager</p>
<p>MapReduce Daemon: <strong>They are configured by mapred-site.xml</strong></p>
<p>Job History Server</p>
<h1 id="cluster-tests">Cluster Tests</h1>
<h2 id="hdfs-test-result">HDFS Test Result</h2>
<h2 id="mapreduce-test-result">MapReduce Test Result</h2>

  

      </main>
      
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-123456789-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  </div>
  </body>
</html>